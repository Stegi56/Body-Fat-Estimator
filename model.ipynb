{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported modules.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pd.options.display.max_rows = 300\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "\n",
    "seed = 420\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(\"Imported modules.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set loaded. Num examples:  252\n",
      "Made training and test sets\n"
     ]
    },
    {
     "data": {
      "text/plain": "     BodyFat  Age  Neck  Knee  Ankle  Biceps  Forearm  Wrist     BMI  \\\n53      6.30   49 35.10 37.60  22.60   38.50    27.40  18.50  317.45   \n249    29.30   72 38.90 37.30  21.50   31.30    27.20  18.00  528.42   \n164    27.30   34 39.50 42.00  23.40   34.00    31.20  18.50  664.61   \n214    19.50   50 37.40 38.10  21.80   28.60    26.70  18.00  408.80   \n245    15.20   68 36.30 37.50  22.60   29.20    27.30  18.50  349.17   \n193    24.70   42 38.50 43.30  26.00   33.70    29.90  18.50  675.75   \n66     21.50   54 35.60 36.10  21.70   29.60    27.40  17.40  324.41   \n69     12.90   55 36.30 37.40  21.60   27.30    27.10  17.30  343.64   \n91     18.20   44 39.20 39.70  23.10   31.40    28.40  18.80  464.89   \n18     16.00   28 38.00 38.70  22.90   37.20    30.50  18.50  498.36   \n24     14.00   28 34.50 35.50  22.90   31.10    28.00  17.60  337.66   \n39     32.60   50 40.20 41.10  24.70   34.10    31.00  18.30  615.06   \n213    18.70   50 39.00 40.90  25.50   32.70    30.00  19.00  536.08   \n234    25.80   60 40.40 35.70  21.00   31.30    28.70  18.30  368.67   \n19     16.50   33 40.00 40.60  24.00   37.10    30.10  18.20  610.04   \n180    26.60   39 40.00 42.80  24.10   35.60    29.00  19.00  647.41   \n194    22.80   42 35.40 38.90  22.40   31.70    27.10  17.10  364.09   \n232    15.40   58 38.00 38.90  23.60   30.90    29.60  18.00  430.77   \n110    19.70   43 37.20 38.00  22.30   33.30    28.20  18.10  425.63   \n80     31.40   67 38.40 38.20  23.70   29.40    27.20  19.00  395.78   \n6      19.20   26 36.40 38.30  22.90   31.90    27.80  17.70  469.69   \n163    15.10   34 36.00 35.60  20.40   28.30    26.20  16.50  278.01   \n148     5.30   25 35.20 35.70  22.00   25.80    25.20  16.90  285.02   \n0      12.30   23 36.20 37.30  21.90   32.00    27.40  17.10  351.19   \n77     22.20   69 38.70 38.30  21.80   30.80    25.70  18.80  461.24   \n150     9.40   26 35.40 35.90  20.40   31.60    29.00  17.80  335.94   \n120    27.90   52 40.80 39.30  24.60   33.90    31.20  19.50  572.38   \n226    14.80   55 37.20 38.50  22.60   33.40    29.30  18.80  420.96   \n7      12.40   25 37.80 39.40  23.20   30.50    29.00  18.80  427.26   \n198     6.60   42 37.60 40.00  22.50   30.60    30.00  18.50  384.50   \n49      4.00   47 34.00 34.40  21.90   26.80    25.80  16.80  243.54   \n127    17.40   43 37.50 35.80  20.80   33.90    28.20  17.40  342.14   \n44      7.70   39 31.50 34.70  21.00   26.10    23.10  16.10  230.70   \n149    25.20   26 40.60 42.70  24.70   36.00    30.40  18.40  707.89   \n160     9.40   31 35.00 36.60  21.00   27.00    26.30  16.50  316.63   \n219    15.00   53 37.60 36.20  22.00   28.50    25.70  17.10  344.70   \n20     19.10   28 39.10 38.00  22.10   32.50    30.30  18.40  471.19   \n42     31.60   48 37.30 40.90  25.00   36.70    29.80  18.40  672.70   \n33     21.30   41 39.80 44.20  25.20   37.50    31.50  18.70  672.43   \n122    14.70   40 36.90 38.70  22.60   34.40    28.00  17.60  373.53   \n165    19.20   35 40.50 41.30  25.60   36.40    33.70  19.40  638.49   \n125    17.50   46 36.60 36.00  21.90   35.60    30.20  17.60  416.25   \n220    12.40   54 38.50 38.10  23.90   31.40    29.90  18.90  333.13   \n189    24.40   41 38.00 40.40  22.90   33.40    29.20  18.50  501.47   \n135    27.10   44 37.80 37.90  22.70   30.90    28.80  17.60  496.00   \n34     32.30   41 42.10 43.30  26.30   37.30    31.70  19.70  831.74   \n170     3.00   35 37.00 36.20  22.10   30.40    27.40  17.70  342.14   \n210     7.10   49 35.80 35.00  21.70   30.90    28.80  17.40  290.30   \n156    31.20   28 38.50 40.00  25.20   35.20    30.70  19.10  613.52   \n145    14.20   24 35.70 36.50  22.00   33.50    28.30  17.30  343.97   \n116    20.10   48 36.80 38.40  22.80   29.90    28.00  18.10  431.86   \n217     7.50   51 36.90 39.00  22.60   27.50    25.90  18.60  341.00   \n79     18.80   66 37.40 39.30  22.70   30.30    28.70  19.00  423.49   \n240    17.00   65 34.70 33.40  20.10   28.50    24.80  16.50  247.24   \n139    20.40   49 40.80 42.50  24.50   35.50    29.80  18.70  603.50   \n25      3.70   27 35.70 36.70  22.50   29.90    28.20  17.70  354.69   \n195    25.50   42 38.50 38.40  24.10   32.90    29.80  18.80  474.73   \n204    34.80   44 40.90 40.30  21.80   34.80    30.70  17.40  712.96   \n11      7.80   27 39.40 39.20  25.90   37.20    30.20  19.00  613.89   \n224    10.90   55 41.10 37.10  21.80   34.10    31.10  19.20  469.96   \n211    27.20   49 40.20 40.30  23.20   36.80    31.00  18.90  627.71   \n62     30.70   54 38.00 39.40  23.60   32.70    29.90  19.10  531.61   \n108    17.30   43 38.50 40.00  24.80   35.10    30.70  19.20  498.49   \n176    13.10   37 35.30 38.10  22.00   31.50    26.60  16.70  340.31   \n233    26.70   58 35.10 35.90  21.00   27.80    26.10  17.60  389.04   \n231    16.10   57 39.40 38.60  22.80   31.80    29.10  19.00  462.93   \n119    18.10   44 38.00 39.20  24.50   32.10    28.60  18.00  486.59   \n200    12.20   43 37.80 39.20  23.80   31.70    28.40  18.60  452.29   \n22     15.60   31 33.90 35.30  22.20   27.90    25.90  16.70  288.21   \n2      25.30   22 34.00 38.90  24.00   28.80    25.20  16.60  357.98   \n181     0.00   40 33.80 33.50  20.20   27.70    24.60  16.50  206.50   \n71      8.80   55 38.70 37.60  21.60   30.30    27.30  18.30  313.24   \n199    23.60   43 37.40 39.00  24.10   33.80    28.80  18.80  431.93   \n46     10.80   40 33.60 34.50  22.50   27.90    26.20  17.30  264.03   \n171     0.70   35 34.00 34.80  22.00   24.80    25.90  16.90  241.42   \n251    31.90   74 40.80 42.20  24.60   33.70    30.00  20.90  615.09   \n102    20.10   41 36.30 38.40  23.20   31.00    29.20  18.40  418.84   \n70     24.30   62 35.50 38.60  22.40   31.50    27.30  18.60  392.40   \n131    22.70   40 36.30 38.50  23.00   31.20    28.40  17.10  415.98   \n13     21.20   30 39.40 41.50  23.70   36.90    31.60  18.80  591.26   \n55     22.60   54 39.90 38.00  22.00   35.90    30.20  18.90  544.50   \n54      3.90   42 37.80 34.90  22.50   27.70    27.50  18.50  275.02   \n112    22.10   47 40.20 38.10  23.90   35.30    31.10  19.80  453.90   \n27     22.90   31 38.80 36.00  21.00   29.20    26.60  17.00  324.50   \n144    10.30   23 38.00 37.60  23.20   31.80    29.70  18.30  456.78   \n173    16.90   36 38.70 37.70  21.50   32.40    28.40  17.80  434.46   \n155    17.30   28 35.60 37.80  21.70   32.20    27.70  17.70  390.86   \n169    16.50   35 37.60 39.10  23.40   32.50    29.80  17.40  429.39   \n64     32.30   57 40.10 41.20  24.70   35.30    31.10  18.40  603.29   \n179    16.90   39 42.80 43.10  25.80   39.10    32.50  19.90  739.70   \n89     14.10   48 36.70 39.90  24.40   28.80    29.60  18.70  424.33   \n241    35.00   65 38.80 42.10  23.40   34.90    30.10  19.40  738.47   \n140    24.90   40 37.40 39.60  21.60   30.80    27.90  16.60  440.01   \n51      6.60   40 34.30 34.90  21.00   26.70    26.10  17.20  281.02   \n57     28.00   62 40.50 39.80  22.70   37.70    30.90  19.20  582.76   \n216    13.60   51 34.80 36.50  21.50   31.30    26.30  17.80  319.36   \n167    20.30   35 43.90 41.70  24.60   37.20    33.10  19.80  699.14   \n87     23.10   64 36.50 39.50  23.30   29.20    28.40  18.10  389.35   \n90     20.50   46 37.20 38.20  22.50   29.10    27.70  17.70  447.56   \n202    28.70   43 37.90 38.30  23.70   32.10    28.90  18.70  562.24   \n188    20.50   41 40.80 41.30  24.80   36.60    32.40  18.80  564.21   \n84     27.00   72 38.50 36.60  22.00   29.70    26.30  18.00  407.57   \n40     34.50   45 43.20 39.60  26.60   36.40    32.70  21.40 1004.18   \n159    22.50   31 36.20 39.00  24.60   30.10    28.20  18.20  439.41   \n67     13.80   55 36.90 35.40  21.50   32.80    27.40  18.70  334.93   \n12     20.80   32 38.40 38.30  21.50   32.50    28.60  17.70  468.78   \n56     20.40   58 39.10 39.60  22.50   33.10    28.30  18.50  484.44   \n41     32.90   44 36.60 42.50  23.70   33.60    28.70  17.40 1424.58   \n117    13.90   51 41.00 38.80  23.30   33.40    29.80  19.50  445.01   \n97     11.30   50 38.70 39.30  23.30   30.60    27.80  18.20  397.09   \n124    13.80   50 37.70 36.60  23.50   34.40    29.20  18.00  389.79   \n157    10.00   28 37.00 38.50  25.00   31.60    28.00  18.60  460.99   \n187    20.40   41 38.50 39.80  23.50   36.40    30.40  19.10  615.42   \n17     22.90   32 42.10 40.00  24.40   38.20    31.60  19.30  616.70   \n38     35.20   46 51.20 49.10  29.60   45.00    29.00  21.40 1825.30   \n190    11.40   41 36.40 36.30  21.80   29.60    27.30  17.90  338.04   \n1       6.10   22 38.50 37.30  23.40   30.50    28.90  18.20  415.44   \n81     26.80   64 38.10 38.00  22.00   29.90    25.20  17.70  335.69   \n237    27.30   63 40.20 41.10  22.30   35.10    29.60  18.50  691.03   \n104    25.40   43 39.60 36.10  22.00   30.10    27.20  17.70  452.40   \n227    25.20   55 38.30 42.60  23.40   33.20    30.00  18.40  530.67   \n30     11.90   32 38.70 38.70  33.90   32.50    27.70  18.40  449.14   \n209    10.80   47 34.50 38.20  22.60   29.00    26.20  17.60  360.71   \n82     18.40   64 39.30 39.00  23.00   34.30    29.60  19.00  497.53   \n32     11.80   27 38.10 36.20  24.50   29.00    30.00  18.80  396.13   \n129    14.90   42 38.30 36.90  22.20   31.60    27.80  17.70  391.51   \n225    12.50   55 33.40 33.00  19.70   25.30    22.00  15.80  239.73   \n109    21.40   40 34.20 36.80  22.80   32.10    26.00  17.30  410.00   \n138    22.40   40 34.30 38.40  22.50   31.70    27.40  17.60  397.31   \n93     24.90   46 38.00 40.50  24.50   33.30    29.60  19.10  516.46   \n58     31.50   54 40.50 38.00  22.50   31.60    28.80  18.20  579.59   \n26      7.90   34 36.20 34.70  21.40   28.70    27.00  16.50  256.18   \n153    16.50   27 37.90 37.40  22.80   30.60    28.30  17.90  365.36   \n242    30.40   66 41.40 42.40  24.60   35.60    30.70  19.50  762.13   \n162    13.00   33 40.70 37.30  23.50   33.50    30.60  19.70  493.79   \n65     30.00   55 40.90 40.20  22.70   34.80    30.10  18.70  498.85   \n36     24.20   40 38.50 39.90  22.60   35.10    30.60  19.00  584.36   \n83     27.00   70 38.70 36.50  24.10   31.20    27.30  19.20  416.51   \n184    17.50   40 37.70 38.90  22.40   30.50    28.90  17.70  391.52   \n161    14.60   33 38.50 40.60  25.00   31.30    29.20  19.10  526.25   \n147    29.60   25 40.90 40.80  24.60   33.30    29.70  18.40  611.36   \n236    24.80   62 40.60 40.30  23.00   32.60    28.50  19.00  507.57   \n50     10.20   47 34.90 37.20  22.40   26.00    25.80  17.30  346.62   \n206    32.90   44 39.10 37.60  21.40   33.10    29.50  17.30  420.70   \n239    29.90   65 40.80 38.10  24.00   35.90    30.50  19.10  547.61   \n166    21.80   35 38.50 34.20  21.90   30.20    28.70  17.70  406.46   \n45     13.90   43 35.70 38.20  23.40   29.70    27.40  18.30  368.30   \n201    22.10   43 35.20 35.70  22.00   29.40    26.60  17.40  324.91   \n235    18.60   62 38.30 37.10  22.70   30.30    26.30  18.30  421.88   \n143     9.40   23 35.50 36.10  22.70   30.50    27.20  18.20  353.22   \n128    20.80   40 39.80 39.00  21.80   33.30    29.60  18.10  504.57   \n141    18.30   40 36.50 38.20  22.00   32.00    28.50  17.80  431.88   \n94      9.00   47 37.30 39.60  24.60   30.30    27.90  17.80  455.68   \n146    19.20   24 39.20 43.50  25.20   36.10    30.30  18.70  597.56   \n222    11.50   54 37.40 40.20  23.40   27.90    27.00  17.80  387.60   \n21     15.20   28 41.30 40.60  24.60   33.00    32.80  19.90  576.35   \n86     14.90   72 37.70 37.70  21.80   32.60    28.00  18.80  370.04   \n8       4.10   25 38.10 38.30  23.80   35.90    31.10  18.20  492.99   \n4      28.70   24 34.40 42.20  24.00   32.20    27.70  17.70  476.46   \n223     5.20   55 35.20 35.20  22.50   29.40    26.80  17.00  300.89   \n106    19.30   43 38.60 39.30  23.50   30.50    28.50  18.10  545.58   \n158    12.50   30 35.90 34.80  21.80   27.00    34.90  16.90  271.01   \n48     13.60   45 32.80 35.80  20.60   28.80    25.50  16.30  269.02   \n178    22.50   38 38.00 39.50  24.70   34.80    30.30  18.10  506.32   \n183    12.10   40 35.30 39.40  22.70   30.00    26.40  17.40  363.59   \n68      6.30   54 37.50 37.40  22.40   32.60    28.10  18.10  348.05   \n74     11.80   61 36.50 35.20  20.90   29.40    27.00  16.80  311.01   \n203     6.00   44 37.90 39.00  24.00   32.90    29.20  18.40  457.51   \n186    23.60   41 41.90 41.30  24.70   37.20    31.80  20.00  729.60   \n118    25.80   40 38.30 41.10  24.80   33.60    29.50  18.50  492.99   \n43     32.00   41 41.50 40.20  23.00   35.80    31.50  18.80  628.59   \n61     29.80   56 35.60 38.00  22.10   32.50    29.80  18.30  466.45   \n168    34.30   35 40.40 40.60  24.00   36.10    31.80  18.80  749.61   \n105    18.00   43 31.10 39.00  24.80   31.00    29.40  18.80  399.86   \n3      10.40   26 37.40 37.30  22.80   32.40    29.40  18.20  472.42   \n172    20.50   35 38.40 37.30  22.40   31.00    28.70  17.70  442.50   \n15     20.90   35 36.40 38.70  21.70   31.10    26.40  16.90  401.33   \n16     29.00   34 38.90 40.80  23.10   36.20    30.80  17.30  539.69   \n243    32.60   67 41.30 46.00  25.40   35.30    29.80  19.50  712.99   \n78     21.50   81 37.80 37.50  21.50   31.40    26.80  18.30  370.13   \n29      8.80   29 36.70 35.30  22.60   30.10    26.70  17.60  374.50   \n238    12.40   64 37.90 39.10  22.30   29.80    28.90  18.30  346.80   \n99     22.20   47 40.00 39.00  22.30   35.30    30.90  18.30  539.01   \n208     9.60   47 36.00 36.20  22.50   31.40    27.50  17.70  366.69   \n248    33.60   72 40.90 40.80  23.20   35.20    28.60  20.10  579.23   \n100    21.20   49 40.10 39.40  22.30   32.20    31.00  18.60  536.09   \n92      8.50   47 37.50 38.30  22.10   30.10    28.20  18.40  387.34   \n246    30.20   69 40.80 44.00  22.60   37.50    32.60  18.80  658.73   \n133    26.10   50 37.80 35.60  20.50   33.60    29.30  17.30  369.27   \n98     17.80   46 35.90 37.30  21.90   31.60    27.50  18.20  358.86   \n60     26.10   62 41.40 40.90  23.10   36.20    31.80  20.20  636.94   \n114    26.70   48 38.00 38.10  21.80   31.80    27.30  17.50  428.05   \n218    24.50   52 39.40 39.20  22.90   35.70    30.40  19.20  553.32   \n207    32.80   47 40.20 39.40  23.30   36.70    31.60  18.40  524.48   \n113    21.30   42 35.30 37.80  21.90   30.70    27.60  17.40  378.21   \n14     22.10   35 40.50 39.00  23.10   36.10    30.50  18.20  507.20   \n196    22.00   42 35.50 38.60  24.00   31.20    27.30  17.40  353.83   \n96      9.60   38 37.50 39.40  22.90   31.60    30.10  18.50  486.37   \n137    29.40   43 37.70 39.20  23.80   34.30    28.40  17.70  476.35   \n103    22.30   49 40.70 39.80  25.40   31.00    30.30  19.70  524.89   \n73     13.50   55 33.20 35.40  19.10   29.30    25.70  16.90  244.14   \n88      8.30   46 38.00 38.40  23.80   30.20    29.30  18.80  430.90   \n215    47.50   51 41.20 36.90  23.60   34.70    29.10  18.40  749.39   \n75     18.50   61 36.00 37.00  21.40   29.30    27.00  18.30  325.60   \n123    16.00   47 36.90 36.50  22.10   30.60    27.50  17.60  343.85   \n9      11.70   23 42.10 41.70  25.00   35.60    30.00  19.20  534.74   \n228    14.90   56 38.10 37.40  22.50   34.60    30.10  18.80  438.13   \n175     9.90   37 36.00 34.80  22.20   31.00    26.90  16.90  304.66   \n111    28.00   43 37.10 40.00  23.60   33.50    27.80  17.40  479.72   \n23     17.70   32 35.50 36.20  22.10   29.80    26.70  17.10  316.09   \n28      3.70   27 36.40 34.50  21.30   30.50    27.90  17.20  274.22   \n244    29.00   67 40.70 38.80  24.10   32.10    29.30  18.50  581.03   \n229    17.00   56 37.40 38.80  23.20   32.40    29.70  19.00  410.80   \n191    38.10   42 41.80 45.00  25.50   37.10    31.20  19.90  784.97   \n101    20.40   48 37.00 38.40  22.40   27.90    26.20  17.00  419.29   \n52      8.00   51 36.50 33.70  21.40   29.60    26.00  16.90  278.05   \n5      20.90   24 39.00 42.00  25.60   35.70    30.60  18.80  591.37   \n59     24.60   61 38.40 37.70  22.90   34.50    29.60  18.50  491.41   \n121    25.30   44 39.50 40.50  23.20   33.00    29.60  18.40  479.97   \n130    18.10   49 35.50 38.70  23.20   27.50    26.50  17.60  412.56   \n95     17.40   53 41.10 42.30  23.20   32.90    30.80  20.40  648.23   \n136    21.80   39 37.00 36.10  22.40   32.70    28.30  17.10  393.01   \n107    18.30   52 42.00 38.70  23.40   35.10    29.60  19.10  556.37   \n247    11.00   70 34.90 34.80  21.50   25.60    25.70  18.50  269.00   \n76      8.80   57 38.70 39.70  24.20   30.20    29.20  18.10  379.95   \n47      5.60   39 34.60 37.50  21.90   28.80    26.80  17.90  309.51   \n197    17.70   42 36.50 38.00  22.30   30.80    27.80  16.90  394.74   \n37     28.40   50 42.10 41.50  24.70   33.20    30.50  19.40  567.19   \n221    26.00   54 42.50 42.70  27.00   38.40    32.00  19.60  732.18   \n205    16.60   44 41.90 39.80  24.10   37.30    23.10  19.40  596.94   \n192    15.90   42 40.70 38.60  24.70   34.00    30.10  18.70  531.10   \n182    11.50   40 35.50 36.20  21.80   31.40    28.30  17.20  315.88   \n85     26.60   67 36.50 37.80  33.70   32.40    27.70  18.20  413.17   \n132    23.60   47 37.80 38.10  22.60   33.50    28.60  17.90  529.82   \n152    10.10   27 34.10 36.80  23.80   27.80    26.30  17.40  295.03   \n35     40.10   49 38.40 38.30  21.90   32.00    29.80  17.00  565.66   \n212    19.50   49 38.30 38.80  23.00   29.50    27.90  18.60  394.54   \n230    10.60   57 35.20 35.00  21.30   31.70    27.30  16.90  332.02   \n142    23.30   52 37.50 36.70  22.30   31.60    27.50  17.90  411.65   \n151    19.60   26 41.80 43.50  25.10   38.50    33.80  19.60  784.47   \n126    27.20   42 38.90 36.80  22.20   33.80    30.30  17.20  458.27   \n250    26.00   72 38.90 41.60  22.70   30.50    29.40  19.80  516.11   \n10      7.10   26 38.50 39.70  25.20   32.80    29.40  18.50  465.62   \n154    21.00   27 38.20 40.00  24.90   33.70    29.20  19.40  545.58   \n185     8.60   40 39.40 39.70  22.60   32.90    29.30  18.20  392.40   \n174    25.30   36 41.50 42.40  24.00   35.40    21.00  20.10  716.59   \n115    16.70   40 36.30 36.30  22.10   29.80    26.30  17.30  360.49   \n31      5.70   29 37.30 38.80  21.50   30.10    26.40  17.90  360.42   \n63     25.80   61 37.40 40.10  22.70   33.60    29.00  18.80  472.90   \n134    24.40   41 36.50 36.90  23.00   34.00    29.80  18.10  407.31   \n72      8.50   56 36.40 37.50  23.10   29.70    27.30  18.20  350.38   \n177    29.90   37 42.10 42.60  24.80   34.40    29.50  18.40  814.01   \n\n     StaticWaistRatio  ChestNeckRatio  ChestWaistRatio  HipThighRatio  \\\n53               0.99            2.66             1.17           1.74   \n249              0.69            2.86             1.00           1.69   \n164              0.79            2.82             1.05           1.71   \n214              0.89            2.64             1.13           1.68   \n245              0.93            2.68             1.16           1.74   \n193              0.83            2.77             1.01           1.71   \n66               0.90            2.53             1.07           1.71   \n69               0.90            2.60             1.12           1.84   \n91               0.88            2.60             1.09           1.71   \n18               0.89            2.81             1.19           1.60   \n24               1.00            2.61             1.18           1.64   \n39               0.78            2.86             1.06           1.67   \n213              0.88            2.66             1.06           1.74   \n234              0.80            2.41             1.04           1.73   \n19               0.82            2.66             1.06           1.66   \n180              0.82            2.71             1.04           1.61   \n194              0.92            2.60             1.08           1.60   \n232              0.91            2.64             1.14           1.71   \n110              0.87            2.59             1.06           1.60   \n80               0.84            2.54             1.02           1.77   \n6                0.87            2.89             1.16           1.72   \n163              0.87            2.48             1.07           1.71   \n148              0.98            2.62             1.21           1.77   \n0                0.90            2.57             1.09           1.60   \n77               0.83            2.64             1.07           1.79   \n150              0.95            2.62             1.20           1.64   \n120              0.84            2.56             1.05           1.78   \n226              0.88            2.73             1.12           1.72   \n7                0.92            2.63             1.13           1.62   \n198              1.04            2.50             1.21           1.72   \n49               1.04            2.45             1.18           1.72   \n127              0.95            2.56             1.23           1.74   \n44               0.94            2.70             1.12           1.76   \n149              0.80            2.81             1.07           1.68   \n160              0.91            2.69             1.16           1.74   \n219              0.85            2.50             1.06           1.76   \n20               0.82            2.64             1.08           1.65   \n42               0.76            3.04             1.02           1.69   \n33               0.88            2.81             1.11           1.61   \n122              0.95            2.69             1.19           1.61   \n165              0.91            2.65             1.13           1.61   \n125              0.84            2.76             1.12           1.65   \n220              0.88            2.57             1.08           1.67   \n189              0.81            2.72             1.02           1.68   \n135              0.77            2.77             1.03           1.73   \n34               0.77            2.78             1.01           1.63   \n170              0.93            2.49             1.13           1.70   \n210              0.93            2.55             1.15           1.74   \n156              0.80            2.74             1.01           1.55   \n145              0.93            2.60             1.13           1.69   \n116              0.88            2.61             1.07           1.70   \n217              0.98            2.53             1.14           1.73   \n79               0.82            2.75             1.04           1.77   \n240              0.88            2.68             1.17           1.73   \n139              0.80            2.57             0.98           1.62   \n25               0.96            2.51             1.12           1.75   \n195              0.84            2.64             1.05           1.65   \n204              0.70            2.97             1.07           1.69   \n11               0.93            2.63             1.14           1.63   \n224              0.82            2.60             1.12           1.71   \n211              0.79            2.88             1.11           1.71   \n62               0.80            2.83             1.05           1.63   \n108              0.95            2.86             1.24           1.78   \n176              0.92            2.62             1.11           1.61   \n233              0.79            2.70             1.00           1.76   \n231              0.83            2.62             1.07           1.70   \n119              0.93            2.68             1.16           1.73   \n200              0.91            2.72             1.15           1.65   \n22               0.97            2.54             1.13           1.65   \n2                0.90            2.82             1.09           1.66   \n181              1.01            2.35             1.14           1.80   \n71               0.94            2.29             1.07           1.62   \n199              0.91            2.77             1.16           1.61   \n46               1.01            2.62             1.20           1.66   \n171              0.98            2.67             1.21           1.78   \n251              0.81            2.75             1.04           1.81   \n102              0.90            2.66             1.08           1.64   \n70               0.87            2.75             1.07           1.74   \n131              0.87            2.61             1.05           1.64   \n13               0.83            2.64             1.02           1.65   \n55               0.79            2.70             1.08           1.74   \n54               0.98            2.32             1.13           1.71   \n112              0.86            2.48             1.05           1.58   \n27               0.83            2.51             1.10           1.65   \n144              0.93            2.54             1.13           1.73   \n173              0.85            2.54             1.09           1.69   \n155              0.92            2.59             1.10           1.72   \n169              0.88            2.64             1.09           1.63   \n64               0.80            2.63             1.00           1.67   \n179              0.85            2.56             1.05           1.58   \n89               0.96            2.63             1.12           1.63   \n241              0.72            3.08             1.01           1.86   \n140              0.84            2.64             1.06           1.72   \n51               0.94            2.60             1.15           1.77   \n57               0.78            2.75             1.07           1.71   \n216              0.93            2.67             1.14           1.79   \n167              0.86            2.46             1.08           1.69   \n87               0.89            2.86             1.15           1.62   \n90               0.82            2.68             1.04           1.75   \n202              0.78            2.83             1.04           1.53   \n188              0.87            2.68             1.11           1.62   \n84               0.77            2.63             1.02           1.71   \n40               0.69            2.97             1.02           1.73   \n159              0.89            2.79             1.09           1.67   \n67               0.87            2.59             1.10           1.69   \n12               0.85            2.66             1.11           1.64   \n56               0.81            2.56             1.00           1.65   \n41               0.80            2.90             1.02           1.64   \n117              0.91            2.42             1.10           1.70   \n97               0.93            2.57             1.15           1.55   \n124              0.93            2.62             1.18           1.61   \n157              0.90            2.66             1.08           1.69   \n187              0.83            2.79             1.09           1.64   \n17               0.86            2.56             1.10           1.60   \n38               0.68            2.66             0.92           1.69   \n190              0.94            2.51             1.13           1.70   \n1                0.95            2.43             1.13           1.68   \n81               0.87            2.55             1.09           1.77   \n237              0.72            2.93             1.03           1.76   \n104              0.77            2.63             1.05           1.67   \n227              0.87            2.75             1.09           1.67   \n30               1.03            2.60             1.13           1.74   \n209              0.93            2.69             1.10           1.68   \n82               0.83            2.62             1.05           1.69   \n32               1.01            2.44             1.18           1.65   \n129              0.88            2.51             1.11           1.70   \n225              0.88            2.66             1.14           1.72   \n109              0.83            2.86             1.06           1.75   \n138              0.89            2.87             1.11           1.69   \n93               0.86            2.81             1.09           1.71   \n58               0.75            2.85             1.10           1.64   \n26               0.97            2.45             1.19           1.65   \n153              0.89            2.48             1.07           1.68   \n242              0.79            2.89             1.10           1.71   \n162              0.87            2.43             1.07           1.62   \n65               0.81            2.52             1.03           1.61   \n36               0.81            2.77             1.06           1.67   \n83               0.84            2.63             1.07           1.70   \n184              0.87            2.62             1.09           1.72   \n161              0.89            2.70             1.09           1.71   \n147              0.83            2.71             1.10           1.55   \n236              0.84            2.56             1.06           1.70   \n50               0.89            2.58             1.04           1.87   \n206              0.81            2.57             1.07           1.70   \n239              0.81            2.61             1.06           1.70   \n166              0.82            2.57             1.10           1.72   \n45               0.98            2.71             1.19           1.66   \n201              0.88            2.59             1.06           1.75   \n235              0.82            2.73             1.10           1.72   \n143              1.00            2.59             1.19           1.67   \n128              0.84            2.61             1.11           1.61   \n141              0.84            2.73             1.07           1.64   \n94               0.92            2.67             1.12           1.77   \n146              0.88            2.60             1.03           1.55   \n222              0.93            2.52             1.08           1.60   \n21               0.86            2.70             1.13           1.65   \n86               0.89            2.59             1.11           1.69   \n8                0.97            2.65             1.22           1.59   \n4                0.84            2.83             0.97           1.61   \n223              0.90            2.63             1.12           1.69   \n106              0.79            2.73             1.02           1.69   \n158              0.96            2.47             1.16           1.79   \n48               0.87            2.81             1.11           1.74   \n178              0.89            2.70             1.11           1.57   \n183              0.92            2.61             1.06           1.66   \n68               0.99            2.38             1.14           1.72   \n74               0.88            2.56             1.12           1.68   \n203              0.91            2.66             1.13           1.69   \n186              0.79            2.80             1.08           1.61   \n118              0.91            2.49             1.03           1.61   \n43               0.79            2.57             1.02           1.63   \n61               0.83            2.89             1.09           1.66   \n168              0.72            2.84             0.99           1.50   \n105              0.95            2.99             1.07           1.77   \n3                0.91            2.72             1.18           1.68   \n172              0.86            2.62             1.11           1.71   \n15               0.83            2.72             1.07           1.57   \n16               0.84            2.62             1.06           1.62   \n243              0.80            2.80             1.02           1.67   \n78               0.81            2.55             1.01           1.86   \n29               0.90            2.65             1.17           1.68   \n238              0.96            2.53             1.16           1.54   \n99               0.85            2.69             1.14           1.65   \n208              0.91            2.77             1.19           1.73   \n248              0.80            2.65             1.03           1.75   \n100              0.85            2.66             1.12           1.72   \n92               0.95            2.59             1.17           1.68   \n246              0.79            2.79             1.06           1.74   \n133              0.82            2.66             1.12           1.65   \n98               0.88            2.65             1.08           1.70   \n60               0.80            2.71             1.07           1.67   \n114              0.84            2.65             1.09           1.64   \n218              0.81            2.71             1.07           1.64   \n207              0.80            2.55             1.01           1.68   \n113              0.86            2.65             1.04           1.62   \n14               0.83            2.50             1.05           1.45   \n196              0.93            2.75             1.14           1.67   \n96               0.88            2.64             1.08           1.69   \n137              0.82            2.59             0.99           1.58   \n103              0.89            2.54             1.08           1.72   \n73               0.94            2.64             1.15           1.74   \n88               0.94            2.56             1.13           1.63   \n215              0.65            2.91             0.98           1.80   \n75               0.94            2.54             1.12           1.74   \n123              0.89            2.55             1.09           1.64   \n9                0.97            2.37             1.12           1.65   \n228              0.88            2.73             1.16           1.68   \n175              0.93            2.69             1.22           1.77   \n111              0.77            2.91             1.03           1.62   \n23               0.94            2.44             1.08           1.70   \n28               0.99            2.57             1.27           1.77   \n244              0.77            2.91             1.11           1.75   \n229              0.87            2.64             1.06           1.75   \n191              0.80            2.76             1.01           1.64   \n101              0.85            2.68             1.08           1.66   \n52               0.88            2.46             1.09           1.81   \n5                0.92            2.68             1.11           1.63   \n59               0.80            2.73             1.07           1.64   \n121              0.84            2.51             1.01           1.78   \n130              0.88            2.75             1.09           1.68   \n95               0.87            2.75             1.14           1.74   \n136              0.88            2.51             1.08           1.63   \n107              0.80            2.62             1.08           1.80   \n247              0.89            2.56             1.07           1.79   \n76               1.04            2.37             1.16           1.66   \n47               0.97            2.60             1.13           1.76   \n197              0.86            2.52             1.03           1.62   \n37               0.87            2.51             1.07           1.59   \n221              0.81            2.82             1.09           1.64   \n205              0.87            2.52             1.10           1.61   \n192              0.87            2.58             1.11           1.69   \n182              0.90            2.69             1.14           1.69   \n85               1.00            2.71             1.10           1.76   \n132              0.79            2.74             1.04           1.69   \n152              1.07            2.60             1.22           1.70   \n35               0.68            3.09             1.05           1.84   \n212              0.90            2.57             1.10           1.76   \n230              0.85            2.83             1.15           1.70   \n142              0.85            2.74             1.13           1.73   \n151              0.86            2.59             1.05           1.57   \n126              0.83            2.54             1.07           1.62   \n250              0.83            2.78             1.07           1.75   \n10               1.00            2.64             1.21           1.64   \n154              0.84            2.65             1.01           1.69   \n185              0.96            2.27             1.07           1.71   \n174              0.80            2.78             1.06           1.65   \n115              0.87            2.67             1.12           1.66   \n31               0.93            2.51             1.11           1.72   \n63               0.82            2.82             1.06           1.64   \n134              0.89            2.70             1.13           1.76   \n72               0.95            2.57             1.13           1.82   \n177              0.78            2.83             1.08           1.63   \n\n     ForearmWristRatio  ForearmBicepRatio  \n53                1.48               2.08  \n249               1.51               1.74  \n164               1.69               1.84  \n214               1.48               1.59  \n245               1.48               1.58  \n193               1.62               1.82  \n66                1.57               1.70  \n69                1.57               1.58  \n91                1.51               1.67  \n18                1.65               2.01  \n24                1.59               1.77  \n39                1.69               1.86  \n213               1.58               1.72  \n234               1.57               1.71  \n19                1.65               2.04  \n180               1.53               1.87  \n194               1.58               1.85  \n232               1.64               1.72  \n110               1.56               1.84  \n80                1.43               1.55  \n6                 1.57               1.80  \n163               1.59               1.72  \n148               1.49               1.53  \n0                 1.60               1.87  \n77                1.37               1.64  \n150               1.63               1.78  \n120               1.60               1.74  \n226               1.56               1.78  \n7                 1.54               1.62  \n198               1.62               1.65  \n49                1.54               1.60  \n127               1.62               1.95  \n44                1.43               1.62  \n149               1.65               1.96  \n160               1.59               1.64  \n219               1.50               1.67  \n20                1.65               1.77  \n42                1.62               1.99  \n33                1.68               2.01  \n122               1.59               1.95  \n165               1.74               1.88  \n125               1.72               2.02  \n220               1.58               1.66  \n189               1.58               1.81  \n135               1.64               1.76  \n34                1.61               1.89  \n170               1.55               1.72  \n210               1.66               1.78  \n156               1.61               1.84  \n145               1.64               1.94  \n116               1.55               1.65  \n217               1.39               1.48  \n79                1.51               1.59  \n240               1.50               1.73  \n139               1.59               1.90  \n25                1.59               1.69  \n195               1.59               1.75  \n204               1.76               2.00  \n11                1.59               1.96  \n224               1.62               1.78  \n211               1.64               1.95  \n62                1.57               1.71  \n108               1.60               1.83  \n176               1.59               1.89  \n233               1.48               1.58  \n231               1.53               1.67  \n119               1.59               1.78  \n200               1.53               1.70  \n22                1.55               1.67  \n2                 1.52               1.73  \n181               1.49               1.68  \n71                1.49               1.66  \n199               1.53               1.80  \n46                1.51               1.61  \n171               1.53               1.47  \n251               1.44               1.61  \n102               1.59               1.68  \n70                1.47               1.69  \n131               1.66               1.82  \n13                1.68               1.96  \n55                1.60               1.90  \n54                1.49               1.50  \n112               1.57               1.78  \n27                1.56               1.72  \n144               1.62               1.74  \n173               1.60               1.82  \n155               1.56               1.82  \n169               1.71               1.87  \n64                1.69               1.92  \n179               1.63               1.96  \n89                1.58               1.54  \n241               1.55               1.80  \n140               1.68               1.86  \n51                1.52               1.55  \n57                1.61               1.96  \n216               1.48               1.76  \n167               1.67               1.88  \n87                1.57               1.61  \n90                1.56               1.64  \n202               1.55               1.72  \n188               1.72               1.95  \n84                1.46               1.65  \n40                1.53               1.70  \n159               1.55               1.65  \n67                1.47               1.75  \n12                1.62               1.84  \n56                1.53               1.79  \n41                1.65               1.93  \n117               1.53               1.71  \n97                1.53               1.68  \n124               1.62               1.91  \n157               1.51               1.70  \n187               1.59               1.91  \n17                1.64               1.98  \n38                1.36               2.10  \n190               1.53               1.65  \n1                 1.59               1.68  \n81                1.42               1.69  \n237               1.60               1.90  \n104               1.54               1.70  \n227               1.63               1.80  \n30                1.51               1.77  \n209               1.49               1.65  \n82                1.56               1.81  \n32                1.60               1.54  \n129               1.57               1.79  \n225               1.39               1.60  \n109               1.50               1.86  \n138               1.56               1.80  \n93                1.55               1.74  \n58                1.58               1.74  \n26                1.64               1.74  \n153               1.58               1.71  \n242               1.57               1.83  \n162               1.55               1.70  \n65                1.61               1.86  \n36                1.61               1.85  \n83                1.42               1.62  \n184               1.63               1.72  \n161               1.53               1.64  \n147               1.61               1.81  \n236               1.50               1.72  \n50                1.49               1.50  \n206               1.71               1.91  \n239               1.60               1.88  \n166               1.62               1.71  \n45                1.50               1.62  \n201               1.53               1.69  \n235               1.44               1.66  \n143               1.49               1.68  \n128               1.64               1.84  \n141               1.60               1.80  \n94                1.57               1.70  \n146               1.62               1.93  \n222               1.52               1.57  \n21                1.65               1.66  \n86                1.49               1.73  \n8                 1.71               1.97  \n4                 1.56               1.82  \n223               1.58               1.73  \n106               1.57               1.69  \n158               2.07               1.60  \n48                1.56               1.77  \n178               1.67               1.92  \n183               1.52               1.72  \n68                1.55               1.80  \n74                1.61               1.75  \n203               1.59               1.79  \n186               1.59               1.86  \n118               1.59               1.82  \n43                1.68               1.90  \n61                1.63               1.78  \n168               1.69               1.92  \n105               1.56               1.65  \n3                 1.62               1.78  \n172               1.62               1.75  \n15                1.56               1.84  \n16                1.78               2.09  \n243               1.53               1.81  \n78                1.46               1.72  \n29                1.52               1.71  \n238               1.58               1.63  \n99                1.69               1.93  \n208               1.55               1.77  \n248               1.42               1.75  \n100               1.67               1.73  \n92                1.53               1.64  \n246               1.73               1.99  \n133               1.69               1.94  \n98                1.51               1.74  \n60                1.57               1.79  \n114               1.56               1.82  \n218               1.58               1.86  \n207               1.72               1.99  \n113               1.59               1.76  \n14                1.68               1.98  \n196               1.57               1.79  \n96                1.63               1.71  \n137               1.60               1.94  \n103               1.54               1.57  \n73                1.52               1.73  \n88                1.56               1.61  \n215               1.58               1.89  \n75                1.48               1.60  \n123               1.56               1.74  \n9                 1.56               1.85  \n228               1.60               1.84  \n175               1.59               1.83  \n111               1.60               1.93  \n23                1.56               1.74  \n28                1.62               1.77  \n244               1.58               1.74  \n229               1.56               1.71  \n191               1.57               1.86  \n101               1.54               1.64  \n52                1.54               1.75  \n5                 1.63               1.90  \n59                1.60               1.86  \n121               1.61               1.79  \n130               1.51               1.56  \n95                1.51               1.61  \n136               1.65               1.91  \n107               1.55               1.84  \n247               1.39               1.38  \n76                1.61               1.67  \n47                1.50               1.61  \n197               1.64               1.82  \n37                1.57               1.71  \n221               1.63               1.96  \n205               1.19               1.92  \n192               1.61               1.82  \n182               1.65               1.83  \n85                1.52               1.78  \n132               1.60               1.87  \n152               1.51               1.60  \n35                1.75               1.88  \n212               1.50               1.59  \n230               1.62               1.88  \n142               1.54               1.77  \n151               1.72               1.96  \n126               1.76               1.97  \n250               1.48               1.54  \n10                1.59               1.77  \n154               1.51               1.74  \n185               1.61               1.81  \n174               1.04               1.76  \n115               1.52               1.72  \n31                1.47               1.68  \n63                1.54               1.79  \n134               1.65               1.88  \n72                1.50               1.63  \n177               1.60               1.87  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BodyFat</th>\n      <th>Age</th>\n      <th>Neck</th>\n      <th>Knee</th>\n      <th>Ankle</th>\n      <th>Biceps</th>\n      <th>Forearm</th>\n      <th>Wrist</th>\n      <th>BMI</th>\n      <th>StaticWaistRatio</th>\n      <th>ChestNeckRatio</th>\n      <th>ChestWaistRatio</th>\n      <th>HipThighRatio</th>\n      <th>ForearmWristRatio</th>\n      <th>ForearmBicepRatio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>53</th>\n      <td>6.30</td>\n      <td>49</td>\n      <td>35.10</td>\n      <td>37.60</td>\n      <td>22.60</td>\n      <td>38.50</td>\n      <td>27.40</td>\n      <td>18.50</td>\n      <td>317.45</td>\n      <td>0.99</td>\n      <td>2.66</td>\n      <td>1.17</td>\n      <td>1.74</td>\n      <td>1.48</td>\n      <td>2.08</td>\n    </tr>\n    <tr>\n      <th>249</th>\n      <td>29.30</td>\n      <td>72</td>\n      <td>38.90</td>\n      <td>37.30</td>\n      <td>21.50</td>\n      <td>31.30</td>\n      <td>27.20</td>\n      <td>18.00</td>\n      <td>528.42</td>\n      <td>0.69</td>\n      <td>2.86</td>\n      <td>1.00</td>\n      <td>1.69</td>\n      <td>1.51</td>\n      <td>1.74</td>\n    </tr>\n    <tr>\n      <th>164</th>\n      <td>27.30</td>\n      <td>34</td>\n      <td>39.50</td>\n      <td>42.00</td>\n      <td>23.40</td>\n      <td>34.00</td>\n      <td>31.20</td>\n      <td>18.50</td>\n      <td>664.61</td>\n      <td>0.79</td>\n      <td>2.82</td>\n      <td>1.05</td>\n      <td>1.71</td>\n      <td>1.69</td>\n      <td>1.84</td>\n    </tr>\n    <tr>\n      <th>214</th>\n      <td>19.50</td>\n      <td>50</td>\n      <td>37.40</td>\n      <td>38.10</td>\n      <td>21.80</td>\n      <td>28.60</td>\n      <td>26.70</td>\n      <td>18.00</td>\n      <td>408.80</td>\n      <td>0.89</td>\n      <td>2.64</td>\n      <td>1.13</td>\n      <td>1.68</td>\n      <td>1.48</td>\n      <td>1.59</td>\n    </tr>\n    <tr>\n      <th>245</th>\n      <td>15.20</td>\n      <td>68</td>\n      <td>36.30</td>\n      <td>37.50</td>\n      <td>22.60</td>\n      <td>29.20</td>\n      <td>27.30</td>\n      <td>18.50</td>\n      <td>349.17</td>\n      <td>0.93</td>\n      <td>2.68</td>\n      <td>1.16</td>\n      <td>1.74</td>\n      <td>1.48</td>\n      <td>1.58</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>24.70</td>\n      <td>42</td>\n      <td>38.50</td>\n      <td>43.30</td>\n      <td>26.00</td>\n      <td>33.70</td>\n      <td>29.90</td>\n      <td>18.50</td>\n      <td>675.75</td>\n      <td>0.83</td>\n      <td>2.77</td>\n      <td>1.01</td>\n      <td>1.71</td>\n      <td>1.62</td>\n      <td>1.82</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>21.50</td>\n      <td>54</td>\n      <td>35.60</td>\n      <td>36.10</td>\n      <td>21.70</td>\n      <td>29.60</td>\n      <td>27.40</td>\n      <td>17.40</td>\n      <td>324.41</td>\n      <td>0.90</td>\n      <td>2.53</td>\n      <td>1.07</td>\n      <td>1.71</td>\n      <td>1.57</td>\n      <td>1.70</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>12.90</td>\n      <td>55</td>\n      <td>36.30</td>\n      <td>37.40</td>\n      <td>21.60</td>\n      <td>27.30</td>\n      <td>27.10</td>\n      <td>17.30</td>\n      <td>343.64</td>\n      <td>0.90</td>\n      <td>2.60</td>\n      <td>1.12</td>\n      <td>1.84</td>\n      <td>1.57</td>\n      <td>1.58</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>18.20</td>\n      <td>44</td>\n      <td>39.20</td>\n      <td>39.70</td>\n      <td>23.10</td>\n      <td>31.40</td>\n      <td>28.40</td>\n      <td>18.80</td>\n      <td>464.89</td>\n      <td>0.88</td>\n      <td>2.60</td>\n      <td>1.09</td>\n      <td>1.71</td>\n      <td>1.51</td>\n      <td>1.67</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>16.00</td>\n      <td>28</td>\n      <td>38.00</td>\n      <td>38.70</td>\n      <td>22.90</td>\n      <td>37.20</td>\n      <td>30.50</td>\n      <td>18.50</td>\n      <td>498.36</td>\n      <td>0.89</td>\n      <td>2.81</td>\n      <td>1.19</td>\n      <td>1.60</td>\n      <td>1.65</td>\n      <td>2.01</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>14.00</td>\n      <td>28</td>\n      <td>34.50</td>\n      <td>35.50</td>\n      <td>22.90</td>\n      <td>31.10</td>\n      <td>28.00</td>\n      <td>17.60</td>\n      <td>337.66</td>\n      <td>1.00</td>\n      <td>2.61</td>\n      <td>1.18</td>\n      <td>1.64</td>\n      <td>1.59</td>\n      <td>1.77</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>32.60</td>\n      <td>50</td>\n      <td>40.20</td>\n      <td>41.10</td>\n      <td>24.70</td>\n      <td>34.10</td>\n      <td>31.00</td>\n      <td>18.30</td>\n      <td>615.06</td>\n      <td>0.78</td>\n      <td>2.86</td>\n      <td>1.06</td>\n      <td>1.67</td>\n      <td>1.69</td>\n      <td>1.86</td>\n    </tr>\n    <tr>\n      <th>213</th>\n      <td>18.70</td>\n      <td>50</td>\n      <td>39.00</td>\n      <td>40.90</td>\n      <td>25.50</td>\n      <td>32.70</td>\n      <td>30.00</td>\n      <td>19.00</td>\n      <td>536.08</td>\n      <td>0.88</td>\n      <td>2.66</td>\n      <td>1.06</td>\n      <td>1.74</td>\n      <td>1.58</td>\n      <td>1.72</td>\n    </tr>\n    <tr>\n      <th>234</th>\n      <td>25.80</td>\n      <td>60</td>\n      <td>40.40</td>\n      <td>35.70</td>\n      <td>21.00</td>\n      <td>31.30</td>\n      <td>28.70</td>\n      <td>18.30</td>\n      <td>368.67</td>\n      <td>0.80</td>\n      <td>2.41</td>\n      <td>1.04</td>\n      <td>1.73</td>\n      <td>1.57</td>\n      <td>1.71</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>16.50</td>\n      <td>33</td>\n      <td>40.00</td>\n      <td>40.60</td>\n      <td>24.00</td>\n      <td>37.10</td>\n      <td>30.10</td>\n      <td>18.20</td>\n      <td>610.04</td>\n      <td>0.82</td>\n      <td>2.66</td>\n      <td>1.06</td>\n      <td>1.66</td>\n      <td>1.65</td>\n      <td>2.04</td>\n    </tr>\n    <tr>\n      <th>180</th>\n      <td>26.60</td>\n      <td>39</td>\n      <td>40.00</td>\n      <td>42.80</td>\n      <td>24.10</td>\n      <td>35.60</td>\n      <td>29.00</td>\n      <td>19.00</td>\n      <td>647.41</td>\n      <td>0.82</td>\n      <td>2.71</td>\n      <td>1.04</td>\n      <td>1.61</td>\n      <td>1.53</td>\n      <td>1.87</td>\n    </tr>\n    <tr>\n      <th>194</th>\n      <td>22.80</td>\n      <td>42</td>\n      <td>35.40</td>\n      <td>38.90</td>\n      <td>22.40</td>\n      <td>31.70</td>\n      <td>27.10</td>\n      <td>17.10</td>\n      <td>364.09</td>\n      <td>0.92</td>\n      <td>2.60</td>\n      <td>1.08</td>\n      <td>1.60</td>\n      <td>1.58</td>\n      <td>1.85</td>\n    </tr>\n    <tr>\n      <th>232</th>\n      <td>15.40</td>\n      <td>58</td>\n      <td>38.00</td>\n      <td>38.90</td>\n      <td>23.60</td>\n      <td>30.90</td>\n      <td>29.60</td>\n      <td>18.00</td>\n      <td>430.77</td>\n      <td>0.91</td>\n      <td>2.64</td>\n      <td>1.14</td>\n      <td>1.71</td>\n      <td>1.64</td>\n      <td>1.72</td>\n    </tr>\n    <tr>\n      <th>110</th>\n      <td>19.70</td>\n      <td>43</td>\n      <td>37.20</td>\n      <td>38.00</td>\n      <td>22.30</td>\n      <td>33.30</td>\n      <td>28.20</td>\n      <td>18.10</td>\n      <td>425.63</td>\n      <td>0.87</td>\n      <td>2.59</td>\n      <td>1.06</td>\n      <td>1.60</td>\n      <td>1.56</td>\n      <td>1.84</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>31.40</td>\n      <td>67</td>\n      <td>38.40</td>\n      <td>38.20</td>\n      <td>23.70</td>\n      <td>29.40</td>\n      <td>27.20</td>\n      <td>19.00</td>\n      <td>395.78</td>\n      <td>0.84</td>\n      <td>2.54</td>\n      <td>1.02</td>\n      <td>1.77</td>\n      <td>1.43</td>\n      <td>1.55</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>19.20</td>\n      <td>26</td>\n      <td>36.40</td>\n      <td>38.30</td>\n      <td>22.90</td>\n      <td>31.90</td>\n      <td>27.80</td>\n      <td>17.70</td>\n      <td>469.69</td>\n      <td>0.87</td>\n      <td>2.89</td>\n      <td>1.16</td>\n      <td>1.72</td>\n      <td>1.57</td>\n      <td>1.80</td>\n    </tr>\n    <tr>\n      <th>163</th>\n      <td>15.10</td>\n      <td>34</td>\n      <td>36.00</td>\n      <td>35.60</td>\n      <td>20.40</td>\n      <td>28.30</td>\n      <td>26.20</td>\n      <td>16.50</td>\n      <td>278.01</td>\n      <td>0.87</td>\n      <td>2.48</td>\n      <td>1.07</td>\n      <td>1.71</td>\n      <td>1.59</td>\n      <td>1.72</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>5.30</td>\n      <td>25</td>\n      <td>35.20</td>\n      <td>35.70</td>\n      <td>22.00</td>\n      <td>25.80</td>\n      <td>25.20</td>\n      <td>16.90</td>\n      <td>285.02</td>\n      <td>0.98</td>\n      <td>2.62</td>\n      <td>1.21</td>\n      <td>1.77</td>\n      <td>1.49</td>\n      <td>1.53</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>12.30</td>\n      <td>23</td>\n      <td>36.20</td>\n      <td>37.30</td>\n      <td>21.90</td>\n      <td>32.00</td>\n      <td>27.40</td>\n      <td>17.10</td>\n      <td>351.19</td>\n      <td>0.90</td>\n      <td>2.57</td>\n      <td>1.09</td>\n      <td>1.60</td>\n      <td>1.60</td>\n      <td>1.87</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>22.20</td>\n      <td>69</td>\n      <td>38.70</td>\n      <td>38.30</td>\n      <td>21.80</td>\n      <td>30.80</td>\n      <td>25.70</td>\n      <td>18.80</td>\n      <td>461.24</td>\n      <td>0.83</td>\n      <td>2.64</td>\n      <td>1.07</td>\n      <td>1.79</td>\n      <td>1.37</td>\n      <td>1.64</td>\n    </tr>\n    <tr>\n      <th>150</th>\n      <td>9.40</td>\n      <td>26</td>\n      <td>35.40</td>\n      <td>35.90</td>\n      <td>20.40</td>\n      <td>31.60</td>\n      <td>29.00</td>\n      <td>17.80</td>\n      <td>335.94</td>\n      <td>0.95</td>\n      <td>2.62</td>\n      <td>1.20</td>\n      <td>1.64</td>\n      <td>1.63</td>\n      <td>1.78</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>27.90</td>\n      <td>52</td>\n      <td>40.80</td>\n      <td>39.30</td>\n      <td>24.60</td>\n      <td>33.90</td>\n      <td>31.20</td>\n      <td>19.50</td>\n      <td>572.38</td>\n      <td>0.84</td>\n      <td>2.56</td>\n      <td>1.05</td>\n      <td>1.78</td>\n      <td>1.60</td>\n      <td>1.74</td>\n    </tr>\n    <tr>\n      <th>226</th>\n      <td>14.80</td>\n      <td>55</td>\n      <td>37.20</td>\n      <td>38.50</td>\n      <td>22.60</td>\n      <td>33.40</td>\n      <td>29.30</td>\n      <td>18.80</td>\n      <td>420.96</td>\n      <td>0.88</td>\n      <td>2.73</td>\n      <td>1.12</td>\n      <td>1.72</td>\n      <td>1.56</td>\n      <td>1.78</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>12.40</td>\n      <td>25</td>\n      <td>37.80</td>\n      <td>39.40</td>\n      <td>23.20</td>\n      <td>30.50</td>\n      <td>29.00</td>\n      <td>18.80</td>\n      <td>427.26</td>\n      <td>0.92</td>\n      <td>2.63</td>\n      <td>1.13</td>\n      <td>1.62</td>\n      <td>1.54</td>\n      <td>1.62</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>6.60</td>\n      <td>42</td>\n      <td>37.60</td>\n      <td>40.00</td>\n      <td>22.50</td>\n      <td>30.60</td>\n      <td>30.00</td>\n      <td>18.50</td>\n      <td>384.50</td>\n      <td>1.04</td>\n      <td>2.50</td>\n      <td>1.21</td>\n      <td>1.72</td>\n      <td>1.62</td>\n      <td>1.65</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>4.00</td>\n      <td>47</td>\n      <td>34.00</td>\n      <td>34.40</td>\n      <td>21.90</td>\n      <td>26.80</td>\n      <td>25.80</td>\n      <td>16.80</td>\n      <td>243.54</td>\n      <td>1.04</td>\n      <td>2.45</td>\n      <td>1.18</td>\n      <td>1.72</td>\n      <td>1.54</td>\n      <td>1.60</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>17.40</td>\n      <td>43</td>\n      <td>37.50</td>\n      <td>35.80</td>\n      <td>20.80</td>\n      <td>33.90</td>\n      <td>28.20</td>\n      <td>17.40</td>\n      <td>342.14</td>\n      <td>0.95</td>\n      <td>2.56</td>\n      <td>1.23</td>\n      <td>1.74</td>\n      <td>1.62</td>\n      <td>1.95</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>7.70</td>\n      <td>39</td>\n      <td>31.50</td>\n      <td>34.70</td>\n      <td>21.00</td>\n      <td>26.10</td>\n      <td>23.10</td>\n      <td>16.10</td>\n      <td>230.70</td>\n      <td>0.94</td>\n      <td>2.70</td>\n      <td>1.12</td>\n      <td>1.76</td>\n      <td>1.43</td>\n      <td>1.62</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>25.20</td>\n      <td>26</td>\n      <td>40.60</td>\n      <td>42.70</td>\n      <td>24.70</td>\n      <td>36.00</td>\n      <td>30.40</td>\n      <td>18.40</td>\n      <td>707.89</td>\n      <td>0.80</td>\n      <td>2.81</td>\n      <td>1.07</td>\n      <td>1.68</td>\n      <td>1.65</td>\n      <td>1.96</td>\n    </tr>\n    <tr>\n      <th>160</th>\n      <td>9.40</td>\n      <td>31</td>\n      <td>35.00</td>\n      <td>36.60</td>\n      <td>21.00</td>\n      <td>27.00</td>\n      <td>26.30</td>\n      <td>16.50</td>\n      <td>316.63</td>\n      <td>0.91</td>\n      <td>2.69</td>\n      <td>1.16</td>\n      <td>1.74</td>\n      <td>1.59</td>\n      <td>1.64</td>\n    </tr>\n    <tr>\n      <th>219</th>\n      <td>15.00</td>\n      <td>53</td>\n      <td>37.60</td>\n      <td>36.20</td>\n      <td>22.00</td>\n      <td>28.50</td>\n      <td>25.70</td>\n      <td>17.10</td>\n      <td>344.70</td>\n      <td>0.85</td>\n      <td>2.50</td>\n      <td>1.06</td>\n      <td>1.76</td>\n      <td>1.50</td>\n      <td>1.67</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>19.10</td>\n      <td>28</td>\n      <td>39.10</td>\n      <td>38.00</td>\n      <td>22.10</td>\n      <td>32.50</td>\n      <td>30.30</td>\n      <td>18.40</td>\n      <td>471.19</td>\n      <td>0.82</td>\n      <td>2.64</td>\n      <td>1.08</td>\n      <td>1.65</td>\n      <td>1.65</td>\n      <td>1.77</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>31.60</td>\n      <td>48</td>\n      <td>37.30</td>\n      <td>40.90</td>\n      <td>25.00</td>\n      <td>36.70</td>\n      <td>29.80</td>\n      <td>18.40</td>\n      <td>672.70</td>\n      <td>0.76</td>\n      <td>3.04</td>\n      <td>1.02</td>\n      <td>1.69</td>\n      <td>1.62</td>\n      <td>1.99</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>21.30</td>\n      <td>41</td>\n      <td>39.80</td>\n      <td>44.20</td>\n      <td>25.20</td>\n      <td>37.50</td>\n      <td>31.50</td>\n      <td>18.70</td>\n      <td>672.43</td>\n      <td>0.88</td>\n      <td>2.81</td>\n      <td>1.11</td>\n      <td>1.61</td>\n      <td>1.68</td>\n      <td>2.01</td>\n    </tr>\n    <tr>\n      <th>122</th>\n      <td>14.70</td>\n      <td>40</td>\n      <td>36.90</td>\n      <td>38.70</td>\n      <td>22.60</td>\n      <td>34.40</td>\n      <td>28.00</td>\n      <td>17.60</td>\n      <td>373.53</td>\n      <td>0.95</td>\n      <td>2.69</td>\n      <td>1.19</td>\n      <td>1.61</td>\n      <td>1.59</td>\n      <td>1.95</td>\n    </tr>\n    <tr>\n      <th>165</th>\n      <td>19.20</td>\n      <td>35</td>\n      <td>40.50</td>\n      <td>41.30</td>\n      <td>25.60</td>\n      <td>36.40</td>\n      <td>33.70</td>\n      <td>19.40</td>\n      <td>638.49</td>\n      <td>0.91</td>\n      <td>2.65</td>\n      <td>1.13</td>\n      <td>1.61</td>\n      <td>1.74</td>\n      <td>1.88</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>17.50</td>\n      <td>46</td>\n      <td>36.60</td>\n      <td>36.00</td>\n      <td>21.90</td>\n      <td>35.60</td>\n      <td>30.20</td>\n      <td>17.60</td>\n      <td>416.25</td>\n      <td>0.84</td>\n      <td>2.76</td>\n      <td>1.12</td>\n      <td>1.65</td>\n      <td>1.72</td>\n      <td>2.02</td>\n    </tr>\n    <tr>\n      <th>220</th>\n      <td>12.40</td>\n      <td>54</td>\n      <td>38.50</td>\n      <td>38.10</td>\n      <td>23.90</td>\n      <td>31.40</td>\n      <td>29.90</td>\n      <td>18.90</td>\n      <td>333.13</td>\n      <td>0.88</td>\n      <td>2.57</td>\n      <td>1.08</td>\n      <td>1.67</td>\n      <td>1.58</td>\n      <td>1.66</td>\n    </tr>\n    <tr>\n      <th>189</th>\n      <td>24.40</td>\n      <td>41</td>\n      <td>38.00</td>\n      <td>40.40</td>\n      <td>22.90</td>\n      <td>33.40</td>\n      <td>29.20</td>\n      <td>18.50</td>\n      <td>501.47</td>\n      <td>0.81</td>\n      <td>2.72</td>\n      <td>1.02</td>\n      <td>1.68</td>\n      <td>1.58</td>\n      <td>1.81</td>\n    </tr>\n    <tr>\n      <th>135</th>\n      <td>27.10</td>\n      <td>44</td>\n      <td>37.80</td>\n      <td>37.90</td>\n      <td>22.70</td>\n      <td>30.90</td>\n      <td>28.80</td>\n      <td>17.60</td>\n      <td>496.00</td>\n      <td>0.77</td>\n      <td>2.77</td>\n      <td>1.03</td>\n      <td>1.73</td>\n      <td>1.64</td>\n      <td>1.76</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>32.30</td>\n      <td>41</td>\n      <td>42.10</td>\n      <td>43.30</td>\n      <td>26.30</td>\n      <td>37.30</td>\n      <td>31.70</td>\n      <td>19.70</td>\n      <td>831.74</td>\n      <td>0.77</td>\n      <td>2.78</td>\n      <td>1.01</td>\n      <td>1.63</td>\n      <td>1.61</td>\n      <td>1.89</td>\n    </tr>\n    <tr>\n      <th>170</th>\n      <td>3.00</td>\n      <td>35</td>\n      <td>37.00</td>\n      <td>36.20</td>\n      <td>22.10</td>\n      <td>30.40</td>\n      <td>27.40</td>\n      <td>17.70</td>\n      <td>342.14</td>\n      <td>0.93</td>\n      <td>2.49</td>\n      <td>1.13</td>\n      <td>1.70</td>\n      <td>1.55</td>\n      <td>1.72</td>\n    </tr>\n    <tr>\n      <th>210</th>\n      <td>7.10</td>\n      <td>49</td>\n      <td>35.80</td>\n      <td>35.00</td>\n      <td>21.70</td>\n      <td>30.90</td>\n      <td>28.80</td>\n      <td>17.40</td>\n      <td>290.30</td>\n      <td>0.93</td>\n      <td>2.55</td>\n      <td>1.15</td>\n      <td>1.74</td>\n      <td>1.66</td>\n      <td>1.78</td>\n    </tr>\n    <tr>\n      <th>156</th>\n      <td>31.20</td>\n      <td>28</td>\n      <td>38.50</td>\n      <td>40.00</td>\n      <td>25.20</td>\n      <td>35.20</td>\n      <td>30.70</td>\n      <td>19.10</td>\n      <td>613.52</td>\n      <td>0.80</td>\n      <td>2.74</td>\n      <td>1.01</td>\n      <td>1.55</td>\n      <td>1.61</td>\n      <td>1.84</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>14.20</td>\n      <td>24</td>\n      <td>35.70</td>\n      <td>36.50</td>\n      <td>22.00</td>\n      <td>33.50</td>\n      <td>28.30</td>\n      <td>17.30</td>\n      <td>343.97</td>\n      <td>0.93</td>\n      <td>2.60</td>\n      <td>1.13</td>\n      <td>1.69</td>\n      <td>1.64</td>\n      <td>1.94</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>20.10</td>\n      <td>48</td>\n      <td>36.80</td>\n      <td>38.40</td>\n      <td>22.80</td>\n      <td>29.90</td>\n      <td>28.00</td>\n      <td>18.10</td>\n      <td>431.86</td>\n      <td>0.88</td>\n      <td>2.61</td>\n      <td>1.07</td>\n      <td>1.70</td>\n      <td>1.55</td>\n      <td>1.65</td>\n    </tr>\n    <tr>\n      <th>217</th>\n      <td>7.50</td>\n      <td>51</td>\n      <td>36.90</td>\n      <td>39.00</td>\n      <td>22.60</td>\n      <td>27.50</td>\n      <td>25.90</td>\n      <td>18.60</td>\n      <td>341.00</td>\n      <td>0.98</td>\n      <td>2.53</td>\n      <td>1.14</td>\n      <td>1.73</td>\n      <td>1.39</td>\n      <td>1.48</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>18.80</td>\n      <td>66</td>\n      <td>37.40</td>\n      <td>39.30</td>\n      <td>22.70</td>\n      <td>30.30</td>\n      <td>28.70</td>\n      <td>19.00</td>\n      <td>423.49</td>\n      <td>0.82</td>\n      <td>2.75</td>\n      <td>1.04</td>\n      <td>1.77</td>\n      <td>1.51</td>\n      <td>1.59</td>\n    </tr>\n    <tr>\n      <th>240</th>\n      <td>17.00</td>\n      <td>65</td>\n      <td>34.70</td>\n      <td>33.40</td>\n      <td>20.10</td>\n      <td>28.50</td>\n      <td>24.80</td>\n      <td>16.50</td>\n      <td>247.24</td>\n      <td>0.88</td>\n      <td>2.68</td>\n      <td>1.17</td>\n      <td>1.73</td>\n      <td>1.50</td>\n      <td>1.73</td>\n    </tr>\n    <tr>\n      <th>139</th>\n      <td>20.40</td>\n      <td>49</td>\n      <td>40.80</td>\n      <td>42.50</td>\n      <td>24.50</td>\n      <td>35.50</td>\n      <td>29.80</td>\n      <td>18.70</td>\n      <td>603.50</td>\n      <td>0.80</td>\n      <td>2.57</td>\n      <td>0.98</td>\n      <td>1.62</td>\n      <td>1.59</td>\n      <td>1.90</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>3.70</td>\n      <td>27</td>\n      <td>35.70</td>\n      <td>36.70</td>\n      <td>22.50</td>\n      <td>29.90</td>\n      <td>28.20</td>\n      <td>17.70</td>\n      <td>354.69</td>\n      <td>0.96</td>\n      <td>2.51</td>\n      <td>1.12</td>\n      <td>1.75</td>\n      <td>1.59</td>\n      <td>1.69</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>25.50</td>\n      <td>42</td>\n      <td>38.50</td>\n      <td>38.40</td>\n      <td>24.10</td>\n      <td>32.90</td>\n      <td>29.80</td>\n      <td>18.80</td>\n      <td>474.73</td>\n      <td>0.84</td>\n      <td>2.64</td>\n      <td>1.05</td>\n      <td>1.65</td>\n      <td>1.59</td>\n      <td>1.75</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>34.80</td>\n      <td>44</td>\n      <td>40.90</td>\n      <td>40.30</td>\n      <td>21.80</td>\n      <td>34.80</td>\n      <td>30.70</td>\n      <td>17.40</td>\n      <td>712.96</td>\n      <td>0.70</td>\n      <td>2.97</td>\n      <td>1.07</td>\n      <td>1.69</td>\n      <td>1.76</td>\n      <td>2.00</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>7.80</td>\n      <td>27</td>\n      <td>39.40</td>\n      <td>39.20</td>\n      <td>25.90</td>\n      <td>37.20</td>\n      <td>30.20</td>\n      <td>19.00</td>\n      <td>613.89</td>\n      <td>0.93</td>\n      <td>2.63</td>\n      <td>1.14</td>\n      <td>1.63</td>\n      <td>1.59</td>\n      <td>1.96</td>\n    </tr>\n    <tr>\n      <th>224</th>\n      <td>10.90</td>\n      <td>55</td>\n      <td>41.10</td>\n      <td>37.10</td>\n      <td>21.80</td>\n      <td>34.10</td>\n      <td>31.10</td>\n      <td>19.20</td>\n      <td>469.96</td>\n      <td>0.82</td>\n      <td>2.60</td>\n      <td>1.12</td>\n      <td>1.71</td>\n      <td>1.62</td>\n      <td>1.78</td>\n    </tr>\n    <tr>\n      <th>211</th>\n      <td>27.20</td>\n      <td>49</td>\n      <td>40.20</td>\n      <td>40.30</td>\n      <td>23.20</td>\n      <td>36.80</td>\n      <td>31.00</td>\n      <td>18.90</td>\n      <td>627.71</td>\n      <td>0.79</td>\n      <td>2.88</td>\n      <td>1.11</td>\n      <td>1.71</td>\n      <td>1.64</td>\n      <td>1.95</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>30.70</td>\n      <td>54</td>\n      <td>38.00</td>\n      <td>39.40</td>\n      <td>23.60</td>\n      <td>32.70</td>\n      <td>29.90</td>\n      <td>19.10</td>\n      <td>531.61</td>\n      <td>0.80</td>\n      <td>2.83</td>\n      <td>1.05</td>\n      <td>1.63</td>\n      <td>1.57</td>\n      <td>1.71</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>17.30</td>\n      <td>43</td>\n      <td>38.50</td>\n      <td>40.00</td>\n      <td>24.80</td>\n      <td>35.10</td>\n      <td>30.70</td>\n      <td>19.20</td>\n      <td>498.49</td>\n      <td>0.95</td>\n      <td>2.86</td>\n      <td>1.24</td>\n      <td>1.78</td>\n      <td>1.60</td>\n      <td>1.83</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>13.10</td>\n      <td>37</td>\n      <td>35.30</td>\n      <td>38.10</td>\n      <td>22.00</td>\n      <td>31.50</td>\n      <td>26.60</td>\n      <td>16.70</td>\n      <td>340.31</td>\n      <td>0.92</td>\n      <td>2.62</td>\n      <td>1.11</td>\n      <td>1.61</td>\n      <td>1.59</td>\n      <td>1.89</td>\n    </tr>\n    <tr>\n      <th>233</th>\n      <td>26.70</td>\n      <td>58</td>\n      <td>35.10</td>\n      <td>35.90</td>\n      <td>21.00</td>\n      <td>27.80</td>\n      <td>26.10</td>\n      <td>17.60</td>\n      <td>389.04</td>\n      <td>0.79</td>\n      <td>2.70</td>\n      <td>1.00</td>\n      <td>1.76</td>\n      <td>1.48</td>\n      <td>1.58</td>\n    </tr>\n    <tr>\n      <th>231</th>\n      <td>16.10</td>\n      <td>57</td>\n      <td>39.40</td>\n      <td>38.60</td>\n      <td>22.80</td>\n      <td>31.80</td>\n      <td>29.10</td>\n      <td>19.00</td>\n      <td>462.93</td>\n      <td>0.83</td>\n      <td>2.62</td>\n      <td>1.07</td>\n      <td>1.70</td>\n      <td>1.53</td>\n      <td>1.67</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>18.10</td>\n      <td>44</td>\n      <td>38.00</td>\n      <td>39.20</td>\n      <td>24.50</td>\n      <td>32.10</td>\n      <td>28.60</td>\n      <td>18.00</td>\n      <td>486.59</td>\n      <td>0.93</td>\n      <td>2.68</td>\n      <td>1.16</td>\n      <td>1.73</td>\n      <td>1.59</td>\n      <td>1.78</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>12.20</td>\n      <td>43</td>\n      <td>37.80</td>\n      <td>39.20</td>\n      <td>23.80</td>\n      <td>31.70</td>\n      <td>28.40</td>\n      <td>18.60</td>\n      <td>452.29</td>\n      <td>0.91</td>\n      <td>2.72</td>\n      <td>1.15</td>\n      <td>1.65</td>\n      <td>1.53</td>\n      <td>1.70</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>15.60</td>\n      <td>31</td>\n      <td>33.90</td>\n      <td>35.30</td>\n      <td>22.20</td>\n      <td>27.90</td>\n      <td>25.90</td>\n      <td>16.70</td>\n      <td>288.21</td>\n      <td>0.97</td>\n      <td>2.54</td>\n      <td>1.13</td>\n      <td>1.65</td>\n      <td>1.55</td>\n      <td>1.67</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>25.30</td>\n      <td>22</td>\n      <td>34.00</td>\n      <td>38.90</td>\n      <td>24.00</td>\n      <td>28.80</td>\n      <td>25.20</td>\n      <td>16.60</td>\n      <td>357.98</td>\n      <td>0.90</td>\n      <td>2.82</td>\n      <td>1.09</td>\n      <td>1.66</td>\n      <td>1.52</td>\n      <td>1.73</td>\n    </tr>\n    <tr>\n      <th>181</th>\n      <td>0.00</td>\n      <td>40</td>\n      <td>33.80</td>\n      <td>33.50</td>\n      <td>20.20</td>\n      <td>27.70</td>\n      <td>24.60</td>\n      <td>16.50</td>\n      <td>206.50</td>\n      <td>1.01</td>\n      <td>2.35</td>\n      <td>1.14</td>\n      <td>1.80</td>\n      <td>1.49</td>\n      <td>1.68</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>8.80</td>\n      <td>55</td>\n      <td>38.70</td>\n      <td>37.60</td>\n      <td>21.60</td>\n      <td>30.30</td>\n      <td>27.30</td>\n      <td>18.30</td>\n      <td>313.24</td>\n      <td>0.94</td>\n      <td>2.29</td>\n      <td>1.07</td>\n      <td>1.62</td>\n      <td>1.49</td>\n      <td>1.66</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>23.60</td>\n      <td>43</td>\n      <td>37.40</td>\n      <td>39.00</td>\n      <td>24.10</td>\n      <td>33.80</td>\n      <td>28.80</td>\n      <td>18.80</td>\n      <td>431.93</td>\n      <td>0.91</td>\n      <td>2.77</td>\n      <td>1.16</td>\n      <td>1.61</td>\n      <td>1.53</td>\n      <td>1.80</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>10.80</td>\n      <td>40</td>\n      <td>33.60</td>\n      <td>34.50</td>\n      <td>22.50</td>\n      <td>27.90</td>\n      <td>26.20</td>\n      <td>17.30</td>\n      <td>264.03</td>\n      <td>1.01</td>\n      <td>2.62</td>\n      <td>1.20</td>\n      <td>1.66</td>\n      <td>1.51</td>\n      <td>1.61</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>0.70</td>\n      <td>35</td>\n      <td>34.00</td>\n      <td>34.80</td>\n      <td>22.00</td>\n      <td>24.80</td>\n      <td>25.90</td>\n      <td>16.90</td>\n      <td>241.42</td>\n      <td>0.98</td>\n      <td>2.67</td>\n      <td>1.21</td>\n      <td>1.78</td>\n      <td>1.53</td>\n      <td>1.47</td>\n    </tr>\n    <tr>\n      <th>251</th>\n      <td>31.90</td>\n      <td>74</td>\n      <td>40.80</td>\n      <td>42.20</td>\n      <td>24.60</td>\n      <td>33.70</td>\n      <td>30.00</td>\n      <td>20.90</td>\n      <td>615.09</td>\n      <td>0.81</td>\n      <td>2.75</td>\n      <td>1.04</td>\n      <td>1.81</td>\n      <td>1.44</td>\n      <td>1.61</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>20.10</td>\n      <td>41</td>\n      <td>36.30</td>\n      <td>38.40</td>\n      <td>23.20</td>\n      <td>31.00</td>\n      <td>29.20</td>\n      <td>18.40</td>\n      <td>418.84</td>\n      <td>0.90</td>\n      <td>2.66</td>\n      <td>1.08</td>\n      <td>1.64</td>\n      <td>1.59</td>\n      <td>1.68</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>24.30</td>\n      <td>62</td>\n      <td>35.50</td>\n      <td>38.60</td>\n      <td>22.40</td>\n      <td>31.50</td>\n      <td>27.30</td>\n      <td>18.60</td>\n      <td>392.40</td>\n      <td>0.87</td>\n      <td>2.75</td>\n      <td>1.07</td>\n      <td>1.74</td>\n      <td>1.47</td>\n      <td>1.69</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>22.70</td>\n      <td>40</td>\n      <td>36.30</td>\n      <td>38.50</td>\n      <td>23.00</td>\n      <td>31.20</td>\n      <td>28.40</td>\n      <td>17.10</td>\n      <td>415.98</td>\n      <td>0.87</td>\n      <td>2.61</td>\n      <td>1.05</td>\n      <td>1.64</td>\n      <td>1.66</td>\n      <td>1.82</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>21.20</td>\n      <td>30</td>\n      <td>39.40</td>\n      <td>41.50</td>\n      <td>23.70</td>\n      <td>36.90</td>\n      <td>31.60</td>\n      <td>18.80</td>\n      <td>591.26</td>\n      <td>0.83</td>\n      <td>2.64</td>\n      <td>1.02</td>\n      <td>1.65</td>\n      <td>1.68</td>\n      <td>1.96</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>22.60</td>\n      <td>54</td>\n      <td>39.90</td>\n      <td>38.00</td>\n      <td>22.00</td>\n      <td>35.90</td>\n      <td>30.20</td>\n      <td>18.90</td>\n      <td>544.50</td>\n      <td>0.79</td>\n      <td>2.70</td>\n      <td>1.08</td>\n      <td>1.74</td>\n      <td>1.60</td>\n      <td>1.90</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>3.90</td>\n      <td>42</td>\n      <td>37.80</td>\n      <td>34.90</td>\n      <td>22.50</td>\n      <td>27.70</td>\n      <td>27.50</td>\n      <td>18.50</td>\n      <td>275.02</td>\n      <td>0.98</td>\n      <td>2.32</td>\n      <td>1.13</td>\n      <td>1.71</td>\n      <td>1.49</td>\n      <td>1.50</td>\n    </tr>\n    <tr>\n      <th>112</th>\n      <td>22.10</td>\n      <td>47</td>\n      <td>40.20</td>\n      <td>38.10</td>\n      <td>23.90</td>\n      <td>35.30</td>\n      <td>31.10</td>\n      <td>19.80</td>\n      <td>453.90</td>\n      <td>0.86</td>\n      <td>2.48</td>\n      <td>1.05</td>\n      <td>1.58</td>\n      <td>1.57</td>\n      <td>1.78</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>22.90</td>\n      <td>31</td>\n      <td>38.80</td>\n      <td>36.00</td>\n      <td>21.00</td>\n      <td>29.20</td>\n      <td>26.60</td>\n      <td>17.00</td>\n      <td>324.50</td>\n      <td>0.83</td>\n      <td>2.51</td>\n      <td>1.10</td>\n      <td>1.65</td>\n      <td>1.56</td>\n      <td>1.72</td>\n    </tr>\n    <tr>\n      <th>144</th>\n      <td>10.30</td>\n      <td>23</td>\n      <td>38.00</td>\n      <td>37.60</td>\n      <td>23.20</td>\n      <td>31.80</td>\n      <td>29.70</td>\n      <td>18.30</td>\n      <td>456.78</td>\n      <td>0.93</td>\n      <td>2.54</td>\n      <td>1.13</td>\n      <td>1.73</td>\n      <td>1.62</td>\n      <td>1.74</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>16.90</td>\n      <td>36</td>\n      <td>38.70</td>\n      <td>37.70</td>\n      <td>21.50</td>\n      <td>32.40</td>\n      <td>28.40</td>\n      <td>17.80</td>\n      <td>434.46</td>\n      <td>0.85</td>\n      <td>2.54</td>\n      <td>1.09</td>\n      <td>1.69</td>\n      <td>1.60</td>\n      <td>1.82</td>\n    </tr>\n    <tr>\n      <th>155</th>\n      <td>17.30</td>\n      <td>28</td>\n      <td>35.60</td>\n      <td>37.80</td>\n      <td>21.70</td>\n      <td>32.20</td>\n      <td>27.70</td>\n      <td>17.70</td>\n      <td>390.86</td>\n      <td>0.92</td>\n      <td>2.59</td>\n      <td>1.10</td>\n      <td>1.72</td>\n      <td>1.56</td>\n      <td>1.82</td>\n    </tr>\n    <tr>\n      <th>169</th>\n      <td>16.50</td>\n      <td>35</td>\n      <td>37.60</td>\n      <td>39.10</td>\n      <td>23.40</td>\n      <td>32.50</td>\n      <td>29.80</td>\n      <td>17.40</td>\n      <td>429.39</td>\n      <td>0.88</td>\n      <td>2.64</td>\n      <td>1.09</td>\n      <td>1.63</td>\n      <td>1.71</td>\n      <td>1.87</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>32.30</td>\n      <td>57</td>\n      <td>40.10</td>\n      <td>41.20</td>\n      <td>24.70</td>\n      <td>35.30</td>\n      <td>31.10</td>\n      <td>18.40</td>\n      <td>603.29</td>\n      <td>0.80</td>\n      <td>2.63</td>\n      <td>1.00</td>\n      <td>1.67</td>\n      <td>1.69</td>\n      <td>1.92</td>\n    </tr>\n    <tr>\n      <th>179</th>\n      <td>16.90</td>\n      <td>39</td>\n      <td>42.80</td>\n      <td>43.10</td>\n      <td>25.80</td>\n      <td>39.10</td>\n      <td>32.50</td>\n      <td>19.90</td>\n      <td>739.70</td>\n      <td>0.85</td>\n      <td>2.56</td>\n      <td>1.05</td>\n      <td>1.58</td>\n      <td>1.63</td>\n      <td>1.96</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>14.10</td>\n      <td>48</td>\n      <td>36.70</td>\n      <td>39.90</td>\n      <td>24.40</td>\n      <td>28.80</td>\n      <td>29.60</td>\n      <td>18.70</td>\n      <td>424.33</td>\n      <td>0.96</td>\n      <td>2.63</td>\n      <td>1.12</td>\n      <td>1.63</td>\n      <td>1.58</td>\n      <td>1.54</td>\n    </tr>\n    <tr>\n      <th>241</th>\n      <td>35.00</td>\n      <td>65</td>\n      <td>38.80</td>\n      <td>42.10</td>\n      <td>23.40</td>\n      <td>34.90</td>\n      <td>30.10</td>\n      <td>19.40</td>\n      <td>738.47</td>\n      <td>0.72</td>\n      <td>3.08</td>\n      <td>1.01</td>\n      <td>1.86</td>\n      <td>1.55</td>\n      <td>1.80</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>24.90</td>\n      <td>40</td>\n      <td>37.40</td>\n      <td>39.60</td>\n      <td>21.60</td>\n      <td>30.80</td>\n      <td>27.90</td>\n      <td>16.60</td>\n      <td>440.01</td>\n      <td>0.84</td>\n      <td>2.64</td>\n      <td>1.06</td>\n      <td>1.72</td>\n      <td>1.68</td>\n      <td>1.86</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>6.60</td>\n      <td>40</td>\n      <td>34.30</td>\n      <td>34.90</td>\n      <td>21.00</td>\n      <td>26.70</td>\n      <td>26.10</td>\n      <td>17.20</td>\n      <td>281.02</td>\n      <td>0.94</td>\n      <td>2.60</td>\n      <td>1.15</td>\n      <td>1.77</td>\n      <td>1.52</td>\n      <td>1.55</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>28.00</td>\n      <td>62</td>\n      <td>40.50</td>\n      <td>39.80</td>\n      <td>22.70</td>\n      <td>37.70</td>\n      <td>30.90</td>\n      <td>19.20</td>\n      <td>582.76</td>\n      <td>0.78</td>\n      <td>2.75</td>\n      <td>1.07</td>\n      <td>1.71</td>\n      <td>1.61</td>\n      <td>1.96</td>\n    </tr>\n    <tr>\n      <th>216</th>\n      <td>13.60</td>\n      <td>51</td>\n      <td>34.80</td>\n      <td>36.50</td>\n      <td>21.50</td>\n      <td>31.30</td>\n      <td>26.30</td>\n      <td>17.80</td>\n      <td>319.36</td>\n      <td>0.93</td>\n      <td>2.67</td>\n      <td>1.14</td>\n      <td>1.79</td>\n      <td>1.48</td>\n      <td>1.76</td>\n    </tr>\n    <tr>\n      <th>167</th>\n      <td>20.30</td>\n      <td>35</td>\n      <td>43.90</td>\n      <td>41.70</td>\n      <td>24.60</td>\n      <td>37.20</td>\n      <td>33.10</td>\n      <td>19.80</td>\n      <td>699.14</td>\n      <td>0.86</td>\n      <td>2.46</td>\n      <td>1.08</td>\n      <td>1.69</td>\n      <td>1.67</td>\n      <td>1.88</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>23.10</td>\n      <td>64</td>\n      <td>36.50</td>\n      <td>39.50</td>\n      <td>23.30</td>\n      <td>29.20</td>\n      <td>28.40</td>\n      <td>18.10</td>\n      <td>389.35</td>\n      <td>0.89</td>\n      <td>2.86</td>\n      <td>1.15</td>\n      <td>1.62</td>\n      <td>1.57</td>\n      <td>1.61</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>20.50</td>\n      <td>46</td>\n      <td>37.20</td>\n      <td>38.20</td>\n      <td>22.50</td>\n      <td>29.10</td>\n      <td>27.70</td>\n      <td>17.70</td>\n      <td>447.56</td>\n      <td>0.82</td>\n      <td>2.68</td>\n      <td>1.04</td>\n      <td>1.75</td>\n      <td>1.56</td>\n      <td>1.64</td>\n    </tr>\n    <tr>\n      <th>202</th>\n      <td>28.70</td>\n      <td>43</td>\n      <td>37.90</td>\n      <td>38.30</td>\n      <td>23.70</td>\n      <td>32.10</td>\n      <td>28.90</td>\n      <td>18.70</td>\n      <td>562.24</td>\n      <td>0.78</td>\n      <td>2.83</td>\n      <td>1.04</td>\n      <td>1.53</td>\n      <td>1.55</td>\n      <td>1.72</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>20.50</td>\n      <td>41</td>\n      <td>40.80</td>\n      <td>41.30</td>\n      <td>24.80</td>\n      <td>36.60</td>\n      <td>32.40</td>\n      <td>18.80</td>\n      <td>564.21</td>\n      <td>0.87</td>\n      <td>2.68</td>\n      <td>1.11</td>\n      <td>1.62</td>\n      <td>1.72</td>\n      <td>1.95</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>27.00</td>\n      <td>72</td>\n      <td>38.50</td>\n      <td>36.60</td>\n      <td>22.00</td>\n      <td>29.70</td>\n      <td>26.30</td>\n      <td>18.00</td>\n      <td>407.57</td>\n      <td>0.77</td>\n      <td>2.63</td>\n      <td>1.02</td>\n      <td>1.71</td>\n      <td>1.46</td>\n      <td>1.65</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>34.50</td>\n      <td>45</td>\n      <td>43.20</td>\n      <td>39.60</td>\n      <td>26.60</td>\n      <td>36.40</td>\n      <td>32.70</td>\n      <td>21.40</td>\n      <td>1004.18</td>\n      <td>0.69</td>\n      <td>2.97</td>\n      <td>1.02</td>\n      <td>1.73</td>\n      <td>1.53</td>\n      <td>1.70</td>\n    </tr>\n    <tr>\n      <th>159</th>\n      <td>22.50</td>\n      <td>31</td>\n      <td>36.20</td>\n      <td>39.00</td>\n      <td>24.60</td>\n      <td>30.10</td>\n      <td>28.20</td>\n      <td>18.20</td>\n      <td>439.41</td>\n      <td>0.89</td>\n      <td>2.79</td>\n      <td>1.09</td>\n      <td>1.67</td>\n      <td>1.55</td>\n      <td>1.65</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>13.80</td>\n      <td>55</td>\n      <td>36.90</td>\n      <td>35.40</td>\n      <td>21.50</td>\n      <td>32.80</td>\n      <td>27.40</td>\n      <td>18.70</td>\n      <td>334.93</td>\n      <td>0.87</td>\n      <td>2.59</td>\n      <td>1.10</td>\n      <td>1.69</td>\n      <td>1.47</td>\n      <td>1.75</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>20.80</td>\n      <td>32</td>\n      <td>38.40</td>\n      <td>38.30</td>\n      <td>21.50</td>\n      <td>32.50</td>\n      <td>28.60</td>\n      <td>17.70</td>\n      <td>468.78</td>\n      <td>0.85</td>\n      <td>2.66</td>\n      <td>1.11</td>\n      <td>1.64</td>\n      <td>1.62</td>\n      <td>1.84</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>20.40</td>\n      <td>58</td>\n      <td>39.10</td>\n      <td>39.60</td>\n      <td>22.50</td>\n      <td>33.10</td>\n      <td>28.30</td>\n      <td>18.50</td>\n      <td>484.44</td>\n      <td>0.81</td>\n      <td>2.56</td>\n      <td>1.00</td>\n      <td>1.65</td>\n      <td>1.53</td>\n      <td>1.79</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>32.90</td>\n      <td>44</td>\n      <td>36.60</td>\n      <td>42.50</td>\n      <td>23.70</td>\n      <td>33.60</td>\n      <td>28.70</td>\n      <td>17.40</td>\n      <td>1424.58</td>\n      <td>0.80</td>\n      <td>2.90</td>\n      <td>1.02</td>\n      <td>1.64</td>\n      <td>1.65</td>\n      <td>1.93</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>13.90</td>\n      <td>51</td>\n      <td>41.00</td>\n      <td>38.80</td>\n      <td>23.30</td>\n      <td>33.40</td>\n      <td>29.80</td>\n      <td>19.50</td>\n      <td>445.01</td>\n      <td>0.91</td>\n      <td>2.42</td>\n      <td>1.10</td>\n      <td>1.70</td>\n      <td>1.53</td>\n      <td>1.71</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>11.30</td>\n      <td>50</td>\n      <td>38.70</td>\n      <td>39.30</td>\n      <td>23.30</td>\n      <td>30.60</td>\n      <td>27.80</td>\n      <td>18.20</td>\n      <td>397.09</td>\n      <td>0.93</td>\n      <td>2.57</td>\n      <td>1.15</td>\n      <td>1.55</td>\n      <td>1.53</td>\n      <td>1.68</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>13.80</td>\n      <td>50</td>\n      <td>37.70</td>\n      <td>36.60</td>\n      <td>23.50</td>\n      <td>34.40</td>\n      <td>29.20</td>\n      <td>18.00</td>\n      <td>389.79</td>\n      <td>0.93</td>\n      <td>2.62</td>\n      <td>1.18</td>\n      <td>1.61</td>\n      <td>1.62</td>\n      <td>1.91</td>\n    </tr>\n    <tr>\n      <th>157</th>\n      <td>10.00</td>\n      <td>28</td>\n      <td>37.00</td>\n      <td>38.50</td>\n      <td>25.00</td>\n      <td>31.60</td>\n      <td>28.00</td>\n      <td>18.60</td>\n      <td>460.99</td>\n      <td>0.90</td>\n      <td>2.66</td>\n      <td>1.08</td>\n      <td>1.69</td>\n      <td>1.51</td>\n      <td>1.70</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>20.40</td>\n      <td>41</td>\n      <td>38.50</td>\n      <td>39.80</td>\n      <td>23.50</td>\n      <td>36.40</td>\n      <td>30.40</td>\n      <td>19.10</td>\n      <td>615.42</td>\n      <td>0.83</td>\n      <td>2.79</td>\n      <td>1.09</td>\n      <td>1.64</td>\n      <td>1.59</td>\n      <td>1.91</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>22.90</td>\n      <td>32</td>\n      <td>42.10</td>\n      <td>40.00</td>\n      <td>24.40</td>\n      <td>38.20</td>\n      <td>31.60</td>\n      <td>19.30</td>\n      <td>616.70</td>\n      <td>0.86</td>\n      <td>2.56</td>\n      <td>1.10</td>\n      <td>1.60</td>\n      <td>1.64</td>\n      <td>1.98</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>35.20</td>\n      <td>46</td>\n      <td>51.20</td>\n      <td>49.10</td>\n      <td>29.60</td>\n      <td>45.00</td>\n      <td>29.00</td>\n      <td>21.40</td>\n      <td>1825.30</td>\n      <td>0.68</td>\n      <td>2.66</td>\n      <td>0.92</td>\n      <td>1.69</td>\n      <td>1.36</td>\n      <td>2.10</td>\n    </tr>\n    <tr>\n      <th>190</th>\n      <td>11.40</td>\n      <td>41</td>\n      <td>36.40</td>\n      <td>36.30</td>\n      <td>21.80</td>\n      <td>29.60</td>\n      <td>27.30</td>\n      <td>17.90</td>\n      <td>338.04</td>\n      <td>0.94</td>\n      <td>2.51</td>\n      <td>1.13</td>\n      <td>1.70</td>\n      <td>1.53</td>\n      <td>1.65</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6.10</td>\n      <td>22</td>\n      <td>38.50</td>\n      <td>37.30</td>\n      <td>23.40</td>\n      <td>30.50</td>\n      <td>28.90</td>\n      <td>18.20</td>\n      <td>415.44</td>\n      <td>0.95</td>\n      <td>2.43</td>\n      <td>1.13</td>\n      <td>1.68</td>\n      <td>1.59</td>\n      <td>1.68</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>26.80</td>\n      <td>64</td>\n      <td>38.10</td>\n      <td>38.00</td>\n      <td>22.00</td>\n      <td>29.90</td>\n      <td>25.20</td>\n      <td>17.70</td>\n      <td>335.69</td>\n      <td>0.87</td>\n      <td>2.55</td>\n      <td>1.09</td>\n      <td>1.77</td>\n      <td>1.42</td>\n      <td>1.69</td>\n    </tr>\n    <tr>\n      <th>237</th>\n      <td>27.30</td>\n      <td>63</td>\n      <td>40.20</td>\n      <td>41.10</td>\n      <td>22.30</td>\n      <td>35.10</td>\n      <td>29.60</td>\n      <td>18.50</td>\n      <td>691.03</td>\n      <td>0.72</td>\n      <td>2.93</td>\n      <td>1.03</td>\n      <td>1.76</td>\n      <td>1.60</td>\n      <td>1.90</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>25.40</td>\n      <td>43</td>\n      <td>39.60</td>\n      <td>36.10</td>\n      <td>22.00</td>\n      <td>30.10</td>\n      <td>27.20</td>\n      <td>17.70</td>\n      <td>452.40</td>\n      <td>0.77</td>\n      <td>2.63</td>\n      <td>1.05</td>\n      <td>1.67</td>\n      <td>1.54</td>\n      <td>1.70</td>\n    </tr>\n    <tr>\n      <th>227</th>\n      <td>25.20</td>\n      <td>55</td>\n      <td>38.30</td>\n      <td>42.60</td>\n      <td>23.40</td>\n      <td>33.20</td>\n      <td>30.00</td>\n      <td>18.40</td>\n      <td>530.67</td>\n      <td>0.87</td>\n      <td>2.75</td>\n      <td>1.09</td>\n      <td>1.67</td>\n      <td>1.63</td>\n      <td>1.80</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>11.90</td>\n      <td>32</td>\n      <td>38.70</td>\n      <td>38.70</td>\n      <td>33.90</td>\n      <td>32.50</td>\n      <td>27.70</td>\n      <td>18.40</td>\n      <td>449.14</td>\n      <td>1.03</td>\n      <td>2.60</td>\n      <td>1.13</td>\n      <td>1.74</td>\n      <td>1.51</td>\n      <td>1.77</td>\n    </tr>\n    <tr>\n      <th>209</th>\n      <td>10.80</td>\n      <td>47</td>\n      <td>34.50</td>\n      <td>38.20</td>\n      <td>22.60</td>\n      <td>29.00</td>\n      <td>26.20</td>\n      <td>17.60</td>\n      <td>360.71</td>\n      <td>0.93</td>\n      <td>2.69</td>\n      <td>1.10</td>\n      <td>1.68</td>\n      <td>1.49</td>\n      <td>1.65</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>18.40</td>\n      <td>64</td>\n      <td>39.30</td>\n      <td>39.00</td>\n      <td>23.00</td>\n      <td>34.30</td>\n      <td>29.60</td>\n      <td>19.00</td>\n      <td>497.53</td>\n      <td>0.83</td>\n      <td>2.62</td>\n      <td>1.05</td>\n      <td>1.69</td>\n      <td>1.56</td>\n      <td>1.81</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>11.80</td>\n      <td>27</td>\n      <td>38.10</td>\n      <td>36.20</td>\n      <td>24.50</td>\n      <td>29.00</td>\n      <td>30.00</td>\n      <td>18.80</td>\n      <td>396.13</td>\n      <td>1.01</td>\n      <td>2.44</td>\n      <td>1.18</td>\n      <td>1.65</td>\n      <td>1.60</td>\n      <td>1.54</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>14.90</td>\n      <td>42</td>\n      <td>38.30</td>\n      <td>36.90</td>\n      <td>22.20</td>\n      <td>31.60</td>\n      <td>27.80</td>\n      <td>17.70</td>\n      <td>391.51</td>\n      <td>0.88</td>\n      <td>2.51</td>\n      <td>1.11</td>\n      <td>1.70</td>\n      <td>1.57</td>\n      <td>1.79</td>\n    </tr>\n    <tr>\n      <th>225</th>\n      <td>12.50</td>\n      <td>55</td>\n      <td>33.40</td>\n      <td>33.00</td>\n      <td>19.70</td>\n      <td>25.30</td>\n      <td>22.00</td>\n      <td>15.80</td>\n      <td>239.73</td>\n      <td>0.88</td>\n      <td>2.66</td>\n      <td>1.14</td>\n      <td>1.72</td>\n      <td>1.39</td>\n      <td>1.60</td>\n    </tr>\n    <tr>\n      <th>109</th>\n      <td>21.40</td>\n      <td>40</td>\n      <td>34.20</td>\n      <td>36.80</td>\n      <td>22.80</td>\n      <td>32.10</td>\n      <td>26.00</td>\n      <td>17.30</td>\n      <td>410.00</td>\n      <td>0.83</td>\n      <td>2.86</td>\n      <td>1.06</td>\n      <td>1.75</td>\n      <td>1.50</td>\n      <td>1.86</td>\n    </tr>\n    <tr>\n      <th>138</th>\n      <td>22.40</td>\n      <td>40</td>\n      <td>34.30</td>\n      <td>38.40</td>\n      <td>22.50</td>\n      <td>31.70</td>\n      <td>27.40</td>\n      <td>17.60</td>\n      <td>397.31</td>\n      <td>0.89</td>\n      <td>2.87</td>\n      <td>1.11</td>\n      <td>1.69</td>\n      <td>1.56</td>\n      <td>1.80</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>24.90</td>\n      <td>46</td>\n      <td>38.00</td>\n      <td>40.50</td>\n      <td>24.50</td>\n      <td>33.30</td>\n      <td>29.60</td>\n      <td>19.10</td>\n      <td>516.46</td>\n      <td>0.86</td>\n      <td>2.81</td>\n      <td>1.09</td>\n      <td>1.71</td>\n      <td>1.55</td>\n      <td>1.74</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>31.50</td>\n      <td>54</td>\n      <td>40.50</td>\n      <td>38.00</td>\n      <td>22.50</td>\n      <td>31.60</td>\n      <td>28.80</td>\n      <td>18.20</td>\n      <td>579.59</td>\n      <td>0.75</td>\n      <td>2.85</td>\n      <td>1.10</td>\n      <td>1.64</td>\n      <td>1.58</td>\n      <td>1.74</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>7.90</td>\n      <td>34</td>\n      <td>36.20</td>\n      <td>34.70</td>\n      <td>21.40</td>\n      <td>28.70</td>\n      <td>27.00</td>\n      <td>16.50</td>\n      <td>256.18</td>\n      <td>0.97</td>\n      <td>2.45</td>\n      <td>1.19</td>\n      <td>1.65</td>\n      <td>1.64</td>\n      <td>1.74</td>\n    </tr>\n    <tr>\n      <th>153</th>\n      <td>16.50</td>\n      <td>27</td>\n      <td>37.90</td>\n      <td>37.40</td>\n      <td>22.80</td>\n      <td>30.60</td>\n      <td>28.30</td>\n      <td>17.90</td>\n      <td>365.36</td>\n      <td>0.89</td>\n      <td>2.48</td>\n      <td>1.07</td>\n      <td>1.68</td>\n      <td>1.58</td>\n      <td>1.71</td>\n    </tr>\n    <tr>\n      <th>242</th>\n      <td>30.40</td>\n      <td>66</td>\n      <td>41.40</td>\n      <td>42.40</td>\n      <td>24.60</td>\n      <td>35.60</td>\n      <td>30.70</td>\n      <td>19.50</td>\n      <td>762.13</td>\n      <td>0.79</td>\n      <td>2.89</td>\n      <td>1.10</td>\n      <td>1.71</td>\n      <td>1.57</td>\n      <td>1.83</td>\n    </tr>\n    <tr>\n      <th>162</th>\n      <td>13.00</td>\n      <td>33</td>\n      <td>40.70</td>\n      <td>37.30</td>\n      <td>23.50</td>\n      <td>33.50</td>\n      <td>30.60</td>\n      <td>19.70</td>\n      <td>493.79</td>\n      <td>0.87</td>\n      <td>2.43</td>\n      <td>1.07</td>\n      <td>1.62</td>\n      <td>1.55</td>\n      <td>1.70</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>30.00</td>\n      <td>55</td>\n      <td>40.90</td>\n      <td>40.20</td>\n      <td>22.70</td>\n      <td>34.80</td>\n      <td>30.10</td>\n      <td>18.70</td>\n      <td>498.85</td>\n      <td>0.81</td>\n      <td>2.52</td>\n      <td>1.03</td>\n      <td>1.61</td>\n      <td>1.61</td>\n      <td>1.86</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>24.20</td>\n      <td>40</td>\n      <td>38.50</td>\n      <td>39.90</td>\n      <td>22.60</td>\n      <td>35.10</td>\n      <td>30.60</td>\n      <td>19.00</td>\n      <td>584.36</td>\n      <td>0.81</td>\n      <td>2.77</td>\n      <td>1.06</td>\n      <td>1.67</td>\n      <td>1.61</td>\n      <td>1.85</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>27.00</td>\n      <td>70</td>\n      <td>38.70</td>\n      <td>36.50</td>\n      <td>24.10</td>\n      <td>31.20</td>\n      <td>27.30</td>\n      <td>19.20</td>\n      <td>416.51</td>\n      <td>0.84</td>\n      <td>2.63</td>\n      <td>1.07</td>\n      <td>1.70</td>\n      <td>1.42</td>\n      <td>1.62</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>17.50</td>\n      <td>40</td>\n      <td>37.70</td>\n      <td>38.90</td>\n      <td>22.40</td>\n      <td>30.50</td>\n      <td>28.90</td>\n      <td>17.70</td>\n      <td>391.52</td>\n      <td>0.87</td>\n      <td>2.62</td>\n      <td>1.09</td>\n      <td>1.72</td>\n      <td>1.63</td>\n      <td>1.72</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>14.60</td>\n      <td>33</td>\n      <td>38.50</td>\n      <td>40.60</td>\n      <td>25.00</td>\n      <td>31.30</td>\n      <td>29.20</td>\n      <td>19.10</td>\n      <td>526.25</td>\n      <td>0.89</td>\n      <td>2.70</td>\n      <td>1.09</td>\n      <td>1.71</td>\n      <td>1.53</td>\n      <td>1.64</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>29.60</td>\n      <td>25</td>\n      <td>40.90</td>\n      <td>40.80</td>\n      <td>24.60</td>\n      <td>33.30</td>\n      <td>29.70</td>\n      <td>18.40</td>\n      <td>611.36</td>\n      <td>0.83</td>\n      <td>2.71</td>\n      <td>1.10</td>\n      <td>1.55</td>\n      <td>1.61</td>\n      <td>1.81</td>\n    </tr>\n    <tr>\n      <th>236</th>\n      <td>24.80</td>\n      <td>62</td>\n      <td>40.60</td>\n      <td>40.30</td>\n      <td>23.00</td>\n      <td>32.60</td>\n      <td>28.50</td>\n      <td>19.00</td>\n      <td>507.57</td>\n      <td>0.84</td>\n      <td>2.56</td>\n      <td>1.06</td>\n      <td>1.70</td>\n      <td>1.50</td>\n      <td>1.72</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>10.20</td>\n      <td>47</td>\n      <td>34.90</td>\n      <td>37.20</td>\n      <td>22.40</td>\n      <td>26.00</td>\n      <td>25.80</td>\n      <td>17.30</td>\n      <td>346.62</td>\n      <td>0.89</td>\n      <td>2.58</td>\n      <td>1.04</td>\n      <td>1.87</td>\n      <td>1.49</td>\n      <td>1.50</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>32.90</td>\n      <td>44</td>\n      <td>39.10</td>\n      <td>37.60</td>\n      <td>21.40</td>\n      <td>33.10</td>\n      <td>29.50</td>\n      <td>17.30</td>\n      <td>420.70</td>\n      <td>0.81</td>\n      <td>2.57</td>\n      <td>1.07</td>\n      <td>1.70</td>\n      <td>1.71</td>\n      <td>1.91</td>\n    </tr>\n    <tr>\n      <th>239</th>\n      <td>29.90</td>\n      <td>65</td>\n      <td>40.80</td>\n      <td>38.10</td>\n      <td>24.00</td>\n      <td>35.90</td>\n      <td>30.50</td>\n      <td>19.10</td>\n      <td>547.61</td>\n      <td>0.81</td>\n      <td>2.61</td>\n      <td>1.06</td>\n      <td>1.70</td>\n      <td>1.60</td>\n      <td>1.88</td>\n    </tr>\n    <tr>\n      <th>166</th>\n      <td>21.80</td>\n      <td>35</td>\n      <td>38.50</td>\n      <td>34.20</td>\n      <td>21.90</td>\n      <td>30.20</td>\n      <td>28.70</td>\n      <td>17.70</td>\n      <td>406.46</td>\n      <td>0.82</td>\n      <td>2.57</td>\n      <td>1.10</td>\n      <td>1.72</td>\n      <td>1.62</td>\n      <td>1.71</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>13.90</td>\n      <td>43</td>\n      <td>35.70</td>\n      <td>38.20</td>\n      <td>23.40</td>\n      <td>29.70</td>\n      <td>27.40</td>\n      <td>18.30</td>\n      <td>368.30</td>\n      <td>0.98</td>\n      <td>2.71</td>\n      <td>1.19</td>\n      <td>1.66</td>\n      <td>1.50</td>\n      <td>1.62</td>\n    </tr>\n    <tr>\n      <th>201</th>\n      <td>22.10</td>\n      <td>43</td>\n      <td>35.20</td>\n      <td>35.70</td>\n      <td>22.00</td>\n      <td>29.40</td>\n      <td>26.60</td>\n      <td>17.40</td>\n      <td>324.91</td>\n      <td>0.88</td>\n      <td>2.59</td>\n      <td>1.06</td>\n      <td>1.75</td>\n      <td>1.53</td>\n      <td>1.69</td>\n    </tr>\n    <tr>\n      <th>235</th>\n      <td>18.60</td>\n      <td>62</td>\n      <td>38.30</td>\n      <td>37.10</td>\n      <td>22.70</td>\n      <td>30.30</td>\n      <td>26.30</td>\n      <td>18.30</td>\n      <td>421.88</td>\n      <td>0.82</td>\n      <td>2.73</td>\n      <td>1.10</td>\n      <td>1.72</td>\n      <td>1.44</td>\n      <td>1.66</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>9.40</td>\n      <td>23</td>\n      <td>35.50</td>\n      <td>36.10</td>\n      <td>22.70</td>\n      <td>30.50</td>\n      <td>27.20</td>\n      <td>18.20</td>\n      <td>353.22</td>\n      <td>1.00</td>\n      <td>2.59</td>\n      <td>1.19</td>\n      <td>1.67</td>\n      <td>1.49</td>\n      <td>1.68</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>20.80</td>\n      <td>40</td>\n      <td>39.80</td>\n      <td>39.00</td>\n      <td>21.80</td>\n      <td>33.30</td>\n      <td>29.60</td>\n      <td>18.10</td>\n      <td>504.57</td>\n      <td>0.84</td>\n      <td>2.61</td>\n      <td>1.11</td>\n      <td>1.61</td>\n      <td>1.64</td>\n      <td>1.84</td>\n    </tr>\n    <tr>\n      <th>141</th>\n      <td>18.30</td>\n      <td>40</td>\n      <td>36.50</td>\n      <td>38.20</td>\n      <td>22.00</td>\n      <td>32.00</td>\n      <td>28.50</td>\n      <td>17.80</td>\n      <td>431.88</td>\n      <td>0.84</td>\n      <td>2.73</td>\n      <td>1.07</td>\n      <td>1.64</td>\n      <td>1.60</td>\n      <td>1.80</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>9.00</td>\n      <td>47</td>\n      <td>37.30</td>\n      <td>39.60</td>\n      <td>24.60</td>\n      <td>30.30</td>\n      <td>27.90</td>\n      <td>17.80</td>\n      <td>455.68</td>\n      <td>0.92</td>\n      <td>2.67</td>\n      <td>1.12</td>\n      <td>1.77</td>\n      <td>1.57</td>\n      <td>1.70</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>19.20</td>\n      <td>24</td>\n      <td>39.20</td>\n      <td>43.50</td>\n      <td>25.20</td>\n      <td>36.10</td>\n      <td>30.30</td>\n      <td>18.70</td>\n      <td>597.56</td>\n      <td>0.88</td>\n      <td>2.60</td>\n      <td>1.03</td>\n      <td>1.55</td>\n      <td>1.62</td>\n      <td>1.93</td>\n    </tr>\n    <tr>\n      <th>222</th>\n      <td>11.50</td>\n      <td>54</td>\n      <td>37.40</td>\n      <td>40.20</td>\n      <td>23.40</td>\n      <td>27.90</td>\n      <td>27.00</td>\n      <td>17.80</td>\n      <td>387.60</td>\n      <td>0.93</td>\n      <td>2.52</td>\n      <td>1.08</td>\n      <td>1.60</td>\n      <td>1.52</td>\n      <td>1.57</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>15.20</td>\n      <td>28</td>\n      <td>41.30</td>\n      <td>40.60</td>\n      <td>24.60</td>\n      <td>33.00</td>\n      <td>32.80</td>\n      <td>19.90</td>\n      <td>576.35</td>\n      <td>0.86</td>\n      <td>2.70</td>\n      <td>1.13</td>\n      <td>1.65</td>\n      <td>1.65</td>\n      <td>1.66</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>14.90</td>\n      <td>72</td>\n      <td>37.70</td>\n      <td>37.70</td>\n      <td>21.80</td>\n      <td>32.60</td>\n      <td>28.00</td>\n      <td>18.80</td>\n      <td>370.04</td>\n      <td>0.89</td>\n      <td>2.59</td>\n      <td>1.11</td>\n      <td>1.69</td>\n      <td>1.49</td>\n      <td>1.73</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>4.10</td>\n      <td>25</td>\n      <td>38.10</td>\n      <td>38.30</td>\n      <td>23.80</td>\n      <td>35.90</td>\n      <td>31.10</td>\n      <td>18.20</td>\n      <td>492.99</td>\n      <td>0.97</td>\n      <td>2.65</td>\n      <td>1.22</td>\n      <td>1.59</td>\n      <td>1.71</td>\n      <td>1.97</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28.70</td>\n      <td>24</td>\n      <td>34.40</td>\n      <td>42.20</td>\n      <td>24.00</td>\n      <td>32.20</td>\n      <td>27.70</td>\n      <td>17.70</td>\n      <td>476.46</td>\n      <td>0.84</td>\n      <td>2.83</td>\n      <td>0.97</td>\n      <td>1.61</td>\n      <td>1.56</td>\n      <td>1.82</td>\n    </tr>\n    <tr>\n      <th>223</th>\n      <td>5.20</td>\n      <td>55</td>\n      <td>35.20</td>\n      <td>35.20</td>\n      <td>22.50</td>\n      <td>29.40</td>\n      <td>26.80</td>\n      <td>17.00</td>\n      <td>300.89</td>\n      <td>0.90</td>\n      <td>2.63</td>\n      <td>1.12</td>\n      <td>1.69</td>\n      <td>1.58</td>\n      <td>1.73</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>19.30</td>\n      <td>43</td>\n      <td>38.60</td>\n      <td>39.30</td>\n      <td>23.50</td>\n      <td>30.50</td>\n      <td>28.50</td>\n      <td>18.10</td>\n      <td>545.58</td>\n      <td>0.79</td>\n      <td>2.73</td>\n      <td>1.02</td>\n      <td>1.69</td>\n      <td>1.57</td>\n      <td>1.69</td>\n    </tr>\n    <tr>\n      <th>158</th>\n      <td>12.50</td>\n      <td>30</td>\n      <td>35.90</td>\n      <td>34.80</td>\n      <td>21.80</td>\n      <td>27.00</td>\n      <td>34.90</td>\n      <td>16.90</td>\n      <td>271.01</td>\n      <td>0.96</td>\n      <td>2.47</td>\n      <td>1.16</td>\n      <td>1.79</td>\n      <td>2.07</td>\n      <td>1.60</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>13.60</td>\n      <td>45</td>\n      <td>32.80</td>\n      <td>35.80</td>\n      <td>20.60</td>\n      <td>28.80</td>\n      <td>25.50</td>\n      <td>16.30</td>\n      <td>269.02</td>\n      <td>0.87</td>\n      <td>2.81</td>\n      <td>1.11</td>\n      <td>1.74</td>\n      <td>1.56</td>\n      <td>1.77</td>\n    </tr>\n    <tr>\n      <th>178</th>\n      <td>22.50</td>\n      <td>38</td>\n      <td>38.00</td>\n      <td>39.50</td>\n      <td>24.70</td>\n      <td>34.80</td>\n      <td>30.30</td>\n      <td>18.10</td>\n      <td>506.32</td>\n      <td>0.89</td>\n      <td>2.70</td>\n      <td>1.11</td>\n      <td>1.57</td>\n      <td>1.67</td>\n      <td>1.92</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>12.10</td>\n      <td>40</td>\n      <td>35.30</td>\n      <td>39.40</td>\n      <td>22.70</td>\n      <td>30.00</td>\n      <td>26.40</td>\n      <td>17.40</td>\n      <td>363.59</td>\n      <td>0.92</td>\n      <td>2.61</td>\n      <td>1.06</td>\n      <td>1.66</td>\n      <td>1.52</td>\n      <td>1.72</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>6.30</td>\n      <td>54</td>\n      <td>37.50</td>\n      <td>37.40</td>\n      <td>22.40</td>\n      <td>32.60</td>\n      <td>28.10</td>\n      <td>18.10</td>\n      <td>348.05</td>\n      <td>0.99</td>\n      <td>2.38</td>\n      <td>1.14</td>\n      <td>1.72</td>\n      <td>1.55</td>\n      <td>1.80</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>11.80</td>\n      <td>61</td>\n      <td>36.50</td>\n      <td>35.20</td>\n      <td>20.90</td>\n      <td>29.40</td>\n      <td>27.00</td>\n      <td>16.80</td>\n      <td>311.01</td>\n      <td>0.88</td>\n      <td>2.56</td>\n      <td>1.12</td>\n      <td>1.68</td>\n      <td>1.61</td>\n      <td>1.75</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>6.00</td>\n      <td>44</td>\n      <td>37.90</td>\n      <td>39.00</td>\n      <td>24.00</td>\n      <td>32.90</td>\n      <td>29.20</td>\n      <td>18.40</td>\n      <td>457.51</td>\n      <td>0.91</td>\n      <td>2.66</td>\n      <td>1.13</td>\n      <td>1.69</td>\n      <td>1.59</td>\n      <td>1.79</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>23.60</td>\n      <td>41</td>\n      <td>41.90</td>\n      <td>41.30</td>\n      <td>24.70</td>\n      <td>37.20</td>\n      <td>31.80</td>\n      <td>20.00</td>\n      <td>729.60</td>\n      <td>0.79</td>\n      <td>2.80</td>\n      <td>1.08</td>\n      <td>1.61</td>\n      <td>1.59</td>\n      <td>1.86</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>25.80</td>\n      <td>40</td>\n      <td>38.30</td>\n      <td>41.10</td>\n      <td>24.80</td>\n      <td>33.60</td>\n      <td>29.50</td>\n      <td>18.50</td>\n      <td>492.99</td>\n      <td>0.91</td>\n      <td>2.49</td>\n      <td>1.03</td>\n      <td>1.61</td>\n      <td>1.59</td>\n      <td>1.82</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>32.00</td>\n      <td>41</td>\n      <td>41.50</td>\n      <td>40.20</td>\n      <td>23.00</td>\n      <td>35.80</td>\n      <td>31.50</td>\n      <td>18.80</td>\n      <td>628.59</td>\n      <td>0.79</td>\n      <td>2.57</td>\n      <td>1.02</td>\n      <td>1.63</td>\n      <td>1.68</td>\n      <td>1.90</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>29.80</td>\n      <td>56</td>\n      <td>35.60</td>\n      <td>38.00</td>\n      <td>22.10</td>\n      <td>32.50</td>\n      <td>29.80</td>\n      <td>18.30</td>\n      <td>466.45</td>\n      <td>0.83</td>\n      <td>2.89</td>\n      <td>1.09</td>\n      <td>1.66</td>\n      <td>1.63</td>\n      <td>1.78</td>\n    </tr>\n    <tr>\n      <th>168</th>\n      <td>34.30</td>\n      <td>35</td>\n      <td>40.40</td>\n      <td>40.60</td>\n      <td>24.00</td>\n      <td>36.10</td>\n      <td>31.80</td>\n      <td>18.80</td>\n      <td>749.61</td>\n      <td>0.72</td>\n      <td>2.84</td>\n      <td>0.99</td>\n      <td>1.50</td>\n      <td>1.69</td>\n      <td>1.92</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>18.00</td>\n      <td>43</td>\n      <td>31.10</td>\n      <td>39.00</td>\n      <td>24.80</td>\n      <td>31.00</td>\n      <td>29.40</td>\n      <td>18.80</td>\n      <td>399.86</td>\n      <td>0.95</td>\n      <td>2.99</td>\n      <td>1.07</td>\n      <td>1.77</td>\n      <td>1.56</td>\n      <td>1.65</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10.40</td>\n      <td>26</td>\n      <td>37.40</td>\n      <td>37.30</td>\n      <td>22.80</td>\n      <td>32.40</td>\n      <td>29.40</td>\n      <td>18.20</td>\n      <td>472.42</td>\n      <td>0.91</td>\n      <td>2.72</td>\n      <td>1.18</td>\n      <td>1.68</td>\n      <td>1.62</td>\n      <td>1.78</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>20.50</td>\n      <td>35</td>\n      <td>38.40</td>\n      <td>37.30</td>\n      <td>22.40</td>\n      <td>31.00</td>\n      <td>28.70</td>\n      <td>17.70</td>\n      <td>442.50</td>\n      <td>0.86</td>\n      <td>2.62</td>\n      <td>1.11</td>\n      <td>1.71</td>\n      <td>1.62</td>\n      <td>1.75</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>20.90</td>\n      <td>35</td>\n      <td>36.40</td>\n      <td>38.70</td>\n      <td>21.70</td>\n      <td>31.10</td>\n      <td>26.40</td>\n      <td>16.90</td>\n      <td>401.33</td>\n      <td>0.83</td>\n      <td>2.72</td>\n      <td>1.07</td>\n      <td>1.57</td>\n      <td>1.56</td>\n      <td>1.84</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>29.00</td>\n      <td>34</td>\n      <td>38.90</td>\n      <td>40.80</td>\n      <td>23.10</td>\n      <td>36.20</td>\n      <td>30.80</td>\n      <td>17.30</td>\n      <td>539.69</td>\n      <td>0.84</td>\n      <td>2.62</td>\n      <td>1.06</td>\n      <td>1.62</td>\n      <td>1.78</td>\n      <td>2.09</td>\n    </tr>\n    <tr>\n      <th>243</th>\n      <td>32.60</td>\n      <td>67</td>\n      <td>41.30</td>\n      <td>46.00</td>\n      <td>25.40</td>\n      <td>35.30</td>\n      <td>29.80</td>\n      <td>19.50</td>\n      <td>712.99</td>\n      <td>0.80</td>\n      <td>2.80</td>\n      <td>1.02</td>\n      <td>1.67</td>\n      <td>1.53</td>\n      <td>1.81</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>21.50</td>\n      <td>81</td>\n      <td>37.80</td>\n      <td>37.50</td>\n      <td>21.50</td>\n      <td>31.40</td>\n      <td>26.80</td>\n      <td>18.30</td>\n      <td>370.13</td>\n      <td>0.81</td>\n      <td>2.55</td>\n      <td>1.01</td>\n      <td>1.86</td>\n      <td>1.46</td>\n      <td>1.72</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>8.80</td>\n      <td>29</td>\n      <td>36.70</td>\n      <td>35.30</td>\n      <td>22.60</td>\n      <td>30.10</td>\n      <td>26.70</td>\n      <td>17.60</td>\n      <td>374.50</td>\n      <td>0.90</td>\n      <td>2.65</td>\n      <td>1.17</td>\n      <td>1.68</td>\n      <td>1.52</td>\n      <td>1.71</td>\n    </tr>\n    <tr>\n      <th>238</th>\n      <td>12.40</td>\n      <td>64</td>\n      <td>37.90</td>\n      <td>39.10</td>\n      <td>22.30</td>\n      <td>29.80</td>\n      <td>28.90</td>\n      <td>18.30</td>\n      <td>346.80</td>\n      <td>0.96</td>\n      <td>2.53</td>\n      <td>1.16</td>\n      <td>1.54</td>\n      <td>1.58</td>\n      <td>1.63</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>22.20</td>\n      <td>47</td>\n      <td>40.00</td>\n      <td>39.00</td>\n      <td>22.30</td>\n      <td>35.30</td>\n      <td>30.90</td>\n      <td>18.30</td>\n      <td>539.01</td>\n      <td>0.85</td>\n      <td>2.69</td>\n      <td>1.14</td>\n      <td>1.65</td>\n      <td>1.69</td>\n      <td>1.93</td>\n    </tr>\n    <tr>\n      <th>208</th>\n      <td>9.60</td>\n      <td>47</td>\n      <td>36.00</td>\n      <td>36.20</td>\n      <td>22.50</td>\n      <td>31.40</td>\n      <td>27.50</td>\n      <td>17.70</td>\n      <td>366.69</td>\n      <td>0.91</td>\n      <td>2.77</td>\n      <td>1.19</td>\n      <td>1.73</td>\n      <td>1.55</td>\n      <td>1.77</td>\n    </tr>\n    <tr>\n      <th>248</th>\n      <td>33.60</td>\n      <td>72</td>\n      <td>40.90</td>\n      <td>40.80</td>\n      <td>23.20</td>\n      <td>35.20</td>\n      <td>28.60</td>\n      <td>20.10</td>\n      <td>579.23</td>\n      <td>0.80</td>\n      <td>2.65</td>\n      <td>1.03</td>\n      <td>1.75</td>\n      <td>1.42</td>\n      <td>1.75</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>21.20</td>\n      <td>49</td>\n      <td>40.10</td>\n      <td>39.40</td>\n      <td>22.30</td>\n      <td>32.20</td>\n      <td>31.00</td>\n      <td>18.60</td>\n      <td>536.09</td>\n      <td>0.85</td>\n      <td>2.66</td>\n      <td>1.12</td>\n      <td>1.72</td>\n      <td>1.67</td>\n      <td>1.73</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>8.50</td>\n      <td>47</td>\n      <td>37.50</td>\n      <td>38.30</td>\n      <td>22.10</td>\n      <td>30.10</td>\n      <td>28.20</td>\n      <td>18.40</td>\n      <td>387.34</td>\n      <td>0.95</td>\n      <td>2.59</td>\n      <td>1.17</td>\n      <td>1.68</td>\n      <td>1.53</td>\n      <td>1.64</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>30.20</td>\n      <td>69</td>\n      <td>40.80</td>\n      <td>44.00</td>\n      <td>22.60</td>\n      <td>37.50</td>\n      <td>32.60</td>\n      <td>18.80</td>\n      <td>658.73</td>\n      <td>0.79</td>\n      <td>2.79</td>\n      <td>1.06</td>\n      <td>1.74</td>\n      <td>1.73</td>\n      <td>1.99</td>\n    </tr>\n    <tr>\n      <th>133</th>\n      <td>26.10</td>\n      <td>50</td>\n      <td>37.80</td>\n      <td>35.60</td>\n      <td>20.50</td>\n      <td>33.60</td>\n      <td>29.30</td>\n      <td>17.30</td>\n      <td>369.27</td>\n      <td>0.82</td>\n      <td>2.66</td>\n      <td>1.12</td>\n      <td>1.65</td>\n      <td>1.69</td>\n      <td>1.94</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>17.80</td>\n      <td>46</td>\n      <td>35.90</td>\n      <td>37.30</td>\n      <td>21.90</td>\n      <td>31.60</td>\n      <td>27.50</td>\n      <td>18.20</td>\n      <td>358.86</td>\n      <td>0.88</td>\n      <td>2.65</td>\n      <td>1.08</td>\n      <td>1.70</td>\n      <td>1.51</td>\n      <td>1.74</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>26.10</td>\n      <td>62</td>\n      <td>41.40</td>\n      <td>40.90</td>\n      <td>23.10</td>\n      <td>36.20</td>\n      <td>31.80</td>\n      <td>20.20</td>\n      <td>636.94</td>\n      <td>0.80</td>\n      <td>2.71</td>\n      <td>1.07</td>\n      <td>1.67</td>\n      <td>1.57</td>\n      <td>1.79</td>\n    </tr>\n    <tr>\n      <th>114</th>\n      <td>26.70</td>\n      <td>48</td>\n      <td>38.00</td>\n      <td>38.10</td>\n      <td>21.80</td>\n      <td>31.80</td>\n      <td>27.30</td>\n      <td>17.50</td>\n      <td>428.05</td>\n      <td>0.84</td>\n      <td>2.65</td>\n      <td>1.09</td>\n      <td>1.64</td>\n      <td>1.56</td>\n      <td>1.82</td>\n    </tr>\n    <tr>\n      <th>218</th>\n      <td>24.50</td>\n      <td>52</td>\n      <td>39.40</td>\n      <td>39.20</td>\n      <td>22.90</td>\n      <td>35.70</td>\n      <td>30.40</td>\n      <td>19.20</td>\n      <td>553.32</td>\n      <td>0.81</td>\n      <td>2.71</td>\n      <td>1.07</td>\n      <td>1.64</td>\n      <td>1.58</td>\n      <td>1.86</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>32.80</td>\n      <td>47</td>\n      <td>40.20</td>\n      <td>39.40</td>\n      <td>23.30</td>\n      <td>36.70</td>\n      <td>31.60</td>\n      <td>18.40</td>\n      <td>524.48</td>\n      <td>0.80</td>\n      <td>2.55</td>\n      <td>1.01</td>\n      <td>1.68</td>\n      <td>1.72</td>\n      <td>1.99</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>21.30</td>\n      <td>42</td>\n      <td>35.30</td>\n      <td>37.80</td>\n      <td>21.90</td>\n      <td>30.70</td>\n      <td>27.60</td>\n      <td>17.40</td>\n      <td>378.21</td>\n      <td>0.86</td>\n      <td>2.65</td>\n      <td>1.04</td>\n      <td>1.62</td>\n      <td>1.59</td>\n      <td>1.76</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>22.10</td>\n      <td>35</td>\n      <td>40.50</td>\n      <td>39.00</td>\n      <td>23.10</td>\n      <td>36.10</td>\n      <td>30.50</td>\n      <td>18.20</td>\n      <td>507.20</td>\n      <td>0.83</td>\n      <td>2.50</td>\n      <td>1.05</td>\n      <td>1.45</td>\n      <td>1.68</td>\n      <td>1.98</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>22.00</td>\n      <td>42</td>\n      <td>35.50</td>\n      <td>38.60</td>\n      <td>24.00</td>\n      <td>31.20</td>\n      <td>27.30</td>\n      <td>17.40</td>\n      <td>353.83</td>\n      <td>0.93</td>\n      <td>2.75</td>\n      <td>1.14</td>\n      <td>1.67</td>\n      <td>1.57</td>\n      <td>1.79</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>9.60</td>\n      <td>38</td>\n      <td>37.50</td>\n      <td>39.40</td>\n      <td>22.90</td>\n      <td>31.60</td>\n      <td>30.10</td>\n      <td>18.50</td>\n      <td>486.37</td>\n      <td>0.88</td>\n      <td>2.64</td>\n      <td>1.08</td>\n      <td>1.69</td>\n      <td>1.63</td>\n      <td>1.71</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>29.40</td>\n      <td>43</td>\n      <td>37.70</td>\n      <td>39.20</td>\n      <td>23.80</td>\n      <td>34.30</td>\n      <td>28.40</td>\n      <td>17.70</td>\n      <td>476.35</td>\n      <td>0.82</td>\n      <td>2.59</td>\n      <td>0.99</td>\n      <td>1.58</td>\n      <td>1.60</td>\n      <td>1.94</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>22.30</td>\n      <td>49</td>\n      <td>40.70</td>\n      <td>39.80</td>\n      <td>25.40</td>\n      <td>31.00</td>\n      <td>30.30</td>\n      <td>19.70</td>\n      <td>524.89</td>\n      <td>0.89</td>\n      <td>2.54</td>\n      <td>1.08</td>\n      <td>1.72</td>\n      <td>1.54</td>\n      <td>1.57</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>13.50</td>\n      <td>55</td>\n      <td>33.20</td>\n      <td>35.40</td>\n      <td>19.10</td>\n      <td>29.30</td>\n      <td>25.70</td>\n      <td>16.90</td>\n      <td>244.14</td>\n      <td>0.94</td>\n      <td>2.64</td>\n      <td>1.15</td>\n      <td>1.74</td>\n      <td>1.52</td>\n      <td>1.73</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>8.30</td>\n      <td>46</td>\n      <td>38.00</td>\n      <td>38.40</td>\n      <td>23.80</td>\n      <td>30.20</td>\n      <td>29.30</td>\n      <td>18.80</td>\n      <td>430.90</td>\n      <td>0.94</td>\n      <td>2.56</td>\n      <td>1.13</td>\n      <td>1.63</td>\n      <td>1.56</td>\n      <td>1.61</td>\n    </tr>\n    <tr>\n      <th>215</th>\n      <td>47.50</td>\n      <td>51</td>\n      <td>41.20</td>\n      <td>36.90</td>\n      <td>23.60</td>\n      <td>34.70</td>\n      <td>29.10</td>\n      <td>18.40</td>\n      <td>749.39</td>\n      <td>0.65</td>\n      <td>2.91</td>\n      <td>0.98</td>\n      <td>1.80</td>\n      <td>1.58</td>\n      <td>1.89</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>18.50</td>\n      <td>61</td>\n      <td>36.00</td>\n      <td>37.00</td>\n      <td>21.40</td>\n      <td>29.30</td>\n      <td>27.00</td>\n      <td>18.30</td>\n      <td>325.60</td>\n      <td>0.94</td>\n      <td>2.54</td>\n      <td>1.12</td>\n      <td>1.74</td>\n      <td>1.48</td>\n      <td>1.60</td>\n    </tr>\n    <tr>\n      <th>123</th>\n      <td>16.00</td>\n      <td>47</td>\n      <td>36.90</td>\n      <td>36.50</td>\n      <td>22.10</td>\n      <td>30.60</td>\n      <td>27.50</td>\n      <td>17.60</td>\n      <td>343.85</td>\n      <td>0.89</td>\n      <td>2.55</td>\n      <td>1.09</td>\n      <td>1.64</td>\n      <td>1.56</td>\n      <td>1.74</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>11.70</td>\n      <td>23</td>\n      <td>42.10</td>\n      <td>41.70</td>\n      <td>25.00</td>\n      <td>35.60</td>\n      <td>30.00</td>\n      <td>19.20</td>\n      <td>534.74</td>\n      <td>0.97</td>\n      <td>2.37</td>\n      <td>1.12</td>\n      <td>1.65</td>\n      <td>1.56</td>\n      <td>1.85</td>\n    </tr>\n    <tr>\n      <th>228</th>\n      <td>14.90</td>\n      <td>56</td>\n      <td>38.10</td>\n      <td>37.40</td>\n      <td>22.50</td>\n      <td>34.60</td>\n      <td>30.10</td>\n      <td>18.80</td>\n      <td>438.13</td>\n      <td>0.88</td>\n      <td>2.73</td>\n      <td>1.16</td>\n      <td>1.68</td>\n      <td>1.60</td>\n      <td>1.84</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>9.90</td>\n      <td>37</td>\n      <td>36.00</td>\n      <td>34.80</td>\n      <td>22.20</td>\n      <td>31.00</td>\n      <td>26.90</td>\n      <td>16.90</td>\n      <td>304.66</td>\n      <td>0.93</td>\n      <td>2.69</td>\n      <td>1.22</td>\n      <td>1.77</td>\n      <td>1.59</td>\n      <td>1.83</td>\n    </tr>\n    <tr>\n      <th>111</th>\n      <td>28.00</td>\n      <td>43</td>\n      <td>37.10</td>\n      <td>40.00</td>\n      <td>23.60</td>\n      <td>33.50</td>\n      <td>27.80</td>\n      <td>17.40</td>\n      <td>479.72</td>\n      <td>0.77</td>\n      <td>2.91</td>\n      <td>1.03</td>\n      <td>1.62</td>\n      <td>1.60</td>\n      <td>1.93</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>17.70</td>\n      <td>32</td>\n      <td>35.50</td>\n      <td>36.20</td>\n      <td>22.10</td>\n      <td>29.80</td>\n      <td>26.70</td>\n      <td>17.10</td>\n      <td>316.09</td>\n      <td>0.94</td>\n      <td>2.44</td>\n      <td>1.08</td>\n      <td>1.70</td>\n      <td>1.56</td>\n      <td>1.74</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>3.70</td>\n      <td>27</td>\n      <td>36.40</td>\n      <td>34.50</td>\n      <td>21.30</td>\n      <td>30.50</td>\n      <td>27.90</td>\n      <td>17.20</td>\n      <td>274.22</td>\n      <td>0.99</td>\n      <td>2.57</td>\n      <td>1.27</td>\n      <td>1.77</td>\n      <td>1.62</td>\n      <td>1.77</td>\n    </tr>\n    <tr>\n      <th>244</th>\n      <td>29.00</td>\n      <td>67</td>\n      <td>40.70</td>\n      <td>38.80</td>\n      <td>24.10</td>\n      <td>32.10</td>\n      <td>29.30</td>\n      <td>18.50</td>\n      <td>581.03</td>\n      <td>0.77</td>\n      <td>2.91</td>\n      <td>1.11</td>\n      <td>1.75</td>\n      <td>1.58</td>\n      <td>1.74</td>\n    </tr>\n    <tr>\n      <th>229</th>\n      <td>17.00</td>\n      <td>56</td>\n      <td>37.40</td>\n      <td>38.80</td>\n      <td>23.20</td>\n      <td>32.40</td>\n      <td>29.70</td>\n      <td>19.00</td>\n      <td>410.80</td>\n      <td>0.87</td>\n      <td>2.64</td>\n      <td>1.06</td>\n      <td>1.75</td>\n      <td>1.56</td>\n      <td>1.71</td>\n    </tr>\n    <tr>\n      <th>191</th>\n      <td>38.10</td>\n      <td>42</td>\n      <td>41.80</td>\n      <td>45.00</td>\n      <td>25.50</td>\n      <td>37.10</td>\n      <td>31.20</td>\n      <td>19.90</td>\n      <td>784.97</td>\n      <td>0.80</td>\n      <td>2.76</td>\n      <td>1.01</td>\n      <td>1.64</td>\n      <td>1.57</td>\n      <td>1.86</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>20.40</td>\n      <td>48</td>\n      <td>37.00</td>\n      <td>38.40</td>\n      <td>22.40</td>\n      <td>27.90</td>\n      <td>26.20</td>\n      <td>17.00</td>\n      <td>419.29</td>\n      <td>0.85</td>\n      <td>2.68</td>\n      <td>1.08</td>\n      <td>1.66</td>\n      <td>1.54</td>\n      <td>1.64</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>8.00</td>\n      <td>51</td>\n      <td>36.50</td>\n      <td>33.70</td>\n      <td>21.40</td>\n      <td>29.60</td>\n      <td>26.00</td>\n      <td>16.90</td>\n      <td>278.05</td>\n      <td>0.88</td>\n      <td>2.46</td>\n      <td>1.09</td>\n      <td>1.81</td>\n      <td>1.54</td>\n      <td>1.75</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>20.90</td>\n      <td>24</td>\n      <td>39.00</td>\n      <td>42.00</td>\n      <td>25.60</td>\n      <td>35.70</td>\n      <td>30.60</td>\n      <td>18.80</td>\n      <td>591.37</td>\n      <td>0.92</td>\n      <td>2.68</td>\n      <td>1.11</td>\n      <td>1.63</td>\n      <td>1.63</td>\n      <td>1.90</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>24.60</td>\n      <td>61</td>\n      <td>38.40</td>\n      <td>37.70</td>\n      <td>22.90</td>\n      <td>34.50</td>\n      <td>29.60</td>\n      <td>18.50</td>\n      <td>491.41</td>\n      <td>0.80</td>\n      <td>2.73</td>\n      <td>1.07</td>\n      <td>1.64</td>\n      <td>1.60</td>\n      <td>1.86</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td>25.30</td>\n      <td>44</td>\n      <td>39.50</td>\n      <td>40.50</td>\n      <td>23.20</td>\n      <td>33.00</td>\n      <td>29.60</td>\n      <td>18.40</td>\n      <td>479.97</td>\n      <td>0.84</td>\n      <td>2.51</td>\n      <td>1.01</td>\n      <td>1.78</td>\n      <td>1.61</td>\n      <td>1.79</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>18.10</td>\n      <td>49</td>\n      <td>35.50</td>\n      <td>38.70</td>\n      <td>23.20</td>\n      <td>27.50</td>\n      <td>26.50</td>\n      <td>17.60</td>\n      <td>412.56</td>\n      <td>0.88</td>\n      <td>2.75</td>\n      <td>1.09</td>\n      <td>1.68</td>\n      <td>1.51</td>\n      <td>1.56</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>17.40</td>\n      <td>53</td>\n      <td>41.10</td>\n      <td>42.30</td>\n      <td>23.20</td>\n      <td>32.90</td>\n      <td>30.80</td>\n      <td>20.40</td>\n      <td>648.23</td>\n      <td>0.87</td>\n      <td>2.75</td>\n      <td>1.14</td>\n      <td>1.74</td>\n      <td>1.51</td>\n      <td>1.61</td>\n    </tr>\n    <tr>\n      <th>136</th>\n      <td>21.80</td>\n      <td>39</td>\n      <td>37.00</td>\n      <td>36.10</td>\n      <td>22.40</td>\n      <td>32.70</td>\n      <td>28.30</td>\n      <td>17.10</td>\n      <td>393.01</td>\n      <td>0.88</td>\n      <td>2.51</td>\n      <td>1.08</td>\n      <td>1.63</td>\n      <td>1.65</td>\n      <td>1.91</td>\n    </tr>\n    <tr>\n      <th>107</th>\n      <td>18.30</td>\n      <td>52</td>\n      <td>42.00</td>\n      <td>38.70</td>\n      <td>23.40</td>\n      <td>35.10</td>\n      <td>29.60</td>\n      <td>19.10</td>\n      <td>556.37</td>\n      <td>0.80</td>\n      <td>2.62</td>\n      <td>1.08</td>\n      <td>1.80</td>\n      <td>1.55</td>\n      <td>1.84</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>11.00</td>\n      <td>70</td>\n      <td>34.90</td>\n      <td>34.80</td>\n      <td>21.50</td>\n      <td>25.60</td>\n      <td>25.70</td>\n      <td>18.50</td>\n      <td>269.00</td>\n      <td>0.89</td>\n      <td>2.56</td>\n      <td>1.07</td>\n      <td>1.79</td>\n      <td>1.39</td>\n      <td>1.38</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>8.80</td>\n      <td>57</td>\n      <td>38.70</td>\n      <td>39.70</td>\n      <td>24.20</td>\n      <td>30.20</td>\n      <td>29.20</td>\n      <td>18.10</td>\n      <td>379.95</td>\n      <td>1.04</td>\n      <td>2.37</td>\n      <td>1.16</td>\n      <td>1.66</td>\n      <td>1.61</td>\n      <td>1.67</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>5.60</td>\n      <td>39</td>\n      <td>34.60</td>\n      <td>37.50</td>\n      <td>21.90</td>\n      <td>28.80</td>\n      <td>26.80</td>\n      <td>17.90</td>\n      <td>309.51</td>\n      <td>0.97</td>\n      <td>2.60</td>\n      <td>1.13</td>\n      <td>1.76</td>\n      <td>1.50</td>\n      <td>1.61</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>17.70</td>\n      <td>42</td>\n      <td>36.50</td>\n      <td>38.00</td>\n      <td>22.30</td>\n      <td>30.80</td>\n      <td>27.80</td>\n      <td>16.90</td>\n      <td>394.74</td>\n      <td>0.86</td>\n      <td>2.52</td>\n      <td>1.03</td>\n      <td>1.62</td>\n      <td>1.64</td>\n      <td>1.82</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>28.40</td>\n      <td>50</td>\n      <td>42.10</td>\n      <td>41.50</td>\n      <td>24.70</td>\n      <td>33.20</td>\n      <td>30.50</td>\n      <td>19.40</td>\n      <td>567.19</td>\n      <td>0.87</td>\n      <td>2.51</td>\n      <td>1.07</td>\n      <td>1.59</td>\n      <td>1.57</td>\n      <td>1.71</td>\n    </tr>\n    <tr>\n      <th>221</th>\n      <td>26.00</td>\n      <td>54</td>\n      <td>42.50</td>\n      <td>42.70</td>\n      <td>27.00</td>\n      <td>38.40</td>\n      <td>32.00</td>\n      <td>19.60</td>\n      <td>732.18</td>\n      <td>0.81</td>\n      <td>2.82</td>\n      <td>1.09</td>\n      <td>1.64</td>\n      <td>1.63</td>\n      <td>1.96</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>16.60</td>\n      <td>44</td>\n      <td>41.90</td>\n      <td>39.80</td>\n      <td>24.10</td>\n      <td>37.30</td>\n      <td>23.10</td>\n      <td>19.40</td>\n      <td>596.94</td>\n      <td>0.87</td>\n      <td>2.52</td>\n      <td>1.10</td>\n      <td>1.61</td>\n      <td>1.19</td>\n      <td>1.92</td>\n    </tr>\n    <tr>\n      <th>192</th>\n      <td>15.90</td>\n      <td>42</td>\n      <td>40.70</td>\n      <td>38.60</td>\n      <td>24.70</td>\n      <td>34.00</td>\n      <td>30.10</td>\n      <td>18.70</td>\n      <td>531.10</td>\n      <td>0.87</td>\n      <td>2.58</td>\n      <td>1.11</td>\n      <td>1.69</td>\n      <td>1.61</td>\n      <td>1.82</td>\n    </tr>\n    <tr>\n      <th>182</th>\n      <td>11.50</td>\n      <td>40</td>\n      <td>35.50</td>\n      <td>36.20</td>\n      <td>21.80</td>\n      <td>31.40</td>\n      <td>28.30</td>\n      <td>17.20</td>\n      <td>315.88</td>\n      <td>0.90</td>\n      <td>2.69</td>\n      <td>1.14</td>\n      <td>1.69</td>\n      <td>1.65</td>\n      <td>1.83</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>26.60</td>\n      <td>67</td>\n      <td>36.50</td>\n      <td>37.80</td>\n      <td>33.70</td>\n      <td>32.40</td>\n      <td>27.70</td>\n      <td>18.20</td>\n      <td>413.17</td>\n      <td>1.00</td>\n      <td>2.71</td>\n      <td>1.10</td>\n      <td>1.76</td>\n      <td>1.52</td>\n      <td>1.78</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>23.60</td>\n      <td>47</td>\n      <td>37.80</td>\n      <td>38.10</td>\n      <td>22.60</td>\n      <td>33.50</td>\n      <td>28.60</td>\n      <td>17.90</td>\n      <td>529.82</td>\n      <td>0.79</td>\n      <td>2.74</td>\n      <td>1.04</td>\n      <td>1.69</td>\n      <td>1.60</td>\n      <td>1.87</td>\n    </tr>\n    <tr>\n      <th>152</th>\n      <td>10.10</td>\n      <td>27</td>\n      <td>34.10</td>\n      <td>36.80</td>\n      <td>23.80</td>\n      <td>27.80</td>\n      <td>26.30</td>\n      <td>17.40</td>\n      <td>295.03</td>\n      <td>1.07</td>\n      <td>2.60</td>\n      <td>1.22</td>\n      <td>1.70</td>\n      <td>1.51</td>\n      <td>1.60</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>40.10</td>\n      <td>49</td>\n      <td>38.40</td>\n      <td>38.30</td>\n      <td>21.90</td>\n      <td>32.00</td>\n      <td>29.80</td>\n      <td>17.00</td>\n      <td>565.66</td>\n      <td>0.68</td>\n      <td>3.09</td>\n      <td>1.05</td>\n      <td>1.84</td>\n      <td>1.75</td>\n      <td>1.88</td>\n    </tr>\n    <tr>\n      <th>212</th>\n      <td>19.50</td>\n      <td>49</td>\n      <td>38.30</td>\n      <td>38.80</td>\n      <td>23.00</td>\n      <td>29.50</td>\n      <td>27.90</td>\n      <td>18.60</td>\n      <td>394.54</td>\n      <td>0.90</td>\n      <td>2.57</td>\n      <td>1.10</td>\n      <td>1.76</td>\n      <td>1.50</td>\n      <td>1.59</td>\n    </tr>\n    <tr>\n      <th>230</th>\n      <td>10.60</td>\n      <td>57</td>\n      <td>35.20</td>\n      <td>35.00</td>\n      <td>21.30</td>\n      <td>31.70</td>\n      <td>27.30</td>\n      <td>16.90</td>\n      <td>332.02</td>\n      <td>0.85</td>\n      <td>2.83</td>\n      <td>1.15</td>\n      <td>1.70</td>\n      <td>1.62</td>\n      <td>1.88</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>23.30</td>\n      <td>52</td>\n      <td>37.50</td>\n      <td>36.70</td>\n      <td>22.30</td>\n      <td>31.60</td>\n      <td>27.50</td>\n      <td>17.90</td>\n      <td>411.65</td>\n      <td>0.85</td>\n      <td>2.74</td>\n      <td>1.13</td>\n      <td>1.73</td>\n      <td>1.54</td>\n      <td>1.77</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>19.60</td>\n      <td>26</td>\n      <td>41.80</td>\n      <td>43.50</td>\n      <td>25.10</td>\n      <td>38.50</td>\n      <td>33.80</td>\n      <td>19.60</td>\n      <td>784.47</td>\n      <td>0.86</td>\n      <td>2.59</td>\n      <td>1.05</td>\n      <td>1.57</td>\n      <td>1.72</td>\n      <td>1.96</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>27.20</td>\n      <td>42</td>\n      <td>38.90</td>\n      <td>36.80</td>\n      <td>22.20</td>\n      <td>33.80</td>\n      <td>30.30</td>\n      <td>17.20</td>\n      <td>458.27</td>\n      <td>0.83</td>\n      <td>2.54</td>\n      <td>1.07</td>\n      <td>1.62</td>\n      <td>1.76</td>\n      <td>1.97</td>\n    </tr>\n    <tr>\n      <th>250</th>\n      <td>26.00</td>\n      <td>72</td>\n      <td>38.90</td>\n      <td>41.60</td>\n      <td>22.70</td>\n      <td>30.50</td>\n      <td>29.40</td>\n      <td>19.80</td>\n      <td>516.11</td>\n      <td>0.83</td>\n      <td>2.78</td>\n      <td>1.07</td>\n      <td>1.75</td>\n      <td>1.48</td>\n      <td>1.54</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>7.10</td>\n      <td>26</td>\n      <td>38.50</td>\n      <td>39.70</td>\n      <td>25.20</td>\n      <td>32.80</td>\n      <td>29.40</td>\n      <td>18.50</td>\n      <td>465.62</td>\n      <td>1.00</td>\n      <td>2.64</td>\n      <td>1.21</td>\n      <td>1.64</td>\n      <td>1.59</td>\n      <td>1.77</td>\n    </tr>\n    <tr>\n      <th>154</th>\n      <td>21.00</td>\n      <td>27</td>\n      <td>38.20</td>\n      <td>40.00</td>\n      <td>24.90</td>\n      <td>33.70</td>\n      <td>29.20</td>\n      <td>19.40</td>\n      <td>545.58</td>\n      <td>0.84</td>\n      <td>2.65</td>\n      <td>1.01</td>\n      <td>1.69</td>\n      <td>1.51</td>\n      <td>1.74</td>\n    </tr>\n    <tr>\n      <th>185</th>\n      <td>8.60</td>\n      <td>40</td>\n      <td>39.40</td>\n      <td>39.70</td>\n      <td>22.60</td>\n      <td>32.90</td>\n      <td>29.30</td>\n      <td>18.20</td>\n      <td>392.40</td>\n      <td>0.96</td>\n      <td>2.27</td>\n      <td>1.07</td>\n      <td>1.71</td>\n      <td>1.61</td>\n      <td>1.81</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>25.30</td>\n      <td>36</td>\n      <td>41.50</td>\n      <td>42.40</td>\n      <td>24.00</td>\n      <td>35.40</td>\n      <td>21.00</td>\n      <td>20.10</td>\n      <td>716.59</td>\n      <td>0.80</td>\n      <td>2.78</td>\n      <td>1.06</td>\n      <td>1.65</td>\n      <td>1.04</td>\n      <td>1.76</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>16.70</td>\n      <td>40</td>\n      <td>36.30</td>\n      <td>36.30</td>\n      <td>22.10</td>\n      <td>29.80</td>\n      <td>26.30</td>\n      <td>17.30</td>\n      <td>360.49</td>\n      <td>0.87</td>\n      <td>2.67</td>\n      <td>1.12</td>\n      <td>1.66</td>\n      <td>1.52</td>\n      <td>1.72</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>5.70</td>\n      <td>29</td>\n      <td>37.30</td>\n      <td>38.80</td>\n      <td>21.50</td>\n      <td>30.10</td>\n      <td>26.40</td>\n      <td>17.90</td>\n      <td>360.42</td>\n      <td>0.93</td>\n      <td>2.51</td>\n      <td>1.11</td>\n      <td>1.72</td>\n      <td>1.47</td>\n      <td>1.68</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>25.80</td>\n      <td>61</td>\n      <td>37.40</td>\n      <td>40.10</td>\n      <td>22.70</td>\n      <td>33.60</td>\n      <td>29.00</td>\n      <td>18.80</td>\n      <td>472.90</td>\n      <td>0.82</td>\n      <td>2.82</td>\n      <td>1.06</td>\n      <td>1.64</td>\n      <td>1.54</td>\n      <td>1.79</td>\n    </tr>\n    <tr>\n      <th>134</th>\n      <td>24.40</td>\n      <td>41</td>\n      <td>36.50</td>\n      <td>36.90</td>\n      <td>23.00</td>\n      <td>34.00</td>\n      <td>29.80</td>\n      <td>18.10</td>\n      <td>407.31</td>\n      <td>0.89</td>\n      <td>2.70</td>\n      <td>1.13</td>\n      <td>1.76</td>\n      <td>1.65</td>\n      <td>1.88</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>8.50</td>\n      <td>56</td>\n      <td>36.40</td>\n      <td>37.50</td>\n      <td>23.10</td>\n      <td>29.70</td>\n      <td>27.30</td>\n      <td>18.20</td>\n      <td>350.38</td>\n      <td>0.95</td>\n      <td>2.57</td>\n      <td>1.13</td>\n      <td>1.82</td>\n      <td>1.50</td>\n      <td>1.63</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>29.90</td>\n      <td>37</td>\n      <td>42.10</td>\n      <td>42.60</td>\n      <td>24.80</td>\n      <td>34.40</td>\n      <td>29.50</td>\n      <td>18.40</td>\n      <td>814.01</td>\n      <td>0.78</td>\n      <td>2.83</td>\n      <td>1.08</td>\n      <td>1.63</td>\n      <td>1.60</td>\n      <td>1.87</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame = pd.read_csv(\"bodyfat.csv\")\n",
    "dataFrame = dataFrame.reindex(np.random.permutation(dataFrame.index))\n",
    "\n",
    "#add bmi feature cross\n",
    "dataFrame['BMI'] = (dataFrame['Weight'] * dataFrame['Weight']) / dataFrame['Height']\n",
    "dataFrame['StaticWaistRatio'] = (dataFrame['Ankle'] + dataFrame['Knee'] + dataFrame['Wrist'])/dataFrame['Abdomen']\n",
    "dataFrame['ChestNeckRatio'] = dataFrame['Chest'] / dataFrame['Neck']\n",
    "dataFrame['ChestWaistRatio'] = dataFrame['Chest'] / dataFrame['Abdomen']\n",
    "dataFrame['HipThighRatio'] = dataFrame['Hip'] / dataFrame['Thigh']\n",
    "dataFrame['ForearmWristRatio'] = dataFrame['Forearm'] / dataFrame['Wrist']\n",
    "dataFrame['ForearmBicepRatio'] = dataFrame['Biceps'] / dataFrame['Wrist']\n",
    "\n",
    "dataFrame = dataFrame.drop(columns=['Density', 'Height', 'Weight', 'Abdomen','Chest','Hip','Thigh'])\n",
    "\n",
    "print(\"Data set loaded. Num examples: \", len(dataFrame))\n",
    "\n",
    "trainDF = dataFrame.sample(frac = 0.8)\n",
    "testDF = dataFrame.drop(trainDF.index)\n",
    "\n",
    "print(\"Made training and test sets\")\n",
    "\n",
    "dataFrame.describe()\n",
    "\n",
    "dataFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing layers defined.\n"
     ]
    }
   ],
   "source": [
    "# Keras Input tensors of float values.\n",
    "inputs = {\n",
    "    'Age':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='Age'),\n",
    "    'BMI':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='BMI'),\n",
    "    'ChestNeckRatio':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='ChestNeckRatio'),\n",
    "    'ChestWaistRatio':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='ChestWaistRatio'),\n",
    "    'HipThighRatio':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='HipThighRatio'),\n",
    "    'Neck':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='Neck'),\n",
    "    'ForearmWristRatio':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='ForearmWristRatio'),\n",
    "    'ForearmBicepRatio':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='ForearmBicepRatio'),\n",
    "    'Knee':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='Knee'),\n",
    "    'Ankle':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='Ankle'),\n",
    "    'Biceps':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='Biceps'),\n",
    "    'Forearm':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='Forearm'),\n",
    "    'Wrist':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='Wrist'),\n",
    "    'StaticWaistRatio':\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32,\n",
    "                              name='StaticWaistRatio')\n",
    "}\n",
    "\n",
    "#Normalise\n",
    "age_boundaries = [0, 30, 40, 50, 60, 70, float('inf')]\n",
    "\n",
    "age = tf.keras.layers.Normalization(\n",
    "    name = 'normalization_age',\n",
    "    axis=None)\n",
    "age.adapt(trainDF['Age'])\n",
    "age = age(inputs.get('Age'))\n",
    "\n",
    "age = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=age_boundaries,\n",
    "    name='discretization_age')(age)\n",
    "\n",
    "\n",
    "bmi_boundaries = np.linspace(200,2000, 9+1)\n",
    "\n",
    "bmi = tf.keras.layers.Normalization(\n",
    "    name = 'normalization_BMI',\n",
    "    axis=None)\n",
    "bmi.adapt(trainDF['BMI'])\n",
    "bmi = bmi(inputs.get('BMI'))\n",
    "\n",
    "bmi = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=bmi_boundaries,\n",
    "    name='discretization_bmi')(bmi)\n",
    "\n",
    "#chestneck\n",
    "\n",
    "chestneckratio_boundaries = np.linspace(1,4, 9+1)\n",
    "\n",
    "chestneckratio = tf.keras.layers.Normalization(\n",
    "    name = 'normalization_chestneckratio',\n",
    "    axis=None)\n",
    "chestneckratio.adapt(trainDF['ChestNeckRatio'])\n",
    "chestneckratio = chestneckratio(inputs.get('ChestNeckRatio'))\n",
    "\n",
    "chestneckratio = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=chestneckratio_boundaries,\n",
    "    name='discretization_chestneckratio')(chestneckratio)\n",
    "\n",
    "#chestwaist\n",
    "\n",
    "chestwaistratio_boundaries = np.linspace(0.5,2, 9+1)\n",
    "\n",
    "chestwaistratio = tf.keras.layers.Normalization(\n",
    "    name = 'normalization_chestwaistratio',\n",
    "    axis=None)\n",
    "chestwaistratio.adapt(trainDF['ChestWaistRatio'])\n",
    "chestwaistratio = chestwaistratio(inputs.get('ChestWaistRatio'))\n",
    "\n",
    "chestwaistratio = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=chestwaistratio_boundaries,\n",
    "    name='discretization_chestwaistratio')(chestwaistratio)\n",
    "\n",
    "#hipthigh\n",
    "\n",
    "hipthighratio_boundaries = np.linspace(0.9,3.2, 9+1)\n",
    "\n",
    "hipthighratio = tf.keras.layers.Normalization(\n",
    "    name = 'normalization_hipthighratio',\n",
    "    axis=None)\n",
    "hipthighratio.adapt(trainDF['HipThighRatio'])\n",
    "hipthighratio = hipthighratio(inputs.get('HipThighRatio'))\n",
    "\n",
    "hipthighratio = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=hipthighratio_boundaries,\n",
    "    name='discretization_hipthighratio')(hipthighratio)\n",
    "\n",
    "neck = tf.keras.layers.Normalization(\n",
    "    name = 'normalization_neck',\n",
    "    axis=None)\n",
    "neck.adapt(trainDF['Neck'])\n",
    "neck = neck(inputs.get('Neck'))\n",
    "\n",
    "knee = tf.keras.layers.Normalization(\n",
    "    name = 'normalization_knee',\n",
    "    axis=None)\n",
    "knee.adapt(trainDF['Knee'])\n",
    "knee = knee(inputs.get('Knee'))\n",
    "\n",
    "ankle = tf.keras.layers.Normalization(\n",
    "    name = 'normalization_ankle',\n",
    "    axis=None)\n",
    "ankle.adapt(trainDF['Ankle'])\n",
    "ankle = ankle(inputs.get('Ankle'))\n",
    "\n",
    "biceps = tf.keras.layers.Normalization(\n",
    "    name = 'normalization_biceps',\n",
    "    axis=None)\n",
    "biceps.adapt(trainDF['Biceps'])\n",
    "biceps = biceps(inputs.get('Biceps'))\n",
    "\n",
    "wrist = tf.keras.layers.Normalization(\n",
    "    name = 'normalization_wrist',\n",
    "    axis=None)\n",
    "wrist.adapt(trainDF['Wrist'])\n",
    "wrist = wrist(inputs.get('Wrist'))\n",
    "\n",
    "#forearmwrist\n",
    "forearmwristratio_boundaries = np.linspace(1,2.5, 9+1)\n",
    "\n",
    "forearmwristratio = tf.keras.layers.Normalization(\n",
    "    name = 'normalization_forearmwristratio',\n",
    "    axis=None)\n",
    "forearmwristratio.adapt(trainDF['ForearmWristRatio'])\n",
    "forearmwristratio = forearmwristratio(inputs.get('ForearmWristRatio'))\n",
    "\n",
    "forearmwristratio = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=forearmwristratio_boundaries,\n",
    "    name='discretization_forearmwristratio')(forearmwristratio)\n",
    "\n",
    "#forearmbicep\n",
    "forearmbicepratio_boundaries = np.linspace(1.1,3, 9+1)\n",
    "\n",
    "forearmbicepratio = tf.keras.layers.Normalization(\n",
    "    name = 'normalization_forearmbicepratio',\n",
    "    axis=None)\n",
    "forearmbicepratio.adapt(trainDF['ForearmBicepRatio'])\n",
    "forearmbicepratio = forearmbicepratio(inputs.get('ForearmBicepRatio'))\n",
    "\n",
    "forearmbicepratio = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=forearmbicepratio_boundaries,\n",
    "    name='discretization_forearmbicepratio')(forearmbicepratio)\n",
    "\n",
    "#staticwaist\n",
    "staticwaistratio_boundaries = np.linspace(1.1,3, 9+1)\n",
    "\n",
    "staticwaistratio = tf.keras.layers.Normalization(\n",
    "    name = 'normalization_staticwaistratio',\n",
    "    axis=None)\n",
    "staticwaistratio.adapt(trainDF['StaticWaistRatio'])\n",
    "staticwaistratio = staticwaistratio(inputs.get('StaticWaistRatio'))\n",
    "\n",
    "staticwaistratio = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=staticwaistratio_boundaries,\n",
    "    name='discretization_staticwaistratio')(staticwaistratio)\n",
    "\n",
    "# Concatenate our inputs into a single tensor.\n",
    "preprocessing_layers = tf.keras.layers.Concatenate()(list(inputs.values()))\n",
    "\n",
    "print(\"Preprocessing layers defined.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the plot_the_loss_curve function.\n"
     ]
    }
   ],
   "source": [
    "#@title Define the plotting function.\n",
    "\n",
    "def plot_the_loss_curve(epochs, mse_training, mse_validation):\n",
    "    \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.yscale(\"log\")  # Set y-axis scale to logarithmic\n",
    "\n",
    "    plt.plot(epochs, mse_training, label=\"Training Loss\")\n",
    "    plt.plot(epochs, mse_validation, label=\"Validation Loss\")\n",
    "\n",
    "    merged_mse_lists = mse_training.tolist() + mse_validation\n",
    "    highest_loss = max(merged_mse_lists)\n",
    "    lowest_loss = min(merged_mse_lists)\n",
    "    top_of_y_axis = highest_loss * 1.03\n",
    "    bottom_of_y_axis = lowest_loss * 0.97\n",
    "\n",
    "    plt.ylim([bottom_of_y_axis, top_of_y_axis])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Defined the plot_the_loss_curve function.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the create_model and train_model functions.\n"
     ]
    }
   ],
   "source": [
    "#@title Define functions to create and train a linear regression model\n",
    "def create_model(my_inputs, my_outputs, my_learning_rate, l2_regularization):\n",
    "    \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "    model = tf.keras.Model(inputs=my_inputs, outputs=my_outputs)\n",
    "\n",
    "    # Add L2 regularization to all trainable weights in the model.\n",
    "    regularizer = tf.keras.regularizers.l2(l2_regularization)\n",
    "    for layer in model.layers:\n",
    "        for attr in ['kernel_regularizer', 'bias_regularizer']:\n",
    "            if hasattr(layer, attr):\n",
    "                setattr(layer, attr, regularizer)\n",
    "\n",
    "    # Construct the layers into a model that TensorFlow can execute.\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=my_learning_rate),\n",
    "        loss=\"mean_squared_error\",\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create Normalization layers\n",
    "train_bodyfat_normalized = tf.keras.layers.Normalization(axis=None)\n",
    "train_bodyfat_normalized.adapt(\n",
    "    np.array(trainDF['BodyFat']))\n",
    "\n",
    "test_bodyfat_normalized = tf.keras.layers.Normalization(axis=None)\n",
    "test_bodyfat_normalized.adapt(\n",
    "    np.array(testDF['BodyFat']))\n",
    "\n",
    "def train_model(model, dataset, epochs, batch_size, label_name, patienceNo, validation_split=0.1):\n",
    "    \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "    # Split the dataset into features and label.\n",
    "    features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    label = train_bodyfat_normalized(\n",
    "        np.array(features.pop(label_name)))\n",
    "    # Define the early stopping criteria\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patienceNo, mode='min')\n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                        epochs=epochs, shuffle=True, callbacks=[early_stopping], validation_split=validation_split)\n",
    "\n",
    "    # Get details that will be useful for plotting the loss curve.\n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    mse = hist[\"mean_squared_error\"]\n",
    "\n",
    "    return epochs, mse, history.history\n",
    "\n",
    "print(\"Defined the create_model and train_model functions.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "#@title Define linear regression model outputs\n",
    "def get_outputs_linear_regression():\n",
    "  # Create the Dense output layer.\n",
    "  dense_output = tf.keras.layers.Dense(units=1, input_shape=(1,),\n",
    "                              name='dense_output')(preprocessing_layers)\n",
    "\n",
    "  # Define an output dictionary we'll send to the model constructor.\n",
    "  outputs = {\n",
    "    'dense_output': dense_output\n",
    "  }\n",
    "  return outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "def get_outputs_dnn():\n",
    "  # Create a Dense layer with 20 nodes.\n",
    "  dense_output = tf.keras.layers.Dense(units=13, input_shape=(1,),\n",
    "                              activation='relu',\n",
    "                              name='hidden_dense_layer_1')(preprocessing_layers)\n",
    "  # Create a Dense layer with 12 nodes.\n",
    "  dense_output = tf.keras.layers.Dense(units=13, input_shape=(1,),\n",
    "                              activation='relu',\n",
    "                              name='hidden_dense_layer_2')(dense_output)\n",
    "  # Create a Dense layer with 12 nodes.\n",
    "  dense_output = tf.keras.layers.Dense(units=13, input_shape=(1,),\n",
    "                              activation='relu',\n",
    "                              name='hidden_dense_layer_3')(dense_output)\n",
    "\n",
    "  # Define an output dictionary we'll send to the model constructor.\n",
    "  outputs = {\n",
    "    'dense_output': dense_output\n",
    "  }\n",
    "\n",
    "  return outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "11/11 [==============================] - 1s 21ms/step - loss: 82.7233 - mean_squared_error: 82.7233 - val_loss: 107.8686 - val_mean_squared_error: 107.8686\n",
      "Epoch 2/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 77.3849 - mean_squared_error: 77.3849 - val_loss: 101.0106 - val_mean_squared_error: 101.0106\n",
      "Epoch 3/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 72.1734 - mean_squared_error: 72.1734 - val_loss: 94.6429 - val_mean_squared_error: 94.6429\n",
      "Epoch 4/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 67.3622 - mean_squared_error: 67.3622 - val_loss: 88.6487 - val_mean_squared_error: 88.6487\n",
      "Epoch 5/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 62.9940 - mean_squared_error: 62.9940 - val_loss: 82.9186 - val_mean_squared_error: 82.9186\n",
      "Epoch 6/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 58.8597 - mean_squared_error: 58.8597 - val_loss: 77.6316 - val_mean_squared_error: 77.6316\n",
      "Epoch 7/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 54.9962 - mean_squared_error: 54.9962 - val_loss: 72.7970 - val_mean_squared_error: 72.7970\n",
      "Epoch 8/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 51.4522 - mean_squared_error: 51.4522 - val_loss: 68.3293 - val_mean_squared_error: 68.3293\n",
      "Epoch 9/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 48.1938 - mean_squared_error: 48.1938 - val_loss: 64.2075 - val_mean_squared_error: 64.2075\n",
      "Epoch 10/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 45.2023 - mean_squared_error: 45.2023 - val_loss: 60.3874 - val_mean_squared_error: 60.3874\n",
      "Epoch 11/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 42.4594 - mean_squared_error: 42.4594 - val_loss: 56.8611 - val_mean_squared_error: 56.8611\n",
      "Epoch 12/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 39.9102 - mean_squared_error: 39.9102 - val_loss: 53.6326 - val_mean_squared_error: 53.6326\n",
      "Epoch 13/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 37.5479 - mean_squared_error: 37.5479 - val_loss: 50.6994 - val_mean_squared_error: 50.6994\n",
      "Epoch 14/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 35.4578 - mean_squared_error: 35.4578 - val_loss: 47.9508 - val_mean_squared_error: 47.9508\n",
      "Epoch 15/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 33.4796 - mean_squared_error: 33.4796 - val_loss: 45.4900 - val_mean_squared_error: 45.4900\n",
      "Epoch 16/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 31.7410 - mean_squared_error: 31.7410 - val_loss: 43.1789 - val_mean_squared_error: 43.1789\n",
      "Epoch 17/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 30.1146 - mean_squared_error: 30.1146 - val_loss: 41.0743 - val_mean_squared_error: 41.0743\n",
      "Epoch 18/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 28.5775 - mean_squared_error: 28.5775 - val_loss: 39.1990 - val_mean_squared_error: 39.1990\n",
      "Epoch 19/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 27.2296 - mean_squared_error: 27.2296 - val_loss: 37.4109 - val_mean_squared_error: 37.4109\n",
      "Epoch 20/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 25.9658 - mean_squared_error: 25.9658 - val_loss: 35.7349 - val_mean_squared_error: 35.7349\n",
      "Epoch 21/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 24.7566 - mean_squared_error: 24.7566 - val_loss: 34.2264 - val_mean_squared_error: 34.2264\n",
      "Epoch 22/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 23.6647 - mean_squared_error: 23.6647 - val_loss: 32.8111 - val_mean_squared_error: 32.8111\n",
      "Epoch 23/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 22.6318 - mean_squared_error: 22.6318 - val_loss: 31.5022 - val_mean_squared_error: 31.5022\n",
      "Epoch 24/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 21.7062 - mean_squared_error: 21.7062 - val_loss: 30.2079 - val_mean_squared_error: 30.2079\n",
      "Epoch 25/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 20.7933 - mean_squared_error: 20.7933 - val_loss: 29.0324 - val_mean_squared_error: 29.0324\n",
      "Epoch 26/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 19.9654 - mean_squared_error: 19.9654 - val_loss: 27.9205 - val_mean_squared_error: 27.9205\n",
      "Epoch 27/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 19.1639 - mean_squared_error: 19.1639 - val_loss: 26.9204 - val_mean_squared_error: 26.9204\n",
      "Epoch 28/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 18.4650 - mean_squared_error: 18.4650 - val_loss: 25.9126 - val_mean_squared_error: 25.9126\n",
      "Epoch 29/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 17.7634 - mean_squared_error: 17.7634 - val_loss: 25.0070 - val_mean_squared_error: 25.0070\n",
      "Epoch 30/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 17.1218 - mean_squared_error: 17.1218 - val_loss: 24.1308 - val_mean_squared_error: 24.1308\n",
      "Epoch 31/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 16.5125 - mean_squared_error: 16.5125 - val_loss: 23.2945 - val_mean_squared_error: 23.2945\n",
      "Epoch 32/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 15.9362 - mean_squared_error: 15.9362 - val_loss: 22.5019 - val_mean_squared_error: 22.5019\n",
      "Epoch 33/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 15.3917 - mean_squared_error: 15.3917 - val_loss: 21.7410 - val_mean_squared_error: 21.7410\n",
      "Epoch 34/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 14.8521 - mean_squared_error: 14.8521 - val_loss: 21.0490 - val_mean_squared_error: 21.0490\n",
      "Epoch 35/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 14.3644 - mean_squared_error: 14.3644 - val_loss: 20.3552 - val_mean_squared_error: 20.3552\n",
      "Epoch 36/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 13.8768 - mean_squared_error: 13.8768 - val_loss: 19.7050 - val_mean_squared_error: 19.7050\n",
      "Epoch 37/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 13.4348 - mean_squared_error: 13.4348 - val_loss: 19.0606 - val_mean_squared_error: 19.0606\n",
      "Epoch 38/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 12.9913 - mean_squared_error: 12.9913 - val_loss: 18.4624 - val_mean_squared_error: 18.4624\n",
      "Epoch 39/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 12.5781 - mean_squared_error: 12.5781 - val_loss: 17.8871 - val_mean_squared_error: 17.8871\n",
      "Epoch 40/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 12.1824 - mean_squared_error: 12.1824 - val_loss: 17.3421 - val_mean_squared_error: 17.3421\n",
      "Epoch 41/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 11.8074 - mean_squared_error: 11.8074 - val_loss: 16.8247 - val_mean_squared_error: 16.8247\n",
      "Epoch 42/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 11.4499 - mean_squared_error: 11.4499 - val_loss: 16.3231 - val_mean_squared_error: 16.3231\n",
      "Epoch 43/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 11.1084 - mean_squared_error: 11.1084 - val_loss: 15.8423 - val_mean_squared_error: 15.8423\n",
      "Epoch 44/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 10.7862 - mean_squared_error: 10.7862 - val_loss: 15.3705 - val_mean_squared_error: 15.3705\n",
      "Epoch 45/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 10.4665 - mean_squared_error: 10.4665 - val_loss: 14.9286 - val_mean_squared_error: 14.9286\n",
      "Epoch 46/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 10.1634 - mean_squared_error: 10.1634 - val_loss: 14.5093 - val_mean_squared_error: 14.5093\n",
      "Epoch 47/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 9.8744 - mean_squared_error: 9.8744 - val_loss: 14.1007 - val_mean_squared_error: 14.1007\n",
      "Epoch 48/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 9.5982 - mean_squared_error: 9.5982 - val_loss: 13.6974 - val_mean_squared_error: 13.6974\n",
      "Epoch 49/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 9.3239 - mean_squared_error: 9.3239 - val_loss: 13.3198 - val_mean_squared_error: 13.3198\n",
      "Epoch 50/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 9.0642 - mean_squared_error: 9.0642 - val_loss: 12.9562 - val_mean_squared_error: 12.9562\n",
      "Epoch 51/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 8.8184 - mean_squared_error: 8.8184 - val_loss: 12.5951 - val_mean_squared_error: 12.5951\n",
      "Epoch 52/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 8.5792 - mean_squared_error: 8.5792 - val_loss: 12.2478 - val_mean_squared_error: 12.2478\n",
      "Epoch 53/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 8.3445 - mean_squared_error: 8.3445 - val_loss: 11.9202 - val_mean_squared_error: 11.9202\n",
      "Epoch 54/2000\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 8.1236 - mean_squared_error: 8.1236 - val_loss: 11.6001 - val_mean_squared_error: 11.6001\n",
      "Epoch 55/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 7.9064 - mean_squared_error: 7.9064 - val_loss: 11.2984 - val_mean_squared_error: 11.2984\n",
      "Epoch 56/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 7.7052 - mean_squared_error: 7.7052 - val_loss: 10.9958 - val_mean_squared_error: 10.9958\n",
      "Epoch 57/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 7.5027 - mean_squared_error: 7.5027 - val_loss: 10.7098 - val_mean_squared_error: 10.7098\n",
      "Epoch 58/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 7.3131 - mean_squared_error: 7.3131 - val_loss: 10.4357 - val_mean_squared_error: 10.4357\n",
      "Epoch 59/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 7.1250 - mean_squared_error: 7.1250 - val_loss: 10.1749 - val_mean_squared_error: 10.1749\n",
      "Epoch 60/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 6.9472 - mean_squared_error: 6.9472 - val_loss: 9.9214 - val_mean_squared_error: 9.9214\n",
      "Epoch 61/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 6.7740 - mean_squared_error: 6.7740 - val_loss: 9.6779 - val_mean_squared_error: 9.6779\n",
      "Epoch 62/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 6.6086 - mean_squared_error: 6.6086 - val_loss: 9.4390 - val_mean_squared_error: 9.4390\n",
      "Epoch 63/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 6.4471 - mean_squared_error: 6.4471 - val_loss: 9.2085 - val_mean_squared_error: 9.2085\n",
      "Epoch 64/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 6.2902 - mean_squared_error: 6.2902 - val_loss: 8.9909 - val_mean_squared_error: 8.9909\n",
      "Epoch 65/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 6.1427 - mean_squared_error: 6.1427 - val_loss: 8.7759 - val_mean_squared_error: 8.7759\n",
      "Epoch 66/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 5.9971 - mean_squared_error: 5.9971 - val_loss: 8.5691 - val_mean_squared_error: 8.5691\n",
      "Epoch 67/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 5.8569 - mean_squared_error: 5.8569 - val_loss: 8.3660 - val_mean_squared_error: 8.3660\n",
      "Epoch 68/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 5.7204 - mean_squared_error: 5.7204 - val_loss: 8.1710 - val_mean_squared_error: 8.1710\n",
      "Epoch 69/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 5.5915 - mean_squared_error: 5.5915 - val_loss: 7.9783 - val_mean_squared_error: 7.9783\n",
      "Epoch 70/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 5.4635 - mean_squared_error: 5.4635 - val_loss: 7.7951 - val_mean_squared_error: 7.7951\n",
      "Epoch 71/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 5.3394 - mean_squared_error: 5.3394 - val_loss: 7.6214 - val_mean_squared_error: 7.6214\n",
      "Epoch 72/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 5.2221 - mean_squared_error: 5.2221 - val_loss: 7.4493 - val_mean_squared_error: 7.4493\n",
      "Epoch 73/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 5.1074 - mean_squared_error: 5.1074 - val_loss: 7.2879 - val_mean_squared_error: 7.2879\n",
      "Epoch 74/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.9955 - mean_squared_error: 4.9955 - val_loss: 7.1331 - val_mean_squared_error: 7.1331\n",
      "Epoch 75/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.8887 - mean_squared_error: 4.8887 - val_loss: 6.9806 - val_mean_squared_error: 6.9806\n",
      "Epoch 76/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.7827 - mean_squared_error: 4.7827 - val_loss: 6.8364 - val_mean_squared_error: 6.8364\n",
      "Epoch 77/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.6818 - mean_squared_error: 4.6818 - val_loss: 6.6963 - val_mean_squared_error: 6.6963\n",
      "Epoch 78/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.5833 - mean_squared_error: 4.5833 - val_loss: 6.5588 - val_mean_squared_error: 6.5588\n",
      "Epoch 79/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 4.4888 - mean_squared_error: 4.4888 - val_loss: 6.4237 - val_mean_squared_error: 6.4237\n",
      "Epoch 80/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.3969 - mean_squared_error: 4.3969 - val_loss: 6.2912 - val_mean_squared_error: 6.2912\n",
      "Epoch 81/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.3063 - mean_squared_error: 4.3063 - val_loss: 6.1658 - val_mean_squared_error: 6.1658\n",
      "Epoch 82/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.2191 - mean_squared_error: 4.2191 - val_loss: 6.0452 - val_mean_squared_error: 6.0452\n",
      "Epoch 83/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.1350 - mean_squared_error: 4.1350 - val_loss: 5.9267 - val_mean_squared_error: 5.9267\n",
      "Epoch 84/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.0527 - mean_squared_error: 4.0527 - val_loss: 5.8076 - val_mean_squared_error: 5.8076\n",
      "Epoch 85/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.9734 - mean_squared_error: 3.9734 - val_loss: 5.6939 - val_mean_squared_error: 5.6939\n",
      "Epoch 86/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.8951 - mean_squared_error: 3.8951 - val_loss: 5.5837 - val_mean_squared_error: 5.5837\n",
      "Epoch 87/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.8203 - mean_squared_error: 3.8203 - val_loss: 5.4769 - val_mean_squared_error: 5.4769\n",
      "Epoch 88/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.7475 - mean_squared_error: 3.7475 - val_loss: 5.3748 - val_mean_squared_error: 5.3748\n",
      "Epoch 89/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.6775 - mean_squared_error: 3.6775 - val_loss: 5.2751 - val_mean_squared_error: 5.2751\n",
      "Epoch 90/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.6085 - mean_squared_error: 3.6085 - val_loss: 5.1760 - val_mean_squared_error: 5.1760\n",
      "Epoch 91/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.5410 - mean_squared_error: 3.5410 - val_loss: 5.0817 - val_mean_squared_error: 5.0817\n",
      "Epoch 92/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.4782 - mean_squared_error: 3.4782 - val_loss: 4.9861 - val_mean_squared_error: 4.9861\n",
      "Epoch 93/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.4138 - mean_squared_error: 3.4138 - val_loss: 4.8968 - val_mean_squared_error: 4.8968\n",
      "Epoch 94/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.3535 - mean_squared_error: 3.3535 - val_loss: 4.8077 - val_mean_squared_error: 4.8077\n",
      "Epoch 95/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.2941 - mean_squared_error: 3.2941 - val_loss: 4.7229 - val_mean_squared_error: 4.7229\n",
      "Epoch 96/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.2361 - mean_squared_error: 3.2361 - val_loss: 4.6396 - val_mean_squared_error: 4.6396\n",
      "Epoch 97/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.1800 - mean_squared_error: 3.1800 - val_loss: 4.5584 - val_mean_squared_error: 4.5584\n",
      "Epoch 98/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.1263 - mean_squared_error: 3.1263 - val_loss: 4.4789 - val_mean_squared_error: 4.4789\n",
      "Epoch 99/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.0723 - mean_squared_error: 3.0723 - val_loss: 4.4026 - val_mean_squared_error: 4.4026\n",
      "Epoch 100/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.0217 - mean_squared_error: 3.0217 - val_loss: 4.3269 - val_mean_squared_error: 4.3269\n",
      "Epoch 101/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.9716 - mean_squared_error: 2.9716 - val_loss: 4.2531 - val_mean_squared_error: 4.2531\n",
      "Epoch 102/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.9230 - mean_squared_error: 2.9230 - val_loss: 4.1813 - val_mean_squared_error: 4.1813\n",
      "Epoch 103/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.8760 - mean_squared_error: 2.8760 - val_loss: 4.1137 - val_mean_squared_error: 4.1137\n",
      "Epoch 104/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.8311 - mean_squared_error: 2.8311 - val_loss: 4.0469 - val_mean_squared_error: 4.0469\n",
      "Epoch 105/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.7864 - mean_squared_error: 2.7864 - val_loss: 3.9818 - val_mean_squared_error: 3.9818\n",
      "Epoch 106/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7430 - mean_squared_error: 2.7430 - val_loss: 3.9204 - val_mean_squared_error: 3.9204\n",
      "Epoch 107/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.7012 - mean_squared_error: 2.7012 - val_loss: 3.8594 - val_mean_squared_error: 3.8594\n",
      "Epoch 108/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6620 - mean_squared_error: 2.6620 - val_loss: 3.7942 - val_mean_squared_error: 3.7942\n",
      "Epoch 109/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.6206 - mean_squared_error: 2.6206 - val_loss: 3.7362 - val_mean_squared_error: 3.7362\n",
      "Epoch 110/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5821 - mean_squared_error: 2.5821 - val_loss: 3.6789 - val_mean_squared_error: 3.6789\n",
      "Epoch 111/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5445 - mean_squared_error: 2.5445 - val_loss: 3.6243 - val_mean_squared_error: 3.6243\n",
      "Epoch 112/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5074 - mean_squared_error: 2.5074 - val_loss: 3.5717 - val_mean_squared_error: 3.5717\n",
      "Epoch 113/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4719 - mean_squared_error: 2.4719 - val_loss: 3.5156 - val_mean_squared_error: 3.5156\n",
      "Epoch 114/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4371 - mean_squared_error: 2.4371 - val_loss: 3.4627 - val_mean_squared_error: 3.4627\n",
      "Epoch 115/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4034 - mean_squared_error: 2.4034 - val_loss: 3.4123 - val_mean_squared_error: 3.4123\n",
      "Epoch 116/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3698 - mean_squared_error: 2.3698 - val_loss: 3.3621 - val_mean_squared_error: 3.3621\n",
      "Epoch 117/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3370 - mean_squared_error: 2.3370 - val_loss: 3.3134 - val_mean_squared_error: 3.3134\n",
      "Epoch 118/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3051 - mean_squared_error: 2.3051 - val_loss: 3.2669 - val_mean_squared_error: 3.2669\n",
      "Epoch 119/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2748 - mean_squared_error: 2.2748 - val_loss: 3.2167 - val_mean_squared_error: 3.2167\n",
      "Epoch 120/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2436 - mean_squared_error: 2.2436 - val_loss: 3.1728 - val_mean_squared_error: 3.1728\n",
      "Epoch 121/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2148 - mean_squared_error: 2.2148 - val_loss: 3.1259 - val_mean_squared_error: 3.1259\n",
      "Epoch 122/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1852 - mean_squared_error: 2.1852 - val_loss: 3.0834 - val_mean_squared_error: 3.0834\n",
      "Epoch 123/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1575 - mean_squared_error: 2.1575 - val_loss: 3.0420 - val_mean_squared_error: 3.0420\n",
      "Epoch 124/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1304 - mean_squared_error: 2.1304 - val_loss: 3.0017 - val_mean_squared_error: 3.0017\n",
      "Epoch 125/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1039 - mean_squared_error: 2.1039 - val_loss: 2.9628 - val_mean_squared_error: 2.9628\n",
      "Epoch 126/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0783 - mean_squared_error: 2.0783 - val_loss: 2.9246 - val_mean_squared_error: 2.9246\n",
      "Epoch 127/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0533 - mean_squared_error: 2.0533 - val_loss: 2.8872 - val_mean_squared_error: 2.8872\n",
      "Epoch 128/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0296 - mean_squared_error: 2.0296 - val_loss: 2.8492 - val_mean_squared_error: 2.8492\n",
      "Epoch 129/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0059 - mean_squared_error: 2.0059 - val_loss: 2.8124 - val_mean_squared_error: 2.8124\n",
      "Epoch 130/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9829 - mean_squared_error: 1.9829 - val_loss: 2.7778 - val_mean_squared_error: 2.7778\n",
      "Epoch 131/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9609 - mean_squared_error: 1.9609 - val_loss: 2.7448 - val_mean_squared_error: 2.7448\n",
      "Epoch 132/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9392 - mean_squared_error: 1.9392 - val_loss: 2.7120 - val_mean_squared_error: 2.7120\n",
      "Epoch 133/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9183 - mean_squared_error: 1.9183 - val_loss: 2.6782 - val_mean_squared_error: 2.6782\n",
      "Epoch 134/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8974 - mean_squared_error: 1.8974 - val_loss: 2.6481 - val_mean_squared_error: 2.6481\n",
      "Epoch 135/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8776 - mean_squared_error: 1.8776 - val_loss: 2.6183 - val_mean_squared_error: 2.6183\n",
      "Epoch 136/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8580 - mean_squared_error: 1.8580 - val_loss: 2.5890 - val_mean_squared_error: 2.5890\n",
      "Epoch 137/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8395 - mean_squared_error: 1.8395 - val_loss: 2.5569 - val_mean_squared_error: 2.5569\n",
      "Epoch 138/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8206 - mean_squared_error: 1.8206 - val_loss: 2.5299 - val_mean_squared_error: 2.5299\n",
      "Epoch 139/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8025 - mean_squared_error: 1.8025 - val_loss: 2.5040 - val_mean_squared_error: 2.5040\n",
      "Epoch 140/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7853 - mean_squared_error: 1.7853 - val_loss: 2.4733 - val_mean_squared_error: 2.4733\n",
      "Epoch 141/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7674 - mean_squared_error: 1.7674 - val_loss: 2.4457 - val_mean_squared_error: 2.4457\n",
      "Epoch 142/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7503 - mean_squared_error: 1.7503 - val_loss: 2.4208 - val_mean_squared_error: 2.4208\n",
      "Epoch 143/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7336 - mean_squared_error: 1.7336 - val_loss: 2.3959 - val_mean_squared_error: 2.3959\n",
      "Epoch 144/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7174 - mean_squared_error: 1.7174 - val_loss: 2.3727 - val_mean_squared_error: 2.3727\n",
      "Epoch 145/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.7020 - mean_squared_error: 1.7020 - val_loss: 2.3438 - val_mean_squared_error: 2.3438\n",
      "Epoch 146/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6862 - mean_squared_error: 1.6862 - val_loss: 2.3235 - val_mean_squared_error: 2.3235\n",
      "Epoch 147/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6714 - mean_squared_error: 1.6714 - val_loss: 2.2980 - val_mean_squared_error: 2.2980\n",
      "Epoch 148/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6563 - mean_squared_error: 1.6563 - val_loss: 2.2746 - val_mean_squared_error: 2.2746\n",
      "Epoch 149/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6421 - mean_squared_error: 1.6421 - val_loss: 2.2547 - val_mean_squared_error: 2.2547\n",
      "Epoch 150/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6283 - mean_squared_error: 1.6283 - val_loss: 2.2299 - val_mean_squared_error: 2.2299\n",
      "Epoch 151/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6145 - mean_squared_error: 1.6145 - val_loss: 2.2101 - val_mean_squared_error: 2.2101\n",
      "Epoch 152/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.6014 - mean_squared_error: 1.6014 - val_loss: 2.1880 - val_mean_squared_error: 2.1880\n",
      "Epoch 153/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5890 - mean_squared_error: 1.5890 - val_loss: 2.1609 - val_mean_squared_error: 2.1609\n",
      "Epoch 154/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.5753 - mean_squared_error: 1.5753 - val_loss: 2.1445 - val_mean_squared_error: 2.1445\n",
      "Epoch 155/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5629 - mean_squared_error: 1.5629 - val_loss: 2.1241 - val_mean_squared_error: 2.1241\n",
      "Epoch 156/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5507 - mean_squared_error: 1.5507 - val_loss: 2.1019 - val_mean_squared_error: 2.1019\n",
      "Epoch 157/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5381 - mean_squared_error: 1.5381 - val_loss: 2.0838 - val_mean_squared_error: 2.0838\n",
      "Epoch 158/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5261 - mean_squared_error: 1.5261 - val_loss: 2.0648 - val_mean_squared_error: 2.0648\n",
      "Epoch 159/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5145 - mean_squared_error: 1.5145 - val_loss: 2.0428 - val_mean_squared_error: 2.0428\n",
      "Epoch 160/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.5025 - mean_squared_error: 1.5025 - val_loss: 2.0234 - val_mean_squared_error: 2.0234\n",
      "Epoch 161/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4909 - mean_squared_error: 1.4909 - val_loss: 2.0066 - val_mean_squared_error: 2.0066\n",
      "Epoch 162/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4800 - mean_squared_error: 1.4800 - val_loss: 1.9866 - val_mean_squared_error: 1.9866\n",
      "Epoch 163/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.4695 - mean_squared_error: 1.4695 - val_loss: 1.9663 - val_mean_squared_error: 1.9663\n",
      "Epoch 164/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4587 - mean_squared_error: 1.4587 - val_loss: 1.9455 - val_mean_squared_error: 1.9455\n",
      "Epoch 165/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4484 - mean_squared_error: 1.4484 - val_loss: 1.9280 - val_mean_squared_error: 1.9280\n",
      "Epoch 166/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4387 - mean_squared_error: 1.4387 - val_loss: 1.9156 - val_mean_squared_error: 1.9156\n",
      "Epoch 167/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4294 - mean_squared_error: 1.4294 - val_loss: 1.8975 - val_mean_squared_error: 1.8975\n",
      "Epoch 168/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4196 - mean_squared_error: 1.4196 - val_loss: 1.8813 - val_mean_squared_error: 1.8813\n",
      "Epoch 169/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4109 - mean_squared_error: 1.4109 - val_loss: 1.8620 - val_mean_squared_error: 1.8620\n",
      "Epoch 170/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.4020 - mean_squared_error: 1.4020 - val_loss: 1.8460 - val_mean_squared_error: 1.8460\n",
      "Epoch 171/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3933 - mean_squared_error: 1.3933 - val_loss: 1.8325 - val_mean_squared_error: 1.8325\n",
      "Epoch 172/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3853 - mean_squared_error: 1.3853 - val_loss: 1.8164 - val_mean_squared_error: 1.8164\n",
      "Epoch 173/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3772 - mean_squared_error: 1.3772 - val_loss: 1.8012 - val_mean_squared_error: 1.8012\n",
      "Epoch 174/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3695 - mean_squared_error: 1.3695 - val_loss: 1.7908 - val_mean_squared_error: 1.7908\n",
      "Epoch 175/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3611 - mean_squared_error: 1.3611 - val_loss: 1.7785 - val_mean_squared_error: 1.7785\n",
      "Epoch 176/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3534 - mean_squared_error: 1.3534 - val_loss: 1.7664 - val_mean_squared_error: 1.7664\n",
      "Epoch 177/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.3454 - mean_squared_error: 1.3454 - val_loss: 1.7500 - val_mean_squared_error: 1.7500\n",
      "Epoch 178/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3377 - mean_squared_error: 1.3377 - val_loss: 1.7337 - val_mean_squared_error: 1.7337\n",
      "Epoch 179/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3299 - mean_squared_error: 1.3299 - val_loss: 1.7222 - val_mean_squared_error: 1.7222\n",
      "Epoch 180/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3225 - mean_squared_error: 1.3225 - val_loss: 1.7066 - val_mean_squared_error: 1.7066\n",
      "Epoch 181/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.3148 - mean_squared_error: 1.3148 - val_loss: 1.6951 - val_mean_squared_error: 1.6951\n",
      "Epoch 182/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3080 - mean_squared_error: 1.3080 - val_loss: 1.6872 - val_mean_squared_error: 1.6872\n",
      "Epoch 183/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.3010 - mean_squared_error: 1.3010 - val_loss: 1.6720 - val_mean_squared_error: 1.6720\n",
      "Epoch 184/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2941 - mean_squared_error: 1.2941 - val_loss: 1.6626 - val_mean_squared_error: 1.6626\n",
      "Epoch 185/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2876 - mean_squared_error: 1.2876 - val_loss: 1.6461 - val_mean_squared_error: 1.6461\n",
      "Epoch 186/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2804 - mean_squared_error: 1.2804 - val_loss: 1.6364 - val_mean_squared_error: 1.6364\n",
      "Epoch 187/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.2735 - mean_squared_error: 1.2735 - val_loss: 1.6228 - val_mean_squared_error: 1.6228\n",
      "Epoch 188/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2668 - mean_squared_error: 1.2668 - val_loss: 1.6085 - val_mean_squared_error: 1.6085\n",
      "Epoch 189/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2595 - mean_squared_error: 1.2595 - val_loss: 1.5993 - val_mean_squared_error: 1.5993\n",
      "Epoch 190/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2531 - mean_squared_error: 1.2531 - val_loss: 1.5878 - val_mean_squared_error: 1.5878\n",
      "Epoch 191/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2464 - mean_squared_error: 1.2464 - val_loss: 1.5782 - val_mean_squared_error: 1.5782\n",
      "Epoch 192/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2401 - mean_squared_error: 1.2401 - val_loss: 1.5680 - val_mean_squared_error: 1.5680\n",
      "Epoch 193/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2343 - mean_squared_error: 1.2343 - val_loss: 1.5575 - val_mean_squared_error: 1.5575\n",
      "Epoch 194/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2283 - mean_squared_error: 1.2283 - val_loss: 1.5459 - val_mean_squared_error: 1.5459\n",
      "Epoch 195/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.2226 - mean_squared_error: 1.2226 - val_loss: 1.5371 - val_mean_squared_error: 1.5371\n",
      "Epoch 196/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2171 - mean_squared_error: 1.2171 - val_loss: 1.5287 - val_mean_squared_error: 1.5287\n",
      "Epoch 197/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2122 - mean_squared_error: 1.2122 - val_loss: 1.5153 - val_mean_squared_error: 1.5153\n",
      "Epoch 198/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.2068 - mean_squared_error: 1.2068 - val_loss: 1.5079 - val_mean_squared_error: 1.5079\n",
      "Epoch 199/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2018 - mean_squared_error: 1.2018 - val_loss: 1.4970 - val_mean_squared_error: 1.4970\n",
      "Epoch 200/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.1970 - mean_squared_error: 1.1970 - val_loss: 1.4881 - val_mean_squared_error: 1.4881\n",
      "Epoch 201/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1921 - mean_squared_error: 1.1921 - val_loss: 1.4787 - val_mean_squared_error: 1.4787\n",
      "Epoch 202/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1873 - mean_squared_error: 1.1873 - val_loss: 1.4695 - val_mean_squared_error: 1.4695\n",
      "Epoch 203/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1825 - mean_squared_error: 1.1825 - val_loss: 1.4636 - val_mean_squared_error: 1.4636\n",
      "Epoch 204/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1776 - mean_squared_error: 1.1776 - val_loss: 1.4526 - val_mean_squared_error: 1.4526\n",
      "Epoch 205/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1733 - mean_squared_error: 1.1733 - val_loss: 1.4441 - val_mean_squared_error: 1.4441\n",
      "Epoch 206/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1690 - mean_squared_error: 1.1690 - val_loss: 1.4356 - val_mean_squared_error: 1.4356\n",
      "Epoch 207/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1648 - mean_squared_error: 1.1648 - val_loss: 1.4271 - val_mean_squared_error: 1.4271\n",
      "Epoch 208/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1604 - mean_squared_error: 1.1604 - val_loss: 1.4185 - val_mean_squared_error: 1.4185\n",
      "Epoch 209/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1561 - mean_squared_error: 1.1561 - val_loss: 1.4084 - val_mean_squared_error: 1.4084\n",
      "Epoch 210/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1516 - mean_squared_error: 1.1516 - val_loss: 1.3985 - val_mean_squared_error: 1.3985\n",
      "Epoch 211/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1468 - mean_squared_error: 1.1468 - val_loss: 1.3929 - val_mean_squared_error: 1.3929\n",
      "Epoch 212/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.1422 - mean_squared_error: 1.1422 - val_loss: 1.3823 - val_mean_squared_error: 1.3823\n",
      "Epoch 213/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1378 - mean_squared_error: 1.1378 - val_loss: 1.3720 - val_mean_squared_error: 1.3720\n",
      "Epoch 214/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1333 - mean_squared_error: 1.1333 - val_loss: 1.3610 - val_mean_squared_error: 1.3610\n",
      "Epoch 215/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1286 - mean_squared_error: 1.1286 - val_loss: 1.3527 - val_mean_squared_error: 1.3527\n",
      "Epoch 216/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1239 - mean_squared_error: 1.1239 - val_loss: 1.3419 - val_mean_squared_error: 1.3419\n",
      "Epoch 217/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1194 - mean_squared_error: 1.1194 - val_loss: 1.3304 - val_mean_squared_error: 1.3304\n",
      "Epoch 218/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1148 - mean_squared_error: 1.1148 - val_loss: 1.3241 - val_mean_squared_error: 1.3241\n",
      "Epoch 219/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1102 - mean_squared_error: 1.1102 - val_loss: 1.3107 - val_mean_squared_error: 1.3107\n",
      "Epoch 220/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1055 - mean_squared_error: 1.1055 - val_loss: 1.3007 - val_mean_squared_error: 1.3007\n",
      "Epoch 221/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.1015 - mean_squared_error: 1.1015 - val_loss: 1.2961 - val_mean_squared_error: 1.2961\n",
      "Epoch 222/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0973 - mean_squared_error: 1.0973 - val_loss: 1.2830 - val_mean_squared_error: 1.2830\n",
      "Epoch 223/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0933 - mean_squared_error: 1.0933 - val_loss: 1.2780 - val_mean_squared_error: 1.2780\n",
      "Epoch 224/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0897 - mean_squared_error: 1.0897 - val_loss: 1.2710 - val_mean_squared_error: 1.2710\n",
      "Epoch 225/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0856 - mean_squared_error: 1.0856 - val_loss: 1.2648 - val_mean_squared_error: 1.2648\n",
      "Epoch 226/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0813 - mean_squared_error: 1.0813 - val_loss: 1.2576 - val_mean_squared_error: 1.2576\n",
      "Epoch 227/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0768 - mean_squared_error: 1.0768 - val_loss: 1.2464 - val_mean_squared_error: 1.2464\n",
      "Epoch 228/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0728 - mean_squared_error: 1.0728 - val_loss: 1.2382 - val_mean_squared_error: 1.2382\n",
      "Epoch 229/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.0681 - mean_squared_error: 1.0681 - val_loss: 1.2304 - val_mean_squared_error: 1.2304\n",
      "Epoch 230/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.0639 - mean_squared_error: 1.0639 - val_loss: 1.2221 - val_mean_squared_error: 1.2221\n",
      "Epoch 231/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0598 - mean_squared_error: 1.0598 - val_loss: 1.2143 - val_mean_squared_error: 1.2143\n",
      "Epoch 232/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.0552 - mean_squared_error: 1.0552 - val_loss: 1.2081 - val_mean_squared_error: 1.2081\n",
      "Epoch 233/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0504 - mean_squared_error: 1.0504 - val_loss: 1.1994 - val_mean_squared_error: 1.1994\n",
      "Epoch 234/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0450 - mean_squared_error: 1.0450 - val_loss: 1.1922 - val_mean_squared_error: 1.1922\n",
      "Epoch 235/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0405 - mean_squared_error: 1.0405 - val_loss: 1.1822 - val_mean_squared_error: 1.1822\n",
      "Epoch 236/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0364 - mean_squared_error: 1.0364 - val_loss: 1.1710 - val_mean_squared_error: 1.1710\n",
      "Epoch 237/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0315 - mean_squared_error: 1.0315 - val_loss: 1.1619 - val_mean_squared_error: 1.1619\n",
      "Epoch 238/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0275 - mean_squared_error: 1.0275 - val_loss: 1.1509 - val_mean_squared_error: 1.1509\n",
      "Epoch 239/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0229 - mean_squared_error: 1.0229 - val_loss: 1.1432 - val_mean_squared_error: 1.1432\n",
      "Epoch 240/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0184 - mean_squared_error: 1.0184 - val_loss: 1.1362 - val_mean_squared_error: 1.1362\n",
      "Epoch 241/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 1.1295 - val_mean_squared_error: 1.1295\n",
      "Epoch 242/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0089 - mean_squared_error: 1.0089 - val_loss: 1.1230 - val_mean_squared_error: 1.1230\n",
      "Epoch 243/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0040 - mean_squared_error: 1.0040 - val_loss: 1.1183 - val_mean_squared_error: 1.1183\n",
      "Epoch 244/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9995 - mean_squared_error: 0.9995 - val_loss: 1.1127 - val_mean_squared_error: 1.1127\n",
      "Epoch 245/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9954 - mean_squared_error: 0.9954 - val_loss: 1.1043 - val_mean_squared_error: 1.1043\n",
      "Epoch 246/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.9918 - mean_squared_error: 0.9918 - val_loss: 1.0998 - val_mean_squared_error: 1.0998\n",
      "Epoch 247/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9883 - mean_squared_error: 0.9883 - val_loss: 1.0919 - val_mean_squared_error: 1.0919\n",
      "Epoch 248/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.9845 - mean_squared_error: 0.9845 - val_loss: 1.0859 - val_mean_squared_error: 1.0859\n",
      "Epoch 249/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9811 - mean_squared_error: 0.9811 - val_loss: 1.0800 - val_mean_squared_error: 1.0800\n",
      "Epoch 250/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9781 - mean_squared_error: 0.9781 - val_loss: 1.0747 - val_mean_squared_error: 1.0747\n",
      "Epoch 251/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9752 - mean_squared_error: 0.9752 - val_loss: 1.0711 - val_mean_squared_error: 1.0711\n",
      "Epoch 252/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9720 - mean_squared_error: 0.9720 - val_loss: 1.0681 - val_mean_squared_error: 1.0681\n",
      "Epoch 253/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9682 - mean_squared_error: 0.9682 - val_loss: 1.0666 - val_mean_squared_error: 1.0666\n",
      "Epoch 254/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9651 - mean_squared_error: 0.9651 - val_loss: 1.0661 - val_mean_squared_error: 1.0661\n",
      "Epoch 255/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9622 - mean_squared_error: 0.9622 - val_loss: 1.0669 - val_mean_squared_error: 1.0669\n",
      "Epoch 256/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9591 - mean_squared_error: 0.9591 - val_loss: 1.0681 - val_mean_squared_error: 1.0681\n",
      "Epoch 257/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9563 - mean_squared_error: 0.9563 - val_loss: 1.0704 - val_mean_squared_error: 1.0704\n",
      "Epoch 258/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9538 - mean_squared_error: 0.9538 - val_loss: 1.0678 - val_mean_squared_error: 1.0678\n",
      "Epoch 259/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9503 - mean_squared_error: 0.9503 - val_loss: 1.0731 - val_mean_squared_error: 1.0731\n",
      "Epoch 260/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9464 - mean_squared_error: 0.9464 - val_loss: 1.0797 - val_mean_squared_error: 1.0797\n",
      "Epoch 261/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9411 - mean_squared_error: 0.9411 - val_loss: 1.0856 - val_mean_squared_error: 1.0856\n",
      "Epoch 262/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9364 - mean_squared_error: 0.9364 - val_loss: 1.0880 - val_mean_squared_error: 1.0880\n",
      "Epoch 263/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.9314 - mean_squared_error: 0.9314 - val_loss: 1.0898 - val_mean_squared_error: 1.0898\n",
      "Epoch 264/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9272 - mean_squared_error: 0.9272 - val_loss: 1.0940 - val_mean_squared_error: 1.0940\n",
      "Epoch 265/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9231 - mean_squared_error: 0.9231 - val_loss: 1.0970 - val_mean_squared_error: 1.0970\n",
      "Epoch 266/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9182 - mean_squared_error: 0.9182 - val_loss: 1.0987 - val_mean_squared_error: 1.0987\n",
      "Epoch 267/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9138 - mean_squared_error: 0.9138 - val_loss: 1.0992 - val_mean_squared_error: 1.0992\n",
      "Epoch 268/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9088 - mean_squared_error: 0.9088 - val_loss: 1.0927 - val_mean_squared_error: 1.0927\n",
      "Epoch 269/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9063 - mean_squared_error: 0.9063 - val_loss: 1.0926 - val_mean_squared_error: 1.0926\n",
      "Epoch 270/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9030 - mean_squared_error: 0.9030 - val_loss: 1.0920 - val_mean_squared_error: 1.0920\n",
      "Epoch 271/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9004 - mean_squared_error: 0.9004 - val_loss: 1.0925 - val_mean_squared_error: 1.0925\n",
      "Epoch 272/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8976 - mean_squared_error: 0.8976 - val_loss: 1.0925 - val_mean_squared_error: 1.0925\n",
      "Epoch 273/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8954 - mean_squared_error: 0.8954 - val_loss: 1.0937 - val_mean_squared_error: 1.0937\n",
      "Epoch 274/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8922 - mean_squared_error: 0.8922 - val_loss: 1.0960 - val_mean_squared_error: 1.0960\n",
      "Epoch 275/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8890 - mean_squared_error: 0.8890 - val_loss: 1.0942 - val_mean_squared_error: 1.0942\n",
      "Epoch 276/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8862 - mean_squared_error: 0.8862 - val_loss: 1.0918 - val_mean_squared_error: 1.0918\n",
      "Epoch 277/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8841 - mean_squared_error: 0.8841 - val_loss: 1.0952 - val_mean_squared_error: 1.0952\n",
      "Epoch 278/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8821 - mean_squared_error: 0.8821 - val_loss: 1.0886 - val_mean_squared_error: 1.0886\n",
      "Epoch 279/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8797 - mean_squared_error: 0.8797 - val_loss: 1.0920 - val_mean_squared_error: 1.0920\n",
      "Epoch 280/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8773 - mean_squared_error: 0.8773 - val_loss: 1.0941 - val_mean_squared_error: 1.0941\n",
      "Epoch 281/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8759 - mean_squared_error: 0.8759 - val_loss: 1.0918 - val_mean_squared_error: 1.0918\n",
      "Epoch 282/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8744 - mean_squared_error: 0.8744 - val_loss: 1.0883 - val_mean_squared_error: 1.0883\n",
      "Epoch 283/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8728 - mean_squared_error: 0.8728 - val_loss: 1.0868 - val_mean_squared_error: 1.0868\n",
      "Epoch 284/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8717 - mean_squared_error: 0.8717 - val_loss: 1.0860 - val_mean_squared_error: 1.0860\n",
      "Epoch 285/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8707 - mean_squared_error: 0.8707 - val_loss: 1.0915 - val_mean_squared_error: 1.0915\n",
      "Epoch 286/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8699 - mean_squared_error: 0.8699 - val_loss: 1.0936 - val_mean_squared_error: 1.0936\n",
      "Epoch 287/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8692 - mean_squared_error: 0.8692 - val_loss: 1.0894 - val_mean_squared_error: 1.0894\n",
      "Epoch 288/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8686 - mean_squared_error: 0.8686 - val_loss: 1.0829 - val_mean_squared_error: 1.0829\n",
      "Epoch 289/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8678 - mean_squared_error: 0.8678 - val_loss: 1.0838 - val_mean_squared_error: 1.0838\n",
      "Epoch 290/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8668 - mean_squared_error: 0.8668 - val_loss: 1.0791 - val_mean_squared_error: 1.0791\n",
      "Epoch 291/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8666 - mean_squared_error: 0.8666 - val_loss: 1.0721 - val_mean_squared_error: 1.0721\n",
      "Epoch 292/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8658 - mean_squared_error: 0.8658 - val_loss: 1.0726 - val_mean_squared_error: 1.0726\n",
      "Epoch 293/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8652 - mean_squared_error: 0.8652 - val_loss: 1.0738 - val_mean_squared_error: 1.0738\n",
      "Epoch 294/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8648 - mean_squared_error: 0.8648 - val_loss: 1.0729 - val_mean_squared_error: 1.0729\n",
      "Epoch 295/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8641 - mean_squared_error: 0.8641 - val_loss: 1.0681 - val_mean_squared_error: 1.0681\n",
      "Epoch 296/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8637 - mean_squared_error: 0.8637 - val_loss: 1.0657 - val_mean_squared_error: 1.0657\n",
      "Epoch 297/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8630 - mean_squared_error: 0.8630 - val_loss: 1.0644 - val_mean_squared_error: 1.0644\n",
      "Epoch 298/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8623 - mean_squared_error: 0.8623 - val_loss: 1.0634 - val_mean_squared_error: 1.0634\n",
      "Epoch 299/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8618 - mean_squared_error: 0.8618 - val_loss: 1.0618 - val_mean_squared_error: 1.0618\n",
      "Epoch 300/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8609 - mean_squared_error: 0.8609 - val_loss: 1.0621 - val_mean_squared_error: 1.0621\n",
      "Epoch 301/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8605 - mean_squared_error: 0.8605 - val_loss: 1.0600 - val_mean_squared_error: 1.0600\n",
      "Epoch 302/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8601 - mean_squared_error: 0.8601 - val_loss: 1.0657 - val_mean_squared_error: 1.0657\n",
      "Epoch 303/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8595 - mean_squared_error: 0.8595 - val_loss: 1.0607 - val_mean_squared_error: 1.0607\n",
      "Epoch 304/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8591 - mean_squared_error: 0.8591 - val_loss: 1.0565 - val_mean_squared_error: 1.0565\n",
      "Epoch 305/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8584 - mean_squared_error: 0.8584 - val_loss: 1.0594 - val_mean_squared_error: 1.0594\n",
      "Epoch 306/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8580 - mean_squared_error: 0.8580 - val_loss: 1.0590 - val_mean_squared_error: 1.0590\n",
      "Epoch 307/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8575 - mean_squared_error: 0.8575 - val_loss: 1.0572 - val_mean_squared_error: 1.0572\n",
      "Epoch 308/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8569 - mean_squared_error: 0.8569 - val_loss: 1.0585 - val_mean_squared_error: 1.0585\n",
      "Epoch 309/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8565 - mean_squared_error: 0.8565 - val_loss: 1.0606 - val_mean_squared_error: 1.0606\n",
      "Epoch 310/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8565 - mean_squared_error: 0.8565 - val_loss: 1.0546 - val_mean_squared_error: 1.0546\n",
      "Epoch 311/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8560 - mean_squared_error: 0.8560 - val_loss: 1.0569 - val_mean_squared_error: 1.0569\n",
      "Epoch 312/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8557 - mean_squared_error: 0.8557 - val_loss: 1.0535 - val_mean_squared_error: 1.0535\n",
      "Epoch 313/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8553 - mean_squared_error: 0.8553 - val_loss: 1.0526 - val_mean_squared_error: 1.0526\n",
      "Epoch 314/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8552 - mean_squared_error: 0.8552 - val_loss: 1.0522 - val_mean_squared_error: 1.0522\n",
      "Epoch 315/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8551 - mean_squared_error: 0.8551 - val_loss: 1.0527 - val_mean_squared_error: 1.0527\n",
      "Epoch 316/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8548 - mean_squared_error: 0.8548 - val_loss: 1.0526 - val_mean_squared_error: 1.0526\n",
      "Epoch 317/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8546 - mean_squared_error: 0.8546 - val_loss: 1.0536 - val_mean_squared_error: 1.0536\n",
      "Epoch 318/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8549 - mean_squared_error: 0.8549 - val_loss: 1.0559 - val_mean_squared_error: 1.0559\n",
      "Epoch 319/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8544 - mean_squared_error: 0.8544 - val_loss: 1.0482 - val_mean_squared_error: 1.0482\n",
      "Epoch 320/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8544 - mean_squared_error: 0.8544 - val_loss: 1.0445 - val_mean_squared_error: 1.0445\n",
      "Epoch 321/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8542 - mean_squared_error: 0.8542 - val_loss: 1.0435 - val_mean_squared_error: 1.0435\n",
      "Epoch 322/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8543 - mean_squared_error: 0.8543 - val_loss: 1.0471 - val_mean_squared_error: 1.0471\n",
      "Epoch 323/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8540 - mean_squared_error: 0.8540 - val_loss: 1.0467 - val_mean_squared_error: 1.0467\n",
      "Epoch 324/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8540 - mean_squared_error: 0.8540 - val_loss: 1.0437 - val_mean_squared_error: 1.0437\n",
      "Epoch 325/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8539 - mean_squared_error: 0.8539 - val_loss: 1.0429 - val_mean_squared_error: 1.0429\n",
      "Epoch 326/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8540 - mean_squared_error: 0.8540 - val_loss: 1.0388 - val_mean_squared_error: 1.0388\n",
      "Epoch 327/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8538 - mean_squared_error: 0.8538 - val_loss: 1.0409 - val_mean_squared_error: 1.0409\n",
      "Epoch 328/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8543 - mean_squared_error: 0.8543 - val_loss: 1.0475 - val_mean_squared_error: 1.0475\n",
      "Epoch 329/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8540 - mean_squared_error: 0.8540 - val_loss: 1.0440 - val_mean_squared_error: 1.0440\n",
      "Epoch 330/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8538 - mean_squared_error: 0.8538 - val_loss: 1.0376 - val_mean_squared_error: 1.0376\n",
      "Epoch 331/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8537 - mean_squared_error: 0.8537 - val_loss: 1.0363 - val_mean_squared_error: 1.0363\n",
      "Epoch 332/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8535 - mean_squared_error: 0.8535 - val_loss: 1.0386 - val_mean_squared_error: 1.0386\n",
      "Epoch 333/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8539 - mean_squared_error: 0.8539 - val_loss: 1.0358 - val_mean_squared_error: 1.0358\n",
      "Epoch 334/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8535 - mean_squared_error: 0.8535 - val_loss: 1.0429 - val_mean_squared_error: 1.0429\n",
      "Epoch 335/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8534 - mean_squared_error: 0.8534 - val_loss: 1.0411 - val_mean_squared_error: 1.0411\n",
      "Epoch 336/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8534 - mean_squared_error: 0.8534 - val_loss: 1.0393 - val_mean_squared_error: 1.0393\n",
      "Epoch 337/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8533 - mean_squared_error: 0.8533 - val_loss: 1.0410 - val_mean_squared_error: 1.0410\n",
      "Epoch 338/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8532 - mean_squared_error: 0.8532 - val_loss: 1.0375 - val_mean_squared_error: 1.0375\n",
      "Epoch 339/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8532 - mean_squared_error: 0.8532 - val_loss: 1.0354 - val_mean_squared_error: 1.0354\n",
      "Epoch 340/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8531 - mean_squared_error: 0.8531 - val_loss: 1.0366 - val_mean_squared_error: 1.0366\n",
      "Epoch 341/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8533 - mean_squared_error: 0.8533 - val_loss: 1.0396 - val_mean_squared_error: 1.0396\n",
      "Epoch 342/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8532 - mean_squared_error: 0.8532 - val_loss: 1.0338 - val_mean_squared_error: 1.0338\n",
      "Epoch 343/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8530 - mean_squared_error: 0.8530 - val_loss: 1.0346 - val_mean_squared_error: 1.0346\n",
      "Epoch 344/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8530 - mean_squared_error: 0.8530 - val_loss: 1.0329 - val_mean_squared_error: 1.0329\n",
      "Epoch 345/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8533 - mean_squared_error: 0.8533 - val_loss: 1.0261 - val_mean_squared_error: 1.0261\n",
      "Epoch 346/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8529 - mean_squared_error: 0.8529 - val_loss: 1.0260 - val_mean_squared_error: 1.0260\n",
      "Epoch 347/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8529 - mean_squared_error: 0.8529 - val_loss: 1.0296 - val_mean_squared_error: 1.0296\n",
      "Epoch 348/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8528 - mean_squared_error: 0.8528 - val_loss: 1.0301 - val_mean_squared_error: 1.0301\n",
      "Epoch 349/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8530 - mean_squared_error: 0.8530 - val_loss: 1.0318 - val_mean_squared_error: 1.0318\n",
      "Epoch 350/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8527 - mean_squared_error: 0.8527 - val_loss: 1.0325 - val_mean_squared_error: 1.0325\n",
      "Epoch 351/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8527 - mean_squared_error: 0.8527 - val_loss: 1.0315 - val_mean_squared_error: 1.0315\n",
      "Epoch 352/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8529 - mean_squared_error: 0.8529 - val_loss: 1.0281 - val_mean_squared_error: 1.0281\n",
      "Epoch 353/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8526 - mean_squared_error: 0.8526 - val_loss: 1.0257 - val_mean_squared_error: 1.0257\n",
      "Epoch 354/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8526 - mean_squared_error: 0.8526 - val_loss: 1.0266 - val_mean_squared_error: 1.0266\n",
      "Epoch 355/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8525 - mean_squared_error: 0.8525 - val_loss: 1.0288 - val_mean_squared_error: 1.0288\n",
      "Epoch 356/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8527 - mean_squared_error: 0.8527 - val_loss: 1.0293 - val_mean_squared_error: 1.0293\n",
      "Epoch 357/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8525 - mean_squared_error: 0.8525 - val_loss: 1.0270 - val_mean_squared_error: 1.0270\n",
      "Epoch 358/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8530 - mean_squared_error: 0.8530 - val_loss: 1.0233 - val_mean_squared_error: 1.0233\n",
      "Epoch 359/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8523 - mean_squared_error: 0.8523 - val_loss: 1.0293 - val_mean_squared_error: 1.0293\n",
      "Epoch 360/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8523 - mean_squared_error: 0.8523 - val_loss: 1.0283 - val_mean_squared_error: 1.0283\n",
      "Epoch 361/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8524 - mean_squared_error: 0.8524 - val_loss: 1.0263 - val_mean_squared_error: 1.0263\n",
      "Epoch 362/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8524 - mean_squared_error: 0.8524 - val_loss: 1.0301 - val_mean_squared_error: 1.0301\n",
      "Epoch 363/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8523 - mean_squared_error: 0.8523 - val_loss: 1.0280 - val_mean_squared_error: 1.0280\n",
      "Epoch 364/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8522 - mean_squared_error: 0.8522 - val_loss: 1.0279 - val_mean_squared_error: 1.0279\n",
      "Epoch 365/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8526 - mean_squared_error: 0.8526 - val_loss: 1.0298 - val_mean_squared_error: 1.0298\n",
      "Epoch 366/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8523 - mean_squared_error: 0.8523 - val_loss: 1.0235 - val_mean_squared_error: 1.0235\n",
      "Epoch 367/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8522 - mean_squared_error: 0.8522 - val_loss: 1.0237 - val_mean_squared_error: 1.0237\n",
      "Epoch 368/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8523 - mean_squared_error: 0.8523 - val_loss: 1.0223 - val_mean_squared_error: 1.0223\n",
      "Epoch 369/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8524 - mean_squared_error: 0.8524 - val_loss: 1.0236 - val_mean_squared_error: 1.0236\n",
      "Epoch 370/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8521 - mean_squared_error: 0.8521 - val_loss: 1.0211 - val_mean_squared_error: 1.0211\n",
      "Epoch 371/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8522 - mean_squared_error: 0.8522 - val_loss: 1.0187 - val_mean_squared_error: 1.0187\n",
      "Epoch 372/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8521 - mean_squared_error: 0.8521 - val_loss: 1.0183 - val_mean_squared_error: 1.0183\n",
      "Epoch 373/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8522 - mean_squared_error: 0.8522 - val_loss: 1.0228 - val_mean_squared_error: 1.0228\n",
      "Epoch 374/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8521 - mean_squared_error: 0.8521 - val_loss: 1.0206 - val_mean_squared_error: 1.0206\n",
      "Epoch 375/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8521 - mean_squared_error: 0.8521 - val_loss: 1.0177 - val_mean_squared_error: 1.0177\n",
      "Epoch 376/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8520 - mean_squared_error: 0.8520 - val_loss: 1.0214 - val_mean_squared_error: 1.0214\n",
      "Epoch 377/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8519 - mean_squared_error: 0.8519 - val_loss: 1.0206 - val_mean_squared_error: 1.0206\n",
      "Epoch 378/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8520 - mean_squared_error: 0.8520 - val_loss: 1.0183 - val_mean_squared_error: 1.0183\n",
      "Epoch 379/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8519 - mean_squared_error: 0.8519 - val_loss: 1.0208 - val_mean_squared_error: 1.0208\n",
      "Epoch 380/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8519 - mean_squared_error: 0.8519 - val_loss: 1.0192 - val_mean_squared_error: 1.0192\n",
      "Epoch 381/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8519 - mean_squared_error: 0.8519 - val_loss: 1.0198 - val_mean_squared_error: 1.0198\n",
      "Epoch 382/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8519 - mean_squared_error: 0.8519 - val_loss: 1.0184 - val_mean_squared_error: 1.0184\n",
      "Epoch 383/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8518 - mean_squared_error: 0.8518 - val_loss: 1.0185 - val_mean_squared_error: 1.0185\n",
      "Epoch 384/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8522 - mean_squared_error: 0.8522 - val_loss: 1.0216 - val_mean_squared_error: 1.0216\n",
      "Epoch 385/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8521 - mean_squared_error: 0.8521 - val_loss: 1.0178 - val_mean_squared_error: 1.0178\n",
      "Epoch 386/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8517 - mean_squared_error: 0.8517 - val_loss: 1.0183 - val_mean_squared_error: 1.0183\n",
      "Epoch 387/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8517 - mean_squared_error: 0.8517 - val_loss: 1.0174 - val_mean_squared_error: 1.0174\n",
      "Epoch 388/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8516 - mean_squared_error: 0.8516 - val_loss: 1.0174 - val_mean_squared_error: 1.0174\n",
      "Epoch 389/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8517 - mean_squared_error: 0.8517 - val_loss: 1.0190 - val_mean_squared_error: 1.0190\n",
      "Epoch 390/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8516 - mean_squared_error: 0.8516 - val_loss: 1.0170 - val_mean_squared_error: 1.0170\n",
      "Epoch 391/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8517 - mean_squared_error: 0.8517 - val_loss: 1.0197 - val_mean_squared_error: 1.0197\n",
      "Epoch 392/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8517 - mean_squared_error: 0.8517 - val_loss: 1.0172 - val_mean_squared_error: 1.0172\n",
      "Epoch 393/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8517 - mean_squared_error: 0.8517 - val_loss: 1.0184 - val_mean_squared_error: 1.0184\n",
      "Epoch 394/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8515 - mean_squared_error: 0.8515 - val_loss: 1.0184 - val_mean_squared_error: 1.0184\n",
      "Epoch 395/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8516 - mean_squared_error: 0.8516 - val_loss: 1.0181 - val_mean_squared_error: 1.0181\n",
      "Epoch 396/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8516 - mean_squared_error: 0.8516 - val_loss: 1.0170 - val_mean_squared_error: 1.0170\n",
      "Epoch 397/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8516 - mean_squared_error: 0.8516 - val_loss: 1.0184 - val_mean_squared_error: 1.0184\n",
      "Epoch 398/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8514 - mean_squared_error: 0.8514 - val_loss: 1.0166 - val_mean_squared_error: 1.0166\n",
      "Epoch 399/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8514 - mean_squared_error: 0.8514 - val_loss: 1.0165 - val_mean_squared_error: 1.0165\n",
      "Epoch 400/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8516 - mean_squared_error: 0.8516 - val_loss: 1.0174 - val_mean_squared_error: 1.0174\n",
      "Epoch 401/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8517 - mean_squared_error: 0.8517 - val_loss: 1.0123 - val_mean_squared_error: 1.0123\n",
      "Epoch 402/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8515 - mean_squared_error: 0.8515 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "Epoch 403/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8516 - mean_squared_error: 0.8516 - val_loss: 1.0164 - val_mean_squared_error: 1.0164\n",
      "Epoch 404/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8514 - mean_squared_error: 0.8514 - val_loss: 1.0143 - val_mean_squared_error: 1.0143\n",
      "Epoch 405/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8514 - mean_squared_error: 0.8514 - val_loss: 1.0140 - val_mean_squared_error: 1.0140\n",
      "Epoch 406/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8513 - mean_squared_error: 0.8513 - val_loss: 1.0159 - val_mean_squared_error: 1.0159\n",
      "Epoch 407/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8517 - mean_squared_error: 0.8517 - val_loss: 1.0173 - val_mean_squared_error: 1.0173\n",
      "Epoch 408/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8514 - mean_squared_error: 0.8514 - val_loss: 1.0158 - val_mean_squared_error: 1.0158\n",
      "Epoch 409/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8514 - mean_squared_error: 0.8514 - val_loss: 1.0165 - val_mean_squared_error: 1.0165\n",
      "Epoch 410/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8514 - mean_squared_error: 0.8514 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "Epoch 411/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8512 - mean_squared_error: 0.8512 - val_loss: 1.0101 - val_mean_squared_error: 1.0101\n",
      "Epoch 412/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8515 - mean_squared_error: 0.8515 - val_loss: 1.0128 - val_mean_squared_error: 1.0128\n",
      "Epoch 413/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8512 - mean_squared_error: 0.8512 - val_loss: 1.0074 - val_mean_squared_error: 1.0074\n",
      "Epoch 414/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8512 - mean_squared_error: 0.8512 - val_loss: 1.0095 - val_mean_squared_error: 1.0095\n",
      "Epoch 415/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8515 - mean_squared_error: 0.8515 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "Epoch 416/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8511 - mean_squared_error: 0.8511 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "Epoch 417/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8510 - mean_squared_error: 0.8510 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "Epoch 418/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8510 - mean_squared_error: 0.8510 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "Epoch 419/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8512 - mean_squared_error: 0.8512 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "Epoch 420/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8511 - mean_squared_error: 0.8511 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "Epoch 421/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8509 - mean_squared_error: 0.8509 - val_loss: 1.0089 - val_mean_squared_error: 1.0089\n",
      "Epoch 422/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8509 - mean_squared_error: 0.8509 - val_loss: 1.0108 - val_mean_squared_error: 1.0108\n",
      "Epoch 423/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8513 - mean_squared_error: 0.8513 - val_loss: 1.0149 - val_mean_squared_error: 1.0149\n",
      "Epoch 424/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8514 - mean_squared_error: 0.8514 - val_loss: 1.0152 - val_mean_squared_error: 1.0152\n",
      "Epoch 425/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8510 - mean_squared_error: 0.8510 - val_loss: 1.0102 - val_mean_squared_error: 1.0102\n",
      "Epoch 426/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8509 - mean_squared_error: 0.8509 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\n",
      "Epoch 427/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8510 - mean_squared_error: 0.8510 - val_loss: 1.0119 - val_mean_squared_error: 1.0119\n",
      "Epoch 428/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8511 - mean_squared_error: 0.8511 - val_loss: 1.0114 - val_mean_squared_error: 1.0114\n",
      "Epoch 429/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8508 - mean_squared_error: 0.8508 - val_loss: 1.0126 - val_mean_squared_error: 1.0126\n",
      "Epoch 430/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8508 - mean_squared_error: 0.8508 - val_loss: 1.0093 - val_mean_squared_error: 1.0093\n",
      "Epoch 431/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8511 - mean_squared_error: 0.8511 - val_loss: 1.0068 - val_mean_squared_error: 1.0068\n",
      "Epoch 432/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8509 - mean_squared_error: 0.8509 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "Epoch 433/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8508 - mean_squared_error: 0.8508 - val_loss: 1.0071 - val_mean_squared_error: 1.0071\n",
      "Epoch 434/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8515 - mean_squared_error: 0.8515 - val_loss: 1.0141 - val_mean_squared_error: 1.0141\n",
      "Epoch 435/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8509 - mean_squared_error: 0.8509 - val_loss: 1.0139 - val_mean_squared_error: 1.0139\n",
      "Epoch 436/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8507 - mean_squared_error: 0.8507 - val_loss: 1.0100 - val_mean_squared_error: 1.0100\n",
      "Epoch 437/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8513 - mean_squared_error: 0.8513 - val_loss: 1.0042 - val_mean_squared_error: 1.0042\n",
      "Epoch 438/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8506 - mean_squared_error: 0.8506 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "Epoch 439/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8506 - mean_squared_error: 0.8506 - val_loss: 1.0103 - val_mean_squared_error: 1.0103\n",
      "Epoch 440/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8512 - mean_squared_error: 0.8512 - val_loss: 1.0047 - val_mean_squared_error: 1.0047\n",
      "Epoch 441/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8508 - mean_squared_error: 0.8508 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "Epoch 442/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8506 - mean_squared_error: 0.8506 - val_loss: 1.0067 - val_mean_squared_error: 1.0067\n",
      "Epoch 443/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8508 - mean_squared_error: 0.8508 - val_loss: 1.0122 - val_mean_squared_error: 1.0122\n",
      "Epoch 444/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8507 - mean_squared_error: 0.8507 - val_loss: 1.0100 - val_mean_squared_error: 1.0100\n",
      "Epoch 445/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8504 - mean_squared_error: 0.8504 - val_loss: 1.0097 - val_mean_squared_error: 1.0097\n",
      "Epoch 446/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8504 - mean_squared_error: 0.8504 - val_loss: 1.0092 - val_mean_squared_error: 1.0092\n",
      "Epoch 447/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8504 - mean_squared_error: 0.8504 - val_loss: 1.0056 - val_mean_squared_error: 1.0056\n",
      "Epoch 448/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8505 - mean_squared_error: 0.8505 - val_loss: 1.0080 - val_mean_squared_error: 1.0080\n",
      "Epoch 449/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8505 - mean_squared_error: 0.8505 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "Epoch 450/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8504 - mean_squared_error: 0.8504 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "Epoch 451/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8506 - mean_squared_error: 0.8506 - val_loss: 1.0041 - val_mean_squared_error: 1.0041\n",
      "Epoch 452/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8504 - mean_squared_error: 0.8504 - val_loss: 1.0060 - val_mean_squared_error: 1.0060\n",
      "Epoch 453/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8503 - mean_squared_error: 0.8503 - val_loss: 1.0043 - val_mean_squared_error: 1.0043\n",
      "Epoch 454/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8510 - mean_squared_error: 0.8510 - val_loss: 1.0082 - val_mean_squared_error: 1.0082\n",
      "Epoch 455/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8502 - mean_squared_error: 0.8502 - val_loss: 1.0055 - val_mean_squared_error: 1.0055\n",
      "Epoch 456/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8505 - mean_squared_error: 0.8505 - val_loss: 1.0010 - val_mean_squared_error: 1.0010\n",
      "Epoch 457/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8503 - mean_squared_error: 0.8503 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "Epoch 458/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8503 - mean_squared_error: 0.8503 - val_loss: 1.0057 - val_mean_squared_error: 1.0057\n",
      "Epoch 459/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8503 - mean_squared_error: 0.8503 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "Epoch 460/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8501 - mean_squared_error: 0.8501 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "Epoch 461/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8503 - mean_squared_error: 0.8503 - val_loss: 1.0026 - val_mean_squared_error: 1.0026\n",
      "Epoch 462/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8503 - mean_squared_error: 0.8503 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "Epoch 463/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8503 - mean_squared_error: 0.8503 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "Epoch 464/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8503 - mean_squared_error: 0.8503 - val_loss: 1.0021 - val_mean_squared_error: 1.0021\n",
      "Epoch 465/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8502 - mean_squared_error: 0.8502 - val_loss: 1.0058 - val_mean_squared_error: 1.0058\n",
      "Epoch 466/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8503 - mean_squared_error: 0.8503 - val_loss: 1.0025 - val_mean_squared_error: 1.0025\n",
      "Epoch 467/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8500 - mean_squared_error: 0.8500 - val_loss: 1.0012 - val_mean_squared_error: 1.0012\n",
      "Epoch 468/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8503 - mean_squared_error: 0.8503 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "Epoch 469/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8500 - mean_squared_error: 0.8500 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "Epoch 470/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8503 - mean_squared_error: 0.8503 - val_loss: 1.0072 - val_mean_squared_error: 1.0072\n",
      "Epoch 471/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8504 - mean_squared_error: 0.8504 - val_loss: 1.0100 - val_mean_squared_error: 1.0100\n",
      "Epoch 472/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8502 - mean_squared_error: 0.8502 - val_loss: 1.0038 - val_mean_squared_error: 1.0038\n",
      "Epoch 473/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8499 - mean_squared_error: 0.8499 - val_loss: 1.0031 - val_mean_squared_error: 1.0031\n",
      "Epoch 474/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8501 - mean_squared_error: 0.8501 - val_loss: 0.9983 - val_mean_squared_error: 0.9983\n",
      "Epoch 475/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8500 - mean_squared_error: 0.8500 - val_loss: 0.9974 - val_mean_squared_error: 0.9974\n",
      "Epoch 476/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8511 - mean_squared_error: 0.8511 - val_loss: 1.0085 - val_mean_squared_error: 1.0085\n",
      "Epoch 477/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8504 - mean_squared_error: 0.8504 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "Epoch 478/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8500 - mean_squared_error: 0.8500 - val_loss: 1.0013 - val_mean_squared_error: 1.0013\n",
      "Epoch 479/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8503 - mean_squared_error: 0.8503 - val_loss: 1.0031 - val_mean_squared_error: 1.0031\n",
      "Epoch 480/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8497 - mean_squared_error: 0.8497 - val_loss: 1.0009 - val_mean_squared_error: 1.0009\n",
      "Epoch 481/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8497 - mean_squared_error: 0.8497 - val_loss: 0.9984 - val_mean_squared_error: 0.9984\n",
      "Epoch 482/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8505 - mean_squared_error: 0.8505 - val_loss: 1.0034 - val_mean_squared_error: 1.0034\n",
      "Epoch 483/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8502 - mean_squared_error: 0.8502 - val_loss: 0.9933 - val_mean_squared_error: 0.9933\n",
      "Epoch 484/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8498 - mean_squared_error: 0.8498 - val_loss: 0.9960 - val_mean_squared_error: 0.9960\n",
      "Epoch 485/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8497 - mean_squared_error: 0.8497 - val_loss: 0.9977 - val_mean_squared_error: 0.9977\n",
      "Epoch 486/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8497 - mean_squared_error: 0.8497 - val_loss: 1.0020 - val_mean_squared_error: 1.0020\n",
      "Epoch 487/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8496 - mean_squared_error: 0.8496 - val_loss: 1.0009 - val_mean_squared_error: 1.0009\n",
      "Epoch 488/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8506 - mean_squared_error: 0.8506 - val_loss: 1.0086 - val_mean_squared_error: 1.0086\n",
      "Epoch 489/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8506 - mean_squared_error: 0.8506 - val_loss: 0.9981 - val_mean_squared_error: 0.9981\n",
      "Epoch 490/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8496 - mean_squared_error: 0.8496 - val_loss: 0.9989 - val_mean_squared_error: 0.9989\n",
      "Epoch 491/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8496 - mean_squared_error: 0.8496 - val_loss: 0.9996 - val_mean_squared_error: 0.9996\n",
      "Epoch 492/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8498 - mean_squared_error: 0.8498 - val_loss: 1.0066 - val_mean_squared_error: 1.0066\n",
      "Epoch 493/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8496 - mean_squared_error: 0.8496 - val_loss: 1.0037 - val_mean_squared_error: 1.0037\n",
      "Epoch 494/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8498 - mean_squared_error: 0.8498 - val_loss: 1.0052 - val_mean_squared_error: 1.0052\n",
      "Epoch 495/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8495 - mean_squared_error: 0.8495 - val_loss: 0.9994 - val_mean_squared_error: 0.9994\n",
      "Epoch 496/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8503 - mean_squared_error: 0.8503 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "Epoch 497/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8495 - mean_squared_error: 0.8495 - val_loss: 0.9961 - val_mean_squared_error: 0.9961\n",
      "Epoch 498/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8498 - mean_squared_error: 0.8498 - val_loss: 1.0001 - val_mean_squared_error: 1.0001\n",
      "Epoch 499/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8494 - mean_squared_error: 0.8494 - val_loss: 0.9998 - val_mean_squared_error: 0.9998\n",
      "Epoch 500/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8504 - mean_squared_error: 0.8504 - val_loss: 0.9932 - val_mean_squared_error: 0.9932\n",
      "Epoch 501/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8499 - mean_squared_error: 0.8499 - val_loss: 1.0010 - val_mean_squared_error: 1.0010\n",
      "Epoch 502/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8498 - mean_squared_error: 0.8498 - val_loss: 0.9969 - val_mean_squared_error: 0.9969\n",
      "Epoch 503/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8494 - mean_squared_error: 0.8494 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "Epoch 504/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8495 - mean_squared_error: 0.8495 - val_loss: 0.9955 - val_mean_squared_error: 0.9955\n",
      "Epoch 505/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8498 - mean_squared_error: 0.8498 - val_loss: 1.0021 - val_mean_squared_error: 1.0021\n",
      "Epoch 506/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8493 - mean_squared_error: 0.8493 - val_loss: 0.9980 - val_mean_squared_error: 0.9980\n",
      "Epoch 507/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8494 - mean_squared_error: 0.8494 - val_loss: 0.9998 - val_mean_squared_error: 0.9998\n",
      "Epoch 508/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8492 - mean_squared_error: 0.8492 - val_loss: 0.9986 - val_mean_squared_error: 0.9986\n",
      "Epoch 509/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8498 - mean_squared_error: 0.8498 - val_loss: 1.0007 - val_mean_squared_error: 1.0007\n",
      "Epoch 510/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8494 - mean_squared_error: 0.8494 - val_loss: 0.9926 - val_mean_squared_error: 0.9926\n",
      "Epoch 511/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8492 - mean_squared_error: 0.8492 - val_loss: 0.9937 - val_mean_squared_error: 0.9937\n",
      "Epoch 512/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8504 - mean_squared_error: 0.8504 - val_loss: 1.0024 - val_mean_squared_error: 1.0024\n",
      "Epoch 513/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8493 - mean_squared_error: 0.8493 - val_loss: 0.9951 - val_mean_squared_error: 0.9951\n",
      "Epoch 514/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8491 - mean_squared_error: 0.8491 - val_loss: 0.9955 - val_mean_squared_error: 0.9955\n",
      "Epoch 515/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8494 - mean_squared_error: 0.8494 - val_loss: 0.9925 - val_mean_squared_error: 0.9925\n",
      "Epoch 516/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8491 - mean_squared_error: 0.8491 - val_loss: 0.9976 - val_mean_squared_error: 0.9976\n",
      "Epoch 517/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8494 - mean_squared_error: 0.8494 - val_loss: 0.9980 - val_mean_squared_error: 0.9980\n",
      "Epoch 518/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8493 - mean_squared_error: 0.8493 - val_loss: 0.9948 - val_mean_squared_error: 0.9948\n",
      "Epoch 519/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8491 - mean_squared_error: 0.8491 - val_loss: 0.9942 - val_mean_squared_error: 0.9942\n",
      "Epoch 520/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8490 - mean_squared_error: 0.8490 - val_loss: 0.9959 - val_mean_squared_error: 0.9959\n",
      "Epoch 521/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8500 - mean_squared_error: 0.8500 - val_loss: 1.0039 - val_mean_squared_error: 1.0039\n",
      "Epoch 522/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8489 - mean_squared_error: 0.8489 - val_loss: 0.9957 - val_mean_squared_error: 0.9957\n",
      "Epoch 523/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8491 - mean_squared_error: 0.8491 - val_loss: 0.9912 - val_mean_squared_error: 0.9912\n",
      "Epoch 524/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8490 - mean_squared_error: 0.8490 - val_loss: 0.9956 - val_mean_squared_error: 0.9956\n",
      "Epoch 525/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8499 - mean_squared_error: 0.8499 - val_loss: 0.9896 - val_mean_squared_error: 0.9896\n",
      "Epoch 526/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8498 - mean_squared_error: 0.8498 - val_loss: 1.0018 - val_mean_squared_error: 1.0018\n",
      "Epoch 527/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8492 - mean_squared_error: 0.8492 - val_loss: 1.0022 - val_mean_squared_error: 1.0022\n",
      "Epoch 528/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8488 - mean_squared_error: 0.8488 - val_loss: 0.9979 - val_mean_squared_error: 0.9979\n",
      "Epoch 529/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8500 - mean_squared_error: 0.8500 - val_loss: 0.9886 - val_mean_squared_error: 0.9886\n",
      "Epoch 530/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8487 - mean_squared_error: 0.8487 - val_loss: 0.9938 - val_mean_squared_error: 0.9938\n",
      "Epoch 531/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8488 - mean_squared_error: 0.8488 - val_loss: 1.0028 - val_mean_squared_error: 1.0028\n",
      "Epoch 532/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8488 - mean_squared_error: 0.8488 - val_loss: 1.0023 - val_mean_squared_error: 1.0023\n",
      "Epoch 533/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8488 - mean_squared_error: 0.8488 - val_loss: 0.9968 - val_mean_squared_error: 0.9968\n",
      "Epoch 534/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8487 - mean_squared_error: 0.8487 - val_loss: 0.9956 - val_mean_squared_error: 0.9956\n",
      "Epoch 535/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8488 - mean_squared_error: 0.8488 - val_loss: 0.9932 - val_mean_squared_error: 0.9932\n",
      "Epoch 536/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8488 - mean_squared_error: 0.8488 - val_loss: 0.9893 - val_mean_squared_error: 0.9893\n",
      "Epoch 537/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8489 - mean_squared_error: 0.8489 - val_loss: 0.9912 - val_mean_squared_error: 0.9912\n",
      "Epoch 538/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8490 - mean_squared_error: 0.8490 - val_loss: 0.9872 - val_mean_squared_error: 0.9872\n",
      "Epoch 539/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8485 - mean_squared_error: 0.8485 - val_loss: 0.9944 - val_mean_squared_error: 0.9944\n",
      "Epoch 540/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8487 - mean_squared_error: 0.8487 - val_loss: 0.9983 - val_mean_squared_error: 0.9983\n",
      "Epoch 541/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8486 - mean_squared_error: 0.8486 - val_loss: 0.9944 - val_mean_squared_error: 0.9944\n",
      "Epoch 542/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8493 - mean_squared_error: 0.8493 - val_loss: 0.9952 - val_mean_squared_error: 0.9952\n",
      "Epoch 543/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8485 - mean_squared_error: 0.8485 - val_loss: 0.9935 - val_mean_squared_error: 0.9935\n",
      "Epoch 544/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8489 - mean_squared_error: 0.8489 - val_loss: 0.9887 - val_mean_squared_error: 0.9887\n",
      "Epoch 545/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8488 - mean_squared_error: 0.8488 - val_loss: 0.9963 - val_mean_squared_error: 0.9963\n",
      "Epoch 546/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8487 - mean_squared_error: 0.8487 - val_loss: 0.9956 - val_mean_squared_error: 0.9956\n",
      "Epoch 547/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8484 - mean_squared_error: 0.8484 - val_loss: 0.9922 - val_mean_squared_error: 0.9922\n",
      "Epoch 548/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8485 - mean_squared_error: 0.8485 - val_loss: 0.9914 - val_mean_squared_error: 0.9914\n",
      "Epoch 549/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8490 - mean_squared_error: 0.8490 - val_loss: 0.9890 - val_mean_squared_error: 0.9890\n",
      "Epoch 550/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8484 - mean_squared_error: 0.8484 - val_loss: 0.9949 - val_mean_squared_error: 0.9949\n",
      "Epoch 551/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8485 - mean_squared_error: 0.8485 - val_loss: 0.9947 - val_mean_squared_error: 0.9947\n",
      "Epoch 552/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8484 - mean_squared_error: 0.8484 - val_loss: 0.9935 - val_mean_squared_error: 0.9935\n",
      "Epoch 553/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8485 - mean_squared_error: 0.8485 - val_loss: 0.9941 - val_mean_squared_error: 0.9941\n",
      "Epoch 554/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8486 - mean_squared_error: 0.8486 - val_loss: 0.9911 - val_mean_squared_error: 0.9911\n",
      "Epoch 555/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8487 - mean_squared_error: 0.8487 - val_loss: 0.9914 - val_mean_squared_error: 0.9914\n",
      "Epoch 556/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8487 - mean_squared_error: 0.8487 - val_loss: 0.9867 - val_mean_squared_error: 0.9867\n",
      "Epoch 557/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8484 - mean_squared_error: 0.8484 - val_loss: 0.9960 - val_mean_squared_error: 0.9960\n",
      "Epoch 558/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8494 - mean_squared_error: 0.8494 - val_loss: 0.9906 - val_mean_squared_error: 0.9906\n",
      "Epoch 559/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8482 - mean_squared_error: 0.8482 - val_loss: 0.9927 - val_mean_squared_error: 0.9927\n",
      "Epoch 560/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8486 - mean_squared_error: 0.8486 - val_loss: 0.9973 - val_mean_squared_error: 0.9973\n",
      "Epoch 561/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8482 - mean_squared_error: 0.8482 - val_loss: 0.9900 - val_mean_squared_error: 0.9900\n",
      "Epoch 562/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8483 - mean_squared_error: 0.8483 - val_loss: 0.9888 - val_mean_squared_error: 0.9888\n",
      "Epoch 563/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8484 - mean_squared_error: 0.8484 - val_loss: 0.9873 - val_mean_squared_error: 0.9873\n",
      "Epoch 564/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8494 - mean_squared_error: 0.8494 - val_loss: 1.0000 - val_mean_squared_error: 1.0000\n",
      "Epoch 565/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8483 - mean_squared_error: 0.8483 - val_loss: 0.9961 - val_mean_squared_error: 0.9961\n",
      "Epoch 566/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8490 - mean_squared_error: 0.8490 - val_loss: 0.9891 - val_mean_squared_error: 0.9891\n",
      "Epoch 567/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8482 - mean_squared_error: 0.8482 - val_loss: 0.9919 - val_mean_squared_error: 0.9919\n",
      "Epoch 568/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8487 - mean_squared_error: 0.8487 - val_loss: 0.9905 - val_mean_squared_error: 0.9905\n",
      "Epoch 569/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8483 - mean_squared_error: 0.8483 - val_loss: 0.9983 - val_mean_squared_error: 0.9983\n",
      "Epoch 570/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8482 - mean_squared_error: 0.8482 - val_loss: 0.9929 - val_mean_squared_error: 0.9929\n",
      "Epoch 571/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8480 - mean_squared_error: 0.8480 - val_loss: 0.9929 - val_mean_squared_error: 0.9929\n",
      "Epoch 572/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8480 - mean_squared_error: 0.8480 - val_loss: 0.9907 - val_mean_squared_error: 0.9907\n",
      "Epoch 573/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8481 - mean_squared_error: 0.8481 - val_loss: 0.9932 - val_mean_squared_error: 0.9932\n",
      "Epoch 574/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8484 - mean_squared_error: 0.8484 - val_loss: 0.9891 - val_mean_squared_error: 0.9891\n",
      "Epoch 575/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8485 - mean_squared_error: 0.8485 - val_loss: 0.9884 - val_mean_squared_error: 0.9884\n",
      "Epoch 576/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8480 - mean_squared_error: 0.8480 - val_loss: 0.9931 - val_mean_squared_error: 0.9931\n",
      "Epoch 577/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8480 - mean_squared_error: 0.8480 - val_loss: 0.9940 - val_mean_squared_error: 0.9940\n",
      "Epoch 578/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8493 - mean_squared_error: 0.8493 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "Epoch 579/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8497 - mean_squared_error: 0.8497 - val_loss: 0.9806 - val_mean_squared_error: 0.9806\n",
      "Epoch 580/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.8482 - mean_squared_error: 0.8482 - val_loss: 0.9868 - val_mean_squared_error: 0.9868\n",
      "Epoch 581/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8478 - mean_squared_error: 0.8478 - val_loss: 0.9926 - val_mean_squared_error: 0.9926\n",
      "Epoch 582/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.8482 - mean_squared_error: 0.8482 - val_loss: 0.9896 - val_mean_squared_error: 0.9896\n",
      "Epoch 583/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8478 - mean_squared_error: 0.8478 - val_loss: 0.9926 - val_mean_squared_error: 0.9926\n",
      "Epoch 584/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8478 - mean_squared_error: 0.8478 - val_loss: 0.9924 - val_mean_squared_error: 0.9924\n",
      "Epoch 585/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8479 - mean_squared_error: 0.8479 - val_loss: 0.9888 - val_mean_squared_error: 0.9888\n",
      "Epoch 586/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8479 - mean_squared_error: 0.8479 - val_loss: 0.9938 - val_mean_squared_error: 0.9938\n",
      "Epoch 587/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8482 - mean_squared_error: 0.8482 - val_loss: 0.9965 - val_mean_squared_error: 0.9965\n",
      "Epoch 588/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8485 - mean_squared_error: 0.8485 - val_loss: 0.9873 - val_mean_squared_error: 0.9873\n",
      "Epoch 589/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8476 - mean_squared_error: 0.8476 - val_loss: 0.9882 - val_mean_squared_error: 0.9882\n",
      "Epoch 590/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8477 - mean_squared_error: 0.8477 - val_loss: 0.9916 - val_mean_squared_error: 0.9916\n",
      "Epoch 591/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8486 - mean_squared_error: 0.8486 - val_loss: 0.9980 - val_mean_squared_error: 0.9980\n",
      "Epoch 592/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8477 - mean_squared_error: 0.8477 - val_loss: 0.9877 - val_mean_squared_error: 0.9877\n",
      "Epoch 593/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8483 - mean_squared_error: 0.8483 - val_loss: 0.9927 - val_mean_squared_error: 0.9927\n",
      "Epoch 594/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8486 - mean_squared_error: 0.8486 - val_loss: 0.9825 - val_mean_squared_error: 0.9825\n",
      "Epoch 595/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8482 - mean_squared_error: 0.8482 - val_loss: 0.9855 - val_mean_squared_error: 0.9855\n",
      "Epoch 596/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8479 - mean_squared_error: 0.8479 - val_loss: 0.9896 - val_mean_squared_error: 0.9896\n",
      "Epoch 597/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8477 - mean_squared_error: 0.8477 - val_loss: 0.9887 - val_mean_squared_error: 0.9887\n",
      "Epoch 598/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8477 - mean_squared_error: 0.8477 - val_loss: 0.9959 - val_mean_squared_error: 0.9959\n",
      "Epoch 599/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8486 - mean_squared_error: 0.8486 - val_loss: 0.9868 - val_mean_squared_error: 0.9868\n",
      "Epoch 600/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8477 - mean_squared_error: 0.8477 - val_loss: 0.9956 - val_mean_squared_error: 0.9956\n",
      "Epoch 601/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8484 - mean_squared_error: 0.8484 - val_loss: 0.9945 - val_mean_squared_error: 0.9945\n",
      "Epoch 602/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8480 - mean_squared_error: 0.8480 - val_loss: 0.9846 - val_mean_squared_error: 0.9846\n",
      "Epoch 603/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8486 - mean_squared_error: 0.8486 - val_loss: 0.9915 - val_mean_squared_error: 0.9915\n",
      "Epoch 604/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8477 - mean_squared_error: 0.8477 - val_loss: 0.9897 - val_mean_squared_error: 0.9897\n",
      "Epoch 605/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8476 - mean_squared_error: 0.8476 - val_loss: 0.9895 - val_mean_squared_error: 0.9895\n",
      "Epoch 606/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8484 - mean_squared_error: 0.8484 - val_loss: 0.9822 - val_mean_squared_error: 0.9822\n",
      "Epoch 607/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8474 - mean_squared_error: 0.8474 - val_loss: 0.9883 - val_mean_squared_error: 0.9883\n",
      "Epoch 608/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8480 - mean_squared_error: 0.8480 - val_loss: 0.9987 - val_mean_squared_error: 0.9987\n",
      "Epoch 609/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8480 - mean_squared_error: 0.8480 - val_loss: 0.9997 - val_mean_squared_error: 0.9997\n",
      "Epoch 610/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8476 - mean_squared_error: 0.8476 - val_loss: 0.9954 - val_mean_squared_error: 0.9954\n",
      "Epoch 611/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8475 - mean_squared_error: 0.8475 - val_loss: 0.9827 - val_mean_squared_error: 0.9827\n",
      "Epoch 612/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8477 - mean_squared_error: 0.8477 - val_loss: 0.9831 - val_mean_squared_error: 0.9831\n",
      "Epoch 613/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8484 - mean_squared_error: 0.8484 - val_loss: 0.9950 - val_mean_squared_error: 0.9950\n",
      "Epoch 614/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8475 - mean_squared_error: 0.8475 - val_loss: 0.9931 - val_mean_squared_error: 0.9931\n",
      "Epoch 615/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8474 - mean_squared_error: 0.8474 - val_loss: 0.9863 - val_mean_squared_error: 0.9863\n",
      "Epoch 616/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8473 - mean_squared_error: 0.8473 - val_loss: 0.9876 - val_mean_squared_error: 0.9876\n",
      "Epoch 617/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8482 - mean_squared_error: 0.8482 - val_loss: 0.9945 - val_mean_squared_error: 0.9945\n",
      "Epoch 618/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8473 - mean_squared_error: 0.8473 - val_loss: 0.9869 - val_mean_squared_error: 0.9869\n",
      "Epoch 619/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8478 - mean_squared_error: 0.8478 - val_loss: 0.9920 - val_mean_squared_error: 0.9920\n",
      "Epoch 620/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8488 - mean_squared_error: 0.8488 - val_loss: 0.9965 - val_mean_squared_error: 0.9965\n",
      "Epoch 621/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8486 - mean_squared_error: 0.8486 - val_loss: 0.9805 - val_mean_squared_error: 0.9805\n",
      "Epoch 622/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8475 - mean_squared_error: 0.8475 - val_loss: 0.9888 - val_mean_squared_error: 0.9888\n",
      "Epoch 623/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8473 - mean_squared_error: 0.8473 - val_loss: 0.9881 - val_mean_squared_error: 0.9881\n",
      "Epoch 624/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8471 - mean_squared_error: 0.8471 - val_loss: 0.9879 - val_mean_squared_error: 0.9879\n",
      "Epoch 625/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8471 - mean_squared_error: 0.8471 - val_loss: 0.9872 - val_mean_squared_error: 0.9872\n",
      "Epoch 626/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8473 - mean_squared_error: 0.8473 - val_loss: 0.9887 - val_mean_squared_error: 0.9887\n",
      "Epoch 627/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8472 - mean_squared_error: 0.8472 - val_loss: 0.9858 - val_mean_squared_error: 0.9858\n",
      "Epoch 628/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8473 - mean_squared_error: 0.8473 - val_loss: 0.9905 - val_mean_squared_error: 0.9905\n",
      "Epoch 629/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8472 - mean_squared_error: 0.8472 - val_loss: 0.9896 - val_mean_squared_error: 0.9896\n",
      "Epoch 630/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8477 - mean_squared_error: 0.8477 - val_loss: 0.9946 - val_mean_squared_error: 0.9946\n",
      "Epoch 631/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8480 - mean_squared_error: 0.8480 - val_loss: 0.9797 - val_mean_squared_error: 0.9797\n",
      "Epoch 632/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8480 - mean_squared_error: 0.8480 - val_loss: 0.9932 - val_mean_squared_error: 0.9932\n",
      "Epoch 633/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8474 - mean_squared_error: 0.8474 - val_loss: 0.9943 - val_mean_squared_error: 0.9943\n",
      "Epoch 634/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8472 - mean_squared_error: 0.8472 - val_loss: 0.9828 - val_mean_squared_error: 0.9828\n",
      "Epoch 635/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8477 - mean_squared_error: 0.8477 - val_loss: 0.9879 - val_mean_squared_error: 0.9879\n",
      "Epoch 636/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8478 - mean_squared_error: 0.8478 - val_loss: 0.9795 - val_mean_squared_error: 0.9795\n",
      "Epoch 637/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8469 - mean_squared_error: 0.8469 - val_loss: 0.9858 - val_mean_squared_error: 0.9858\n",
      "Epoch 638/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8466 - mean_squared_error: 0.8466 - val_loss: 0.9913 - val_mean_squared_error: 0.9913\n",
      "Epoch 639/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8479 - mean_squared_error: 0.8479 - val_loss: 0.9968 - val_mean_squared_error: 0.9968\n",
      "Epoch 640/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8481 - mean_squared_error: 0.8481 - val_loss: 0.9961 - val_mean_squared_error: 0.9961\n",
      "Epoch 641/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8466 - mean_squared_error: 0.8466 - val_loss: 0.9830 - val_mean_squared_error: 0.9830\n",
      "Epoch 642/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8474 - mean_squared_error: 0.8474 - val_loss: 0.9798 - val_mean_squared_error: 0.9798\n",
      "Epoch 643/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8470 - mean_squared_error: 0.8470 - val_loss: 0.9855 - val_mean_squared_error: 0.9855\n",
      "Epoch 644/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8476 - mean_squared_error: 0.8476 - val_loss: 0.9855 - val_mean_squared_error: 0.9855\n",
      "Epoch 645/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8479 - mean_squared_error: 0.8479 - val_loss: 0.9971 - val_mean_squared_error: 0.9971\n",
      "Epoch 646/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8473 - mean_squared_error: 0.8473 - val_loss: 0.9823 - val_mean_squared_error: 0.9823\n",
      "Epoch 647/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8470 - mean_squared_error: 0.8470 - val_loss: 0.9817 - val_mean_squared_error: 0.9817\n",
      "Epoch 648/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8480 - mean_squared_error: 0.8480 - val_loss: 0.9950 - val_mean_squared_error: 0.9950\n",
      "Epoch 649/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8467 - mean_squared_error: 0.8467 - val_loss: 0.9882 - val_mean_squared_error: 0.9882\n",
      "Epoch 650/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8467 - mean_squared_error: 0.8467 - val_loss: 0.9844 - val_mean_squared_error: 0.9844\n",
      "Epoch 651/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8466 - mean_squared_error: 0.8466 - val_loss: 0.9840 - val_mean_squared_error: 0.9840\n",
      "Epoch 652/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8465 - mean_squared_error: 0.8465 - val_loss: 0.9875 - val_mean_squared_error: 0.9875\n",
      "Epoch 653/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8469 - mean_squared_error: 0.8469 - val_loss: 0.9839 - val_mean_squared_error: 0.9839\n",
      "Epoch 654/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8476 - mean_squared_error: 0.8476 - val_loss: 0.9940 - val_mean_squared_error: 0.9940\n",
      "Epoch 655/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8469 - mean_squared_error: 0.8469 - val_loss: 0.9888 - val_mean_squared_error: 0.9888\n",
      "Epoch 656/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8473 - mean_squared_error: 0.8473 - val_loss: 0.9812 - val_mean_squared_error: 0.9812\n",
      "Epoch 657/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8464 - mean_squared_error: 0.8464 - val_loss: 0.9879 - val_mean_squared_error: 0.9879\n",
      "Epoch 658/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8469 - mean_squared_error: 0.8469 - val_loss: 0.9857 - val_mean_squared_error: 0.9857\n",
      "Epoch 659/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8466 - mean_squared_error: 0.8466 - val_loss: 0.9892 - val_mean_squared_error: 0.9892\n",
      "Epoch 660/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8466 - mean_squared_error: 0.8466 - val_loss: 0.9895 - val_mean_squared_error: 0.9895\n",
      "Epoch 661/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8467 - mean_squared_error: 0.8467 - val_loss: 0.9821 - val_mean_squared_error: 0.9821\n",
      "Epoch 662/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8465 - mean_squared_error: 0.8465 - val_loss: 0.9856 - val_mean_squared_error: 0.9856\n",
      "Epoch 663/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8469 - mean_squared_error: 0.8469 - val_loss: 0.9888 - val_mean_squared_error: 0.9888\n",
      "Epoch 664/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8471 - mean_squared_error: 0.8471 - val_loss: 0.9881 - val_mean_squared_error: 0.9881\n",
      "Epoch 665/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8465 - mean_squared_error: 0.8465 - val_loss: 0.9858 - val_mean_squared_error: 0.9858\n",
      "Epoch 666/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8470 - mean_squared_error: 0.8470 - val_loss: 0.9779 - val_mean_squared_error: 0.9779\n",
      "Epoch 667/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8470 - mean_squared_error: 0.8470 - val_loss: 0.9814 - val_mean_squared_error: 0.9814\n",
      "Epoch 668/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8462 - mean_squared_error: 0.8462 - val_loss: 0.9873 - val_mean_squared_error: 0.9873\n",
      "Epoch 669/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8465 - mean_squared_error: 0.8465 - val_loss: 0.9903 - val_mean_squared_error: 0.9903\n",
      "Epoch 670/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8465 - mean_squared_error: 0.8465 - val_loss: 0.9844 - val_mean_squared_error: 0.9844\n",
      "Epoch 671/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8468 - mean_squared_error: 0.8468 - val_loss: 0.9892 - val_mean_squared_error: 0.9892\n",
      "Epoch 672/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8465 - mean_squared_error: 0.8465 - val_loss: 0.9802 - val_mean_squared_error: 0.9802\n",
      "Epoch 673/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8467 - mean_squared_error: 0.8467 - val_loss: 0.9865 - val_mean_squared_error: 0.9865\n",
      "Epoch 674/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8468 - mean_squared_error: 0.8468 - val_loss: 0.9816 - val_mean_squared_error: 0.9816\n",
      "Epoch 675/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8472 - mean_squared_error: 0.8472 - val_loss: 0.9921 - val_mean_squared_error: 0.9921\n",
      "Epoch 676/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8470 - mean_squared_error: 0.8470 - val_loss: 0.9836 - val_mean_squared_error: 0.9836\n",
      "Epoch 677/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8465 - mean_squared_error: 0.8465 - val_loss: 0.9881 - val_mean_squared_error: 0.9881\n",
      "Epoch 678/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8465 - mean_squared_error: 0.8465 - val_loss: 0.9832 - val_mean_squared_error: 0.9832\n",
      "Epoch 679/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8461 - mean_squared_error: 0.8461 - val_loss: 0.9861 - val_mean_squared_error: 0.9861\n",
      "Epoch 680/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8463 - mean_squared_error: 0.8463 - val_loss: 0.9867 - val_mean_squared_error: 0.9867\n",
      "Epoch 681/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8464 - mean_squared_error: 0.8464 - val_loss: 0.9894 - val_mean_squared_error: 0.9894\n",
      "Epoch 682/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8462 - mean_squared_error: 0.8462 - val_loss: 0.9836 - val_mean_squared_error: 0.9836\n",
      "Epoch 683/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8474 - mean_squared_error: 0.8474 - val_loss: 0.9762 - val_mean_squared_error: 0.9762\n",
      "Epoch 684/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8462 - mean_squared_error: 0.8462 - val_loss: 0.9845 - val_mean_squared_error: 0.9845\n",
      "Epoch 685/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8461 - mean_squared_error: 0.8461 - val_loss: 0.9949 - val_mean_squared_error: 0.9949\n",
      "Epoch 686/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8475 - mean_squared_error: 0.8475 - val_loss: 0.9933 - val_mean_squared_error: 0.9933\n",
      "Epoch 687/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8475 - mean_squared_error: 0.8475 - val_loss: 0.9793 - val_mean_squared_error: 0.9793\n",
      "Epoch 688/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8460 - mean_squared_error: 0.8460 - val_loss: 0.9854 - val_mean_squared_error: 0.9854\n",
      "Epoch 689/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8460 - mean_squared_error: 0.8460 - val_loss: 0.9882 - val_mean_squared_error: 0.9882\n",
      "Epoch 690/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8462 - mean_squared_error: 0.8462 - val_loss: 0.9863 - val_mean_squared_error: 0.9863\n",
      "Epoch 691/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8459 - mean_squared_error: 0.8459 - val_loss: 0.9881 - val_mean_squared_error: 0.9881\n",
      "Epoch 692/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8464 - mean_squared_error: 0.8464 - val_loss: 0.9884 - val_mean_squared_error: 0.9884\n",
      "Epoch 693/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8468 - mean_squared_error: 0.8468 - val_loss: 0.9797 - val_mean_squared_error: 0.9797\n",
      "Epoch 694/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8459 - mean_squared_error: 0.8459 - val_loss: 0.9854 - val_mean_squared_error: 0.9854\n",
      "Epoch 695/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8465 - mean_squared_error: 0.8465 - val_loss: 0.9898 - val_mean_squared_error: 0.9898\n",
      "Epoch 696/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8464 - mean_squared_error: 0.8464 - val_loss: 0.9909 - val_mean_squared_error: 0.9909\n",
      "Epoch 697/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8464 - mean_squared_error: 0.8464 - val_loss: 0.9759 - val_mean_squared_error: 0.9759\n",
      "Epoch 698/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8459 - mean_squared_error: 0.8459 - val_loss: 0.9831 - val_mean_squared_error: 0.9831\n",
      "Epoch 699/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8458 - mean_squared_error: 0.8458 - val_loss: 0.9865 - val_mean_squared_error: 0.9865\n",
      "Epoch 700/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8479 - mean_squared_error: 0.8479 - val_loss: 0.9786 - val_mean_squared_error: 0.9786\n",
      "Epoch 701/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8458 - mean_squared_error: 0.8458 - val_loss: 0.9887 - val_mean_squared_error: 0.9887\n",
      "Epoch 702/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8461 - mean_squared_error: 0.8461 - val_loss: 0.9900 - val_mean_squared_error: 0.9900\n",
      "Epoch 703/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8457 - mean_squared_error: 0.8457 - val_loss: 0.9857 - val_mean_squared_error: 0.9857\n",
      "Epoch 704/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8458 - mean_squared_error: 0.8458 - val_loss: 0.9827 - val_mean_squared_error: 0.9827\n",
      "Epoch 705/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8459 - mean_squared_error: 0.8459 - val_loss: 0.9806 - val_mean_squared_error: 0.9806\n",
      "Epoch 706/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8460 - mean_squared_error: 0.8460 - val_loss: 0.9877 - val_mean_squared_error: 0.9877\n",
      "Epoch 707/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8464 - mean_squared_error: 0.8464 - val_loss: 0.9889 - val_mean_squared_error: 0.9889\n",
      "Epoch 708/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8458 - mean_squared_error: 0.8458 - val_loss: 0.9850 - val_mean_squared_error: 0.9850\n",
      "Epoch 709/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8461 - mean_squared_error: 0.8461 - val_loss: 0.9786 - val_mean_squared_error: 0.9786\n",
      "Epoch 710/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8457 - mean_squared_error: 0.8457 - val_loss: 0.9825 - val_mean_squared_error: 0.9825\n",
      "Epoch 711/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8464 - mean_squared_error: 0.8464 - val_loss: 0.9820 - val_mean_squared_error: 0.9820\n",
      "Epoch 712/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8464 - mean_squared_error: 0.8464 - val_loss: 0.9817 - val_mean_squared_error: 0.9817\n",
      "Epoch 713/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8456 - mean_squared_error: 0.8456 - val_loss: 0.9909 - val_mean_squared_error: 0.9909\n",
      "Epoch 714/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8458 - mean_squared_error: 0.8458 - val_loss: 0.9860 - val_mean_squared_error: 0.9860\n",
      "Epoch 715/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8458 - mean_squared_error: 0.8458 - val_loss: 0.9833 - val_mean_squared_error: 0.9833\n",
      "Epoch 716/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8457 - mean_squared_error: 0.8457 - val_loss: 0.9821 - val_mean_squared_error: 0.9821\n",
      "Epoch 717/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8459 - mean_squared_error: 0.8459 - val_loss: 0.9846 - val_mean_squared_error: 0.9846\n",
      "Epoch 718/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8454 - mean_squared_error: 0.8454 - val_loss: 0.9834 - val_mean_squared_error: 0.9834\n",
      "Epoch 719/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8455 - mean_squared_error: 0.8455 - val_loss: 0.9815 - val_mean_squared_error: 0.9815\n",
      "Epoch 720/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8454 - mean_squared_error: 0.8454 - val_loss: 0.9822 - val_mean_squared_error: 0.9822\n",
      "Epoch 721/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8455 - mean_squared_error: 0.8455 - val_loss: 0.9840 - val_mean_squared_error: 0.9840\n",
      "Epoch 722/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8458 - mean_squared_error: 0.8458 - val_loss: 0.9839 - val_mean_squared_error: 0.9839\n",
      "Epoch 723/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8455 - mean_squared_error: 0.8455 - val_loss: 0.9826 - val_mean_squared_error: 0.9826\n",
      "Epoch 724/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8459 - mean_squared_error: 0.8459 - val_loss: 0.9789 - val_mean_squared_error: 0.9789\n",
      "Epoch 725/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8455 - mean_squared_error: 0.8455 - val_loss: 0.9824 - val_mean_squared_error: 0.9824\n",
      "Epoch 726/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8451 - mean_squared_error: 0.8451 - val_loss: 0.9871 - val_mean_squared_error: 0.9871\n",
      "Epoch 727/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8456 - mean_squared_error: 0.8456 - val_loss: 0.9892 - val_mean_squared_error: 0.9892\n",
      "Epoch 728/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8461 - mean_squared_error: 0.8461 - val_loss: 0.9801 - val_mean_squared_error: 0.9801\n",
      "Epoch 729/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8464 - mean_squared_error: 0.8464 - val_loss: 0.9786 - val_mean_squared_error: 0.9786\n",
      "Epoch 730/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8451 - mean_squared_error: 0.8451 - val_loss: 0.9883 - val_mean_squared_error: 0.9883\n",
      "Epoch 731/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8457 - mean_squared_error: 0.8457 - val_loss: 0.9843 - val_mean_squared_error: 0.9843\n",
      "Epoch 732/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8453 - mean_squared_error: 0.8453 - val_loss: 0.9883 - val_mean_squared_error: 0.9883\n",
      "Epoch 733/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8456 - mean_squared_error: 0.8456 - val_loss: 0.9838 - val_mean_squared_error: 0.9838\n",
      "Epoch 734/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8455 - mean_squared_error: 0.8455 - val_loss: 0.9845 - val_mean_squared_error: 0.9845\n",
      "Epoch 735/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8454 - mean_squared_error: 0.8454 - val_loss: 0.9803 - val_mean_squared_error: 0.9803\n",
      "Epoch 736/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8473 - mean_squared_error: 0.8473 - val_loss: 0.9845 - val_mean_squared_error: 0.9845\n",
      "Epoch 737/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8456 - mean_squared_error: 0.8456 - val_loss: 0.9725 - val_mean_squared_error: 0.9725\n",
      "Epoch 738/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8456 - mean_squared_error: 0.8456 - val_loss: 0.9792 - val_mean_squared_error: 0.9792\n",
      "Epoch 739/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8466 - mean_squared_error: 0.8466 - val_loss: 0.9915 - val_mean_squared_error: 0.9915\n",
      "Epoch 740/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8449 - mean_squared_error: 0.8449 - val_loss: 0.9820 - val_mean_squared_error: 0.9820\n",
      "Epoch 741/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8455 - mean_squared_error: 0.8455 - val_loss: 0.9722 - val_mean_squared_error: 0.9722\n",
      "Epoch 742/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8453 - mean_squared_error: 0.8453 - val_loss: 0.9823 - val_mean_squared_error: 0.9823\n",
      "Epoch 743/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8453 - mean_squared_error: 0.8453 - val_loss: 0.9830 - val_mean_squared_error: 0.9830\n",
      "Epoch 744/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8456 - mean_squared_error: 0.8456 - val_loss: 0.9872 - val_mean_squared_error: 0.9872\n",
      "Epoch 745/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8454 - mean_squared_error: 0.8454 - val_loss: 0.9831 - val_mean_squared_error: 0.9831\n",
      "Epoch 746/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8450 - mean_squared_error: 0.8450 - val_loss: 0.9784 - val_mean_squared_error: 0.9784\n",
      "Epoch 747/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8455 - mean_squared_error: 0.8455 - val_loss: 0.9828 - val_mean_squared_error: 0.9828\n",
      "Epoch 748/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8453 - mean_squared_error: 0.8453 - val_loss: 0.9779 - val_mean_squared_error: 0.9779\n",
      "Epoch 749/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8456 - mean_squared_error: 0.8456 - val_loss: 0.9741 - val_mean_squared_error: 0.9741\n",
      "Epoch 750/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8453 - mean_squared_error: 0.8453 - val_loss: 0.9866 - val_mean_squared_error: 0.9866\n",
      "Epoch 751/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8453 - mean_squared_error: 0.8453 - val_loss: 0.9814 - val_mean_squared_error: 0.9814\n",
      "Epoch 752/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8448 - mean_squared_error: 0.8448 - val_loss: 0.9836 - val_mean_squared_error: 0.9836\n",
      "Epoch 753/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8456 - mean_squared_error: 0.8456 - val_loss: 0.9836 - val_mean_squared_error: 0.9836\n",
      "Epoch 754/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8464 - mean_squared_error: 0.8464 - val_loss: 0.9726 - val_mean_squared_error: 0.9726\n",
      "Epoch 755/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8454 - mean_squared_error: 0.8454 - val_loss: 0.9845 - val_mean_squared_error: 0.9845\n",
      "Epoch 756/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8451 - mean_squared_error: 0.8451 - val_loss: 0.9856 - val_mean_squared_error: 0.9856\n",
      "Epoch 757/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8464 - mean_squared_error: 0.8464 - val_loss: 0.9754 - val_mean_squared_error: 0.9754\n",
      "Epoch 758/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8447 - mean_squared_error: 0.8447 - val_loss: 0.9879 - val_mean_squared_error: 0.9879\n",
      "Epoch 759/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8452 - mean_squared_error: 0.8452 - val_loss: 0.9844 - val_mean_squared_error: 0.9844\n",
      "Epoch 760/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8451 - mean_squared_error: 0.8451 - val_loss: 0.9786 - val_mean_squared_error: 0.9786\n",
      "Epoch 761/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8450 - mean_squared_error: 0.8450 - val_loss: 0.9779 - val_mean_squared_error: 0.9779\n",
      "Epoch 762/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8453 - mean_squared_error: 0.8453 - val_loss: 0.9866 - val_mean_squared_error: 0.9866\n",
      "Epoch 763/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8457 - mean_squared_error: 0.8457 - val_loss: 0.9794 - val_mean_squared_error: 0.9794\n",
      "Epoch 764/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8453 - mean_squared_error: 0.8453 - val_loss: 0.9808 - val_mean_squared_error: 0.9808\n",
      "Epoch 765/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8449 - mean_squared_error: 0.8449 - val_loss: 0.9792 - val_mean_squared_error: 0.9792\n",
      "Epoch 766/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8446 - mean_squared_error: 0.8446 - val_loss: 0.9806 - val_mean_squared_error: 0.9806\n",
      "Epoch 767/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8448 - mean_squared_error: 0.8448 - val_loss: 0.9853 - val_mean_squared_error: 0.9853\n",
      "Epoch 768/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8448 - mean_squared_error: 0.8448 - val_loss: 0.9813 - val_mean_squared_error: 0.9813\n",
      "Epoch 769/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8448 - mean_squared_error: 0.8448 - val_loss: 0.9822 - val_mean_squared_error: 0.9822\n",
      "Epoch 770/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8446 - mean_squared_error: 0.8446 - val_loss: 0.9762 - val_mean_squared_error: 0.9762\n",
      "Epoch 771/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8448 - mean_squared_error: 0.8448 - val_loss: 0.9791 - val_mean_squared_error: 0.9791\n",
      "Epoch 772/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8448 - mean_squared_error: 0.8448 - val_loss: 0.9753 - val_mean_squared_error: 0.9753\n",
      "Epoch 773/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8446 - mean_squared_error: 0.8446 - val_loss: 0.9830 - val_mean_squared_error: 0.9830\n",
      "Epoch 774/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8454 - mean_squared_error: 0.8454 - val_loss: 0.9839 - val_mean_squared_error: 0.9839\n",
      "Epoch 775/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8447 - mean_squared_error: 0.8447 - val_loss: 0.9792 - val_mean_squared_error: 0.9792\n",
      "Epoch 776/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8445 - mean_squared_error: 0.8445 - val_loss: 0.9756 - val_mean_squared_error: 0.9756\n",
      "Epoch 777/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8446 - mean_squared_error: 0.8446 - val_loss: 0.9755 - val_mean_squared_error: 0.9755\n",
      "Epoch 778/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8459 - mean_squared_error: 0.8459 - val_loss: 0.9865 - val_mean_squared_error: 0.9865\n",
      "Epoch 779/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8450 - mean_squared_error: 0.8450 - val_loss: 0.9746 - val_mean_squared_error: 0.9746\n",
      "Epoch 780/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8446 - mean_squared_error: 0.8446 - val_loss: 0.9755 - val_mean_squared_error: 0.9755\n",
      "Epoch 781/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8449 - mean_squared_error: 0.8449 - val_loss: 0.9796 - val_mean_squared_error: 0.9796\n",
      "Epoch 782/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8451 - mean_squared_error: 0.8451 - val_loss: 0.9796 - val_mean_squared_error: 0.9796\n",
      "Epoch 783/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8447 - mean_squared_error: 0.8447 - val_loss: 0.9797 - val_mean_squared_error: 0.9797\n",
      "Epoch 784/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8445 - mean_squared_error: 0.8445 - val_loss: 0.9829 - val_mean_squared_error: 0.9829\n",
      "Epoch 785/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8448 - mean_squared_error: 0.8448 - val_loss: 0.9760 - val_mean_squared_error: 0.9760\n",
      "Epoch 786/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8444 - mean_squared_error: 0.8444 - val_loss: 0.9804 - val_mean_squared_error: 0.9804\n",
      "Epoch 787/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8445 - mean_squared_error: 0.8445 - val_loss: 0.9763 - val_mean_squared_error: 0.9763\n",
      "Epoch 788/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8453 - mean_squared_error: 0.8453 - val_loss: 0.9838 - val_mean_squared_error: 0.9838\n",
      "Epoch 789/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8444 - mean_squared_error: 0.8444 - val_loss: 0.9767 - val_mean_squared_error: 0.9767\n",
      "Epoch 790/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8444 - mean_squared_error: 0.8444 - val_loss: 0.9786 - val_mean_squared_error: 0.9786\n",
      "Epoch 791/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8443 - mean_squared_error: 0.8443 - val_loss: 0.9784 - val_mean_squared_error: 0.9784\n",
      "Epoch 792/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8445 - mean_squared_error: 0.8445 - val_loss: 0.9844 - val_mean_squared_error: 0.9844\n",
      "Epoch 793/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8445 - mean_squared_error: 0.8445 - val_loss: 0.9795 - val_mean_squared_error: 0.9795\n",
      "Epoch 794/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8447 - mean_squared_error: 0.8447 - val_loss: 0.9815 - val_mean_squared_error: 0.9815\n",
      "Epoch 795/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8445 - mean_squared_error: 0.8445 - val_loss: 0.9765 - val_mean_squared_error: 0.9765\n",
      "Epoch 796/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8452 - mean_squared_error: 0.8452 - val_loss: 0.9740 - val_mean_squared_error: 0.9740\n",
      "Epoch 797/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8448 - mean_squared_error: 0.8448 - val_loss: 0.9884 - val_mean_squared_error: 0.9884\n",
      "Epoch 798/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8451 - mean_squared_error: 0.8451 - val_loss: 0.9861 - val_mean_squared_error: 0.9861\n",
      "Epoch 799/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8453 - mean_squared_error: 0.8453 - val_loss: 0.9693 - val_mean_squared_error: 0.9693\n",
      "Epoch 800/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8442 - mean_squared_error: 0.8442 - val_loss: 0.9777 - val_mean_squared_error: 0.9777\n",
      "Epoch 801/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8443 - mean_squared_error: 0.8443 - val_loss: 0.9821 - val_mean_squared_error: 0.9821\n",
      "Epoch 802/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8441 - mean_squared_error: 0.8441 - val_loss: 0.9835 - val_mean_squared_error: 0.9835\n",
      "Epoch 803/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8443 - mean_squared_error: 0.8443 - val_loss: 0.9796 - val_mean_squared_error: 0.9796\n",
      "Epoch 804/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8446 - mean_squared_error: 0.8446 - val_loss: 0.9714 - val_mean_squared_error: 0.9714\n",
      "Epoch 805/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8444 - mean_squared_error: 0.8444 - val_loss: 0.9779 - val_mean_squared_error: 0.9779\n",
      "Epoch 806/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8444 - mean_squared_error: 0.8444 - val_loss: 0.9801 - val_mean_squared_error: 0.9801\n",
      "Epoch 807/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8447 - mean_squared_error: 0.8447 - val_loss: 0.9783 - val_mean_squared_error: 0.9783\n",
      "Epoch 808/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8443 - mean_squared_error: 0.8443 - val_loss: 0.9794 - val_mean_squared_error: 0.9794\n",
      "Epoch 809/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8448 - mean_squared_error: 0.8448 - val_loss: 0.9839 - val_mean_squared_error: 0.9839\n",
      "Epoch 810/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8444 - mean_squared_error: 0.8444 - val_loss: 0.9819 - val_mean_squared_error: 0.9819\n",
      "Epoch 811/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.8442 - mean_squared_error: 0.8442 - val_loss: 0.9685 - val_mean_squared_error: 0.9685\n",
      "Epoch 812/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8459 - mean_squared_error: 0.8459 - val_loss: 0.9796 - val_mean_squared_error: 0.9796\n",
      "Epoch 813/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8440 - mean_squared_error: 0.8440 - val_loss: 0.9782 - val_mean_squared_error: 0.9782\n",
      "Epoch 814/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8443 - mean_squared_error: 0.8443 - val_loss: 0.9702 - val_mean_squared_error: 0.9702\n",
      "Epoch 815/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8445 - mean_squared_error: 0.8445 - val_loss: 0.9745 - val_mean_squared_error: 0.9745\n",
      "Epoch 816/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8439 - mean_squared_error: 0.8439 - val_loss: 0.9796 - val_mean_squared_error: 0.9796\n",
      "Epoch 817/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8447 - mean_squared_error: 0.8447 - val_loss: 0.9825 - val_mean_squared_error: 0.9825\n",
      "Epoch 818/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8437 - mean_squared_error: 0.8437 - val_loss: 0.9746 - val_mean_squared_error: 0.9746\n",
      "Epoch 819/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8438 - mean_squared_error: 0.8438 - val_loss: 0.9750 - val_mean_squared_error: 0.9750\n",
      "Epoch 820/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8443 - mean_squared_error: 0.8443 - val_loss: 0.9729 - val_mean_squared_error: 0.9729\n",
      "Epoch 821/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8439 - mean_squared_error: 0.8439 - val_loss: 0.9810 - val_mean_squared_error: 0.9810\n",
      "Epoch 822/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8441 - mean_squared_error: 0.8441 - val_loss: 0.9845 - val_mean_squared_error: 0.9845\n",
      "Epoch 823/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8445 - mean_squared_error: 0.8445 - val_loss: 0.9764 - val_mean_squared_error: 0.9764\n",
      "Epoch 824/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8439 - mean_squared_error: 0.8439 - val_loss: 0.9750 - val_mean_squared_error: 0.9750\n",
      "Epoch 825/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8440 - mean_squared_error: 0.8440 - val_loss: 0.9771 - val_mean_squared_error: 0.9771\n",
      "Epoch 826/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8441 - mean_squared_error: 0.8441 - val_loss: 0.9734 - val_mean_squared_error: 0.9734\n",
      "Epoch 827/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8439 - mean_squared_error: 0.8439 - val_loss: 0.9783 - val_mean_squared_error: 0.9783\n",
      "Epoch 828/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8465 - mean_squared_error: 0.8465 - val_loss: 0.9864 - val_mean_squared_error: 0.9864\n",
      "Epoch 829/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8446 - mean_squared_error: 0.8446 - val_loss: 0.9723 - val_mean_squared_error: 0.9723\n",
      "Epoch 830/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8449 - mean_squared_error: 0.8449 - val_loss: 0.9810 - val_mean_squared_error: 0.9810\n",
      "Epoch 831/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8441 - mean_squared_error: 0.8441 - val_loss: 0.9799 - val_mean_squared_error: 0.9799\n",
      "Epoch 832/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8438 - mean_squared_error: 0.8438 - val_loss: 0.9774 - val_mean_squared_error: 0.9774\n",
      "Epoch 833/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8438 - mean_squared_error: 0.8438 - val_loss: 0.9756 - val_mean_squared_error: 0.9756\n",
      "Epoch 834/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8438 - mean_squared_error: 0.8438 - val_loss: 0.9765 - val_mean_squared_error: 0.9765\n",
      "Epoch 835/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8444 - mean_squared_error: 0.8444 - val_loss: 0.9823 - val_mean_squared_error: 0.9823\n",
      "Epoch 836/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8437 - mean_squared_error: 0.8437 - val_loss: 0.9762 - val_mean_squared_error: 0.9762\n",
      "Epoch 837/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8445 - mean_squared_error: 0.8445 - val_loss: 0.9777 - val_mean_squared_error: 0.9777\n",
      "Epoch 838/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8435 - mean_squared_error: 0.8435 - val_loss: 0.9695 - val_mean_squared_error: 0.9695\n",
      "Epoch 839/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8442 - mean_squared_error: 0.8442 - val_loss: 0.9780 - val_mean_squared_error: 0.9780\n",
      "Epoch 840/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8435 - mean_squared_error: 0.8435 - val_loss: 0.9743 - val_mean_squared_error: 0.9743\n",
      "Epoch 841/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8437 - mean_squared_error: 0.8437 - val_loss: 0.9789 - val_mean_squared_error: 0.9789\n",
      "Epoch 842/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8442 - mean_squared_error: 0.8442 - val_loss: 0.9803 - val_mean_squared_error: 0.9803\n",
      "Epoch 843/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8434 - mean_squared_error: 0.8434 - val_loss: 0.9738 - val_mean_squared_error: 0.9738\n",
      "Epoch 844/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8440 - mean_squared_error: 0.8440 - val_loss: 0.9762 - val_mean_squared_error: 0.9762\n",
      "Epoch 845/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8448 - mean_squared_error: 0.8448 - val_loss: 0.9677 - val_mean_squared_error: 0.9677\n",
      "Epoch 846/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8436 - mean_squared_error: 0.8436 - val_loss: 0.9852 - val_mean_squared_error: 0.9852\n",
      "Epoch 847/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8443 - mean_squared_error: 0.8443 - val_loss: 0.9807 - val_mean_squared_error: 0.9807\n",
      "Epoch 848/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8434 - mean_squared_error: 0.8434 - val_loss: 0.9747 - val_mean_squared_error: 0.9747\n",
      "Epoch 849/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8447 - mean_squared_error: 0.8447 - val_loss: 0.9807 - val_mean_squared_error: 0.9807\n",
      "Epoch 850/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8432 - mean_squared_error: 0.8432 - val_loss: 0.9753 - val_mean_squared_error: 0.9753\n",
      "Epoch 851/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8446 - mean_squared_error: 0.8446 - val_loss: 0.9650 - val_mean_squared_error: 0.9650\n",
      "Epoch 852/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8434 - mean_squared_error: 0.8434 - val_loss: 0.9805 - val_mean_squared_error: 0.9805\n",
      "Epoch 853/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8453 - mean_squared_error: 0.8453 - val_loss: 0.9876 - val_mean_squared_error: 0.9876\n",
      "Epoch 854/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8434 - mean_squared_error: 0.8434 - val_loss: 0.9652 - val_mean_squared_error: 0.9652\n",
      "Epoch 855/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8449 - mean_squared_error: 0.8449 - val_loss: 0.9745 - val_mean_squared_error: 0.9745\n",
      "Epoch 856/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8433 - mean_squared_error: 0.8433 - val_loss: 0.9704 - val_mean_squared_error: 0.9704\n",
      "Epoch 857/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8454 - mean_squared_error: 0.8454 - val_loss: 0.9670 - val_mean_squared_error: 0.9670\n",
      "Epoch 858/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8447 - mean_squared_error: 0.8447 - val_loss: 0.9865 - val_mean_squared_error: 0.9865\n",
      "Epoch 859/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8440 - mean_squared_error: 0.8440 - val_loss: 0.9752 - val_mean_squared_error: 0.9752\n",
      "Epoch 860/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8436 - mean_squared_error: 0.8436 - val_loss: 0.9809 - val_mean_squared_error: 0.9809\n",
      "Epoch 861/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8433 - mean_squared_error: 0.8433 - val_loss: 0.9748 - val_mean_squared_error: 0.9748\n",
      "Epoch 862/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8434 - mean_squared_error: 0.8434 - val_loss: 0.9680 - val_mean_squared_error: 0.9680\n",
      "Epoch 863/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8431 - mean_squared_error: 0.8431 - val_loss: 0.9742 - val_mean_squared_error: 0.9742\n",
      "Epoch 864/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8438 - mean_squared_error: 0.8438 - val_loss: 0.9834 - val_mean_squared_error: 0.9834\n",
      "Epoch 865/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8434 - mean_squared_error: 0.8434 - val_loss: 0.9715 - val_mean_squared_error: 0.9715\n",
      "Epoch 866/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8433 - mean_squared_error: 0.8433 - val_loss: 0.9732 - val_mean_squared_error: 0.9732\n",
      "Epoch 867/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8436 - mean_squared_error: 0.8436 - val_loss: 0.9701 - val_mean_squared_error: 0.9701\n",
      "Epoch 868/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8433 - mean_squared_error: 0.8433 - val_loss: 0.9821 - val_mean_squared_error: 0.9821\n",
      "Epoch 869/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8435 - mean_squared_error: 0.8435 - val_loss: 0.9803 - val_mean_squared_error: 0.9803\n",
      "Epoch 870/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8434 - mean_squared_error: 0.8434 - val_loss: 0.9748 - val_mean_squared_error: 0.9748\n",
      "Epoch 871/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8435 - mean_squared_error: 0.8435 - val_loss: 0.9808 - val_mean_squared_error: 0.9808\n",
      "Epoch 872/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8431 - mean_squared_error: 0.8431 - val_loss: 0.9784 - val_mean_squared_error: 0.9784\n",
      "Epoch 873/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8432 - mean_squared_error: 0.8432 - val_loss: 0.9749 - val_mean_squared_error: 0.9749\n",
      "Epoch 874/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8441 - mean_squared_error: 0.8441 - val_loss: 0.9625 - val_mean_squared_error: 0.9625\n",
      "Epoch 875/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8434 - mean_squared_error: 0.8434 - val_loss: 0.9727 - val_mean_squared_error: 0.9727\n",
      "Epoch 876/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8435 - mean_squared_error: 0.8435 - val_loss: 0.9756 - val_mean_squared_error: 0.9756\n",
      "Epoch 877/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8433 - mean_squared_error: 0.8433 - val_loss: 0.9752 - val_mean_squared_error: 0.9752\n",
      "Epoch 878/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8431 - mean_squared_error: 0.8431 - val_loss: 0.9725 - val_mean_squared_error: 0.9725\n",
      "Epoch 879/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8431 - mean_squared_error: 0.8431 - val_loss: 0.9749 - val_mean_squared_error: 0.9749\n",
      "Epoch 880/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8428 - mean_squared_error: 0.8428 - val_loss: 0.9772 - val_mean_squared_error: 0.9772\n",
      "Epoch 881/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8429 - mean_squared_error: 0.8429 - val_loss: 0.9766 - val_mean_squared_error: 0.9766\n",
      "Epoch 882/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8432 - mean_squared_error: 0.8432 - val_loss: 0.9736 - val_mean_squared_error: 0.9736\n",
      "Epoch 883/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8428 - mean_squared_error: 0.8428 - val_loss: 0.9769 - val_mean_squared_error: 0.9769\n",
      "Epoch 884/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8437 - mean_squared_error: 0.8437 - val_loss: 0.9739 - val_mean_squared_error: 0.9739\n",
      "Epoch 885/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8427 - mean_squared_error: 0.8427 - val_loss: 0.9777 - val_mean_squared_error: 0.9777\n",
      "Epoch 886/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8431 - mean_squared_error: 0.8431 - val_loss: 0.9798 - val_mean_squared_error: 0.9798\n",
      "Epoch 887/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8425 - mean_squared_error: 0.8425 - val_loss: 0.9743 - val_mean_squared_error: 0.9743\n",
      "Epoch 888/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8429 - mean_squared_error: 0.8429 - val_loss: 0.9653 - val_mean_squared_error: 0.9653\n",
      "Epoch 889/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8432 - mean_squared_error: 0.8432 - val_loss: 0.9747 - val_mean_squared_error: 0.9747\n",
      "Epoch 890/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8427 - mean_squared_error: 0.8427 - val_loss: 0.9732 - val_mean_squared_error: 0.9732\n",
      "Epoch 891/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8427 - mean_squared_error: 0.8427 - val_loss: 0.9724 - val_mean_squared_error: 0.9724\n",
      "Epoch 892/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8427 - mean_squared_error: 0.8427 - val_loss: 0.9784 - val_mean_squared_error: 0.9784\n",
      "Epoch 893/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8427 - mean_squared_error: 0.8427 - val_loss: 0.9761 - val_mean_squared_error: 0.9761\n",
      "Epoch 894/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8429 - mean_squared_error: 0.8429 - val_loss: 0.9731 - val_mean_squared_error: 0.9731\n",
      "Epoch 895/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8430 - mean_squared_error: 0.8430 - val_loss: 0.9712 - val_mean_squared_error: 0.9712\n",
      "Epoch 896/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8426 - mean_squared_error: 0.8426 - val_loss: 0.9727 - val_mean_squared_error: 0.9727\n",
      "Epoch 897/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8431 - mean_squared_error: 0.8431 - val_loss: 0.9789 - val_mean_squared_error: 0.9789\n",
      "Epoch 898/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8433 - mean_squared_error: 0.8433 - val_loss: 0.9730 - val_mean_squared_error: 0.9730\n",
      "Epoch 899/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8431 - mean_squared_error: 0.8431 - val_loss: 0.9632 - val_mean_squared_error: 0.9632\n",
      "Epoch 900/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8425 - mean_squared_error: 0.8425 - val_loss: 0.9748 - val_mean_squared_error: 0.9748\n",
      "Epoch 901/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8430 - mean_squared_error: 0.8430 - val_loss: 0.9786 - val_mean_squared_error: 0.9786\n",
      "Epoch 902/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8424 - mean_squared_error: 0.8424 - val_loss: 0.9743 - val_mean_squared_error: 0.9743\n",
      "Epoch 903/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8432 - mean_squared_error: 0.8432 - val_loss: 0.9665 - val_mean_squared_error: 0.9665\n",
      "Epoch 904/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8426 - mean_squared_error: 0.8426 - val_loss: 0.9778 - val_mean_squared_error: 0.9778\n",
      "Epoch 905/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8426 - mean_squared_error: 0.8426 - val_loss: 0.9751 - val_mean_squared_error: 0.9751\n",
      "Epoch 906/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8424 - mean_squared_error: 0.8424 - val_loss: 0.9756 - val_mean_squared_error: 0.9756\n",
      "Epoch 907/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8425 - mean_squared_error: 0.8425 - val_loss: 0.9728 - val_mean_squared_error: 0.9728\n",
      "Epoch 908/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8427 - mean_squared_error: 0.8427 - val_loss: 0.9685 - val_mean_squared_error: 0.9685\n",
      "Epoch 909/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8426 - mean_squared_error: 0.8426 - val_loss: 0.9780 - val_mean_squared_error: 0.9780\n",
      "Epoch 910/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8427 - mean_squared_error: 0.8427 - val_loss: 0.9758 - val_mean_squared_error: 0.9758\n",
      "Epoch 911/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8422 - mean_squared_error: 0.8422 - val_loss: 0.9727 - val_mean_squared_error: 0.9727\n",
      "Epoch 912/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8425 - mean_squared_error: 0.8425 - val_loss: 0.9696 - val_mean_squared_error: 0.9696\n",
      "Epoch 913/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8430 - mean_squared_error: 0.8430 - val_loss: 0.9729 - val_mean_squared_error: 0.9729\n",
      "Epoch 914/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8428 - mean_squared_error: 0.8428 - val_loss: 0.9756 - val_mean_squared_error: 0.9756\n",
      "Epoch 915/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8423 - mean_squared_error: 0.8423 - val_loss: 0.9683 - val_mean_squared_error: 0.9683\n",
      "Epoch 916/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8423 - mean_squared_error: 0.8423 - val_loss: 0.9724 - val_mean_squared_error: 0.9724\n",
      "Epoch 917/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8445 - mean_squared_error: 0.8445 - val_loss: 0.9668 - val_mean_squared_error: 0.9668\n",
      "Epoch 918/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8431 - mean_squared_error: 0.8431 - val_loss: 0.9818 - val_mean_squared_error: 0.9818\n",
      "Epoch 919/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8425 - mean_squared_error: 0.8425 - val_loss: 0.9762 - val_mean_squared_error: 0.9762\n",
      "Epoch 920/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8425 - mean_squared_error: 0.8425 - val_loss: 0.9669 - val_mean_squared_error: 0.9669\n",
      "Epoch 921/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8468 - mean_squared_error: 0.8468 - val_loss: 0.9820 - val_mean_squared_error: 0.9820\n",
      "Epoch 922/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8418 - mean_squared_error: 0.8418 - val_loss: 0.9676 - val_mean_squared_error: 0.9676\n",
      "Epoch 923/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8431 - mean_squared_error: 0.8431 - val_loss: 0.9660 - val_mean_squared_error: 0.9660\n",
      "Epoch 924/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8424 - mean_squared_error: 0.8424 - val_loss: 0.9767 - val_mean_squared_error: 0.9767\n",
      "Epoch 925/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8421 - mean_squared_error: 0.8421 - val_loss: 0.9745 - val_mean_squared_error: 0.9745\n",
      "Epoch 926/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8425 - mean_squared_error: 0.8425 - val_loss: 0.9709 - val_mean_squared_error: 0.9709\n",
      "Epoch 927/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8420 - mean_squared_error: 0.8420 - val_loss: 0.9725 - val_mean_squared_error: 0.9725\n",
      "Epoch 928/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8423 - mean_squared_error: 0.8423 - val_loss: 0.9695 - val_mean_squared_error: 0.9695\n",
      "Epoch 929/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8425 - mean_squared_error: 0.8425 - val_loss: 0.9794 - val_mean_squared_error: 0.9794\n",
      "Epoch 930/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8429 - mean_squared_error: 0.8429 - val_loss: 0.9725 - val_mean_squared_error: 0.9725\n",
      "Epoch 931/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8422 - mean_squared_error: 0.8422 - val_loss: 0.9782 - val_mean_squared_error: 0.9782\n",
      "Epoch 932/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8433 - mean_squared_error: 0.8433 - val_loss: 0.9649 - val_mean_squared_error: 0.9649\n",
      "Epoch 933/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8422 - mean_squared_error: 0.8422 - val_loss: 0.9710 - val_mean_squared_error: 0.9710\n",
      "Epoch 934/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8419 - mean_squared_error: 0.8419 - val_loss: 0.9745 - val_mean_squared_error: 0.9745\n",
      "Epoch 935/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8439 - mean_squared_error: 0.8439 - val_loss: 0.9650 - val_mean_squared_error: 0.9650\n",
      "Epoch 936/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8440 - mean_squared_error: 0.8440 - val_loss: 0.9824 - val_mean_squared_error: 0.9824\n",
      "Epoch 937/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8434 - mean_squared_error: 0.8434 - val_loss: 0.9807 - val_mean_squared_error: 0.9807\n",
      "Epoch 938/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8442 - mean_squared_error: 0.8442 - val_loss: 0.9547 - val_mean_squared_error: 0.9547\n",
      "Epoch 939/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8430 - mean_squared_error: 0.8430 - val_loss: 0.9734 - val_mean_squared_error: 0.9734\n",
      "Epoch 940/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8420 - mean_squared_error: 0.8420 - val_loss: 0.9741 - val_mean_squared_error: 0.9741\n",
      "Epoch 941/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8419 - mean_squared_error: 0.8419 - val_loss: 0.9732 - val_mean_squared_error: 0.9732\n",
      "Epoch 942/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8422 - mean_squared_error: 0.8422 - val_loss: 0.9704 - val_mean_squared_error: 0.9704\n",
      "Epoch 943/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8419 - mean_squared_error: 0.8419 - val_loss: 0.9700 - val_mean_squared_error: 0.9700\n",
      "Epoch 944/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8422 - mean_squared_error: 0.8422 - val_loss: 0.9652 - val_mean_squared_error: 0.9652\n",
      "Epoch 945/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8419 - mean_squared_error: 0.8419 - val_loss: 0.9712 - val_mean_squared_error: 0.9712\n",
      "Epoch 946/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8425 - mean_squared_error: 0.8425 - val_loss: 0.9772 - val_mean_squared_error: 0.9772\n",
      "Epoch 947/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8419 - mean_squared_error: 0.8419 - val_loss: 0.9753 - val_mean_squared_error: 0.9753\n",
      "Epoch 948/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8421 - mean_squared_error: 0.8421 - val_loss: 0.9652 - val_mean_squared_error: 0.9652\n",
      "Epoch 949/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8427 - mean_squared_error: 0.8427 - val_loss: 0.9761 - val_mean_squared_error: 0.9761\n",
      "Epoch 950/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8428 - mean_squared_error: 0.8428 - val_loss: 0.9625 - val_mean_squared_error: 0.9625\n",
      "Epoch 951/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8423 - mean_squared_error: 0.8423 - val_loss: 0.9649 - val_mean_squared_error: 0.9649\n",
      "Epoch 952/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8423 - mean_squared_error: 0.8423 - val_loss: 0.9837 - val_mean_squared_error: 0.9837\n",
      "Epoch 953/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8430 - mean_squared_error: 0.8430 - val_loss: 0.9690 - val_mean_squared_error: 0.9690\n",
      "Epoch 954/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8421 - mean_squared_error: 0.8421 - val_loss: 0.9696 - val_mean_squared_error: 0.9696\n",
      "Epoch 955/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8417 - mean_squared_error: 0.8417 - val_loss: 0.9739 - val_mean_squared_error: 0.9739\n",
      "Epoch 956/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8420 - mean_squared_error: 0.8420 - val_loss: 0.9717 - val_mean_squared_error: 0.9717\n",
      "Epoch 957/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8417 - mean_squared_error: 0.8417 - val_loss: 0.9729 - val_mean_squared_error: 0.9729\n",
      "Epoch 958/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8429 - mean_squared_error: 0.8429 - val_loss: 0.9667 - val_mean_squared_error: 0.9667\n",
      "Epoch 959/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8413 - mean_squared_error: 0.8413 - val_loss: 0.9722 - val_mean_squared_error: 0.9722\n",
      "Epoch 960/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8415 - mean_squared_error: 0.8415 - val_loss: 0.9769 - val_mean_squared_error: 0.9769\n",
      "Epoch 961/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8418 - mean_squared_error: 0.8418 - val_loss: 0.9742 - val_mean_squared_error: 0.9742\n",
      "Epoch 962/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8441 - mean_squared_error: 0.8441 - val_loss: 0.9636 - val_mean_squared_error: 0.9636\n",
      "Epoch 963/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8419 - mean_squared_error: 0.8419 - val_loss: 0.9760 - val_mean_squared_error: 0.9760\n",
      "Epoch 964/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8425 - mean_squared_error: 0.8425 - val_loss: 0.9794 - val_mean_squared_error: 0.9794\n",
      "Epoch 965/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8415 - mean_squared_error: 0.8415 - val_loss: 0.9669 - val_mean_squared_error: 0.9669\n",
      "Epoch 966/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8418 - mean_squared_error: 0.8418 - val_loss: 0.9618 - val_mean_squared_error: 0.9618\n",
      "Epoch 967/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8411 - mean_squared_error: 0.8411 - val_loss: 0.9713 - val_mean_squared_error: 0.9713\n",
      "Epoch 968/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8417 - mean_squared_error: 0.8417 - val_loss: 0.9736 - val_mean_squared_error: 0.9736\n",
      "Epoch 969/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8420 - mean_squared_error: 0.8420 - val_loss: 0.9768 - val_mean_squared_error: 0.9768\n",
      "Epoch 970/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8420 - mean_squared_error: 0.8420 - val_loss: 0.9636 - val_mean_squared_error: 0.9636\n",
      "Epoch 971/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8415 - mean_squared_error: 0.8415 - val_loss: 0.9734 - val_mean_squared_error: 0.9734\n",
      "Epoch 972/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8417 - mean_squared_error: 0.8417 - val_loss: 0.9735 - val_mean_squared_error: 0.9735\n",
      "Epoch 973/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8421 - mean_squared_error: 0.8421 - val_loss: 0.9686 - val_mean_squared_error: 0.9686\n",
      "Epoch 974/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8412 - mean_squared_error: 0.8412 - val_loss: 0.9662 - val_mean_squared_error: 0.9662\n",
      "Epoch 975/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8414 - mean_squared_error: 0.8414 - val_loss: 0.9694 - val_mean_squared_error: 0.9694\n",
      "Epoch 976/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8422 - mean_squared_error: 0.8422 - val_loss: 0.9776 - val_mean_squared_error: 0.9776\n",
      "Epoch 977/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8414 - mean_squared_error: 0.8414 - val_loss: 0.9710 - val_mean_squared_error: 0.9710\n",
      "Epoch 978/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8419 - mean_squared_error: 0.8419 - val_loss: 0.9651 - val_mean_squared_error: 0.9651\n",
      "Epoch 979/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8413 - mean_squared_error: 0.8413 - val_loss: 0.9709 - val_mean_squared_error: 0.9709\n",
      "Epoch 980/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8415 - mean_squared_error: 0.8415 - val_loss: 0.9656 - val_mean_squared_error: 0.9656\n",
      "Epoch 981/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8417 - mean_squared_error: 0.8417 - val_loss: 0.9774 - val_mean_squared_error: 0.9774\n",
      "Epoch 982/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8413 - mean_squared_error: 0.8413 - val_loss: 0.9695 - val_mean_squared_error: 0.9695\n",
      "Epoch 983/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8414 - mean_squared_error: 0.8414 - val_loss: 0.9720 - val_mean_squared_error: 0.9720\n",
      "Epoch 984/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8428 - mean_squared_error: 0.8428 - val_loss: 0.9579 - val_mean_squared_error: 0.9579\n",
      "Epoch 985/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8414 - mean_squared_error: 0.8414 - val_loss: 0.9691 - val_mean_squared_error: 0.9691\n",
      "Epoch 986/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8411 - mean_squared_error: 0.8411 - val_loss: 0.9746 - val_mean_squared_error: 0.9746\n",
      "Epoch 987/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8415 - mean_squared_error: 0.8415 - val_loss: 0.9762 - val_mean_squared_error: 0.9762\n",
      "Epoch 988/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8420 - mean_squared_error: 0.8420 - val_loss: 0.9685 - val_mean_squared_error: 0.9685\n",
      "Epoch 989/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8412 - mean_squared_error: 0.8412 - val_loss: 0.9676 - val_mean_squared_error: 0.9676\n",
      "Epoch 990/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8413 - mean_squared_error: 0.8413 - val_loss: 0.9706 - val_mean_squared_error: 0.9706\n",
      "Epoch 991/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8419 - mean_squared_error: 0.8419 - val_loss: 0.9684 - val_mean_squared_error: 0.9684\n",
      "Epoch 992/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8408 - mean_squared_error: 0.8408 - val_loss: 0.9707 - val_mean_squared_error: 0.9707\n",
      "Epoch 993/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8413 - mean_squared_error: 0.8413 - val_loss: 0.9741 - val_mean_squared_error: 0.9741\n",
      "Epoch 994/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8422 - mean_squared_error: 0.8422 - val_loss: 0.9616 - val_mean_squared_error: 0.9616\n",
      "Epoch 995/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8410 - mean_squared_error: 0.8410 - val_loss: 0.9750 - val_mean_squared_error: 0.9750\n",
      "Epoch 996/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8411 - mean_squared_error: 0.8411 - val_loss: 0.9698 - val_mean_squared_error: 0.9698\n",
      "Epoch 997/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8420 - mean_squared_error: 0.8420 - val_loss: 0.9645 - val_mean_squared_error: 0.9645\n",
      "Epoch 998/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8421 - mean_squared_error: 0.8421 - val_loss: 0.9762 - val_mean_squared_error: 0.9762\n",
      "Epoch 999/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8415 - mean_squared_error: 0.8415 - val_loss: 0.9675 - val_mean_squared_error: 0.9675\n",
      "Epoch 1000/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8409 - mean_squared_error: 0.8409 - val_loss: 0.9715 - val_mean_squared_error: 0.9715\n",
      "Epoch 1001/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8436 - mean_squared_error: 0.8436 - val_loss: 0.9809 - val_mean_squared_error: 0.9809\n",
      "Epoch 1002/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8417 - mean_squared_error: 0.8417 - val_loss: 0.9737 - val_mean_squared_error: 0.9737\n",
      "Epoch 1003/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8406 - mean_squared_error: 0.8406 - val_loss: 0.9574 - val_mean_squared_error: 0.9574\n",
      "Epoch 1004/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8419 - mean_squared_error: 0.8419 - val_loss: 0.9658 - val_mean_squared_error: 0.9658\n",
      "Epoch 1005/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8420 - mean_squared_error: 0.8420 - val_loss: 0.9621 - val_mean_squared_error: 0.9621\n",
      "Epoch 1006/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8403 - mean_squared_error: 0.8403 - val_loss: 0.9739 - val_mean_squared_error: 0.9739\n",
      "Epoch 1007/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8413 - mean_squared_error: 0.8413 - val_loss: 0.9702 - val_mean_squared_error: 0.9702\n",
      "Epoch 1008/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8413 - mean_squared_error: 0.8413 - val_loss: 0.9722 - val_mean_squared_error: 0.9722\n",
      "Epoch 1009/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8412 - mean_squared_error: 0.8412 - val_loss: 0.9667 - val_mean_squared_error: 0.9667\n",
      "Epoch 1010/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8405 - mean_squared_error: 0.8405 - val_loss: 0.9663 - val_mean_squared_error: 0.9663\n",
      "Epoch 1011/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8405 - mean_squared_error: 0.8405 - val_loss: 0.9693 - val_mean_squared_error: 0.9693\n",
      "Epoch 1012/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8410 - mean_squared_error: 0.8410 - val_loss: 0.9745 - val_mean_squared_error: 0.9745\n",
      "Epoch 1013/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8409 - mean_squared_error: 0.8409 - val_loss: 0.9696 - val_mean_squared_error: 0.9696\n",
      "Epoch 1014/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8408 - mean_squared_error: 0.8408 - val_loss: 0.9671 - val_mean_squared_error: 0.9671\n",
      "Epoch 1015/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8408 - mean_squared_error: 0.8408 - val_loss: 0.9651 - val_mean_squared_error: 0.9651\n",
      "Epoch 1016/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8409 - mean_squared_error: 0.8409 - val_loss: 0.9664 - val_mean_squared_error: 0.9664\n",
      "Epoch 1017/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8405 - mean_squared_error: 0.8405 - val_loss: 0.9681 - val_mean_squared_error: 0.9681\n",
      "Epoch 1018/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8408 - mean_squared_error: 0.8408 - val_loss: 0.9696 - val_mean_squared_error: 0.9696\n",
      "Epoch 1019/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8414 - mean_squared_error: 0.8414 - val_loss: 0.9700 - val_mean_squared_error: 0.9700\n",
      "Epoch 1020/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8409 - mean_squared_error: 0.8409 - val_loss: 0.9699 - val_mean_squared_error: 0.9699\n",
      "Epoch 1021/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8408 - mean_squared_error: 0.8408 - val_loss: 0.9653 - val_mean_squared_error: 0.9653\n",
      "Epoch 1022/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8404 - mean_squared_error: 0.8404 - val_loss: 0.9715 - val_mean_squared_error: 0.9715\n",
      "Epoch 1023/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8409 - mean_squared_error: 0.8409 - val_loss: 0.9739 - val_mean_squared_error: 0.9739\n",
      "Epoch 1024/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8405 - mean_squared_error: 0.8405 - val_loss: 0.9713 - val_mean_squared_error: 0.9713\n",
      "Epoch 1025/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8405 - mean_squared_error: 0.8405 - val_loss: 0.9670 - val_mean_squared_error: 0.9670\n",
      "Epoch 1026/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8404 - mean_squared_error: 0.8404 - val_loss: 0.9629 - val_mean_squared_error: 0.9629\n",
      "Epoch 1027/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8414 - mean_squared_error: 0.8414 - val_loss: 0.9598 - val_mean_squared_error: 0.9598\n",
      "Epoch 1028/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8411 - mean_squared_error: 0.8411 - val_loss: 0.9792 - val_mean_squared_error: 0.9792\n",
      "Epoch 1029/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8405 - mean_squared_error: 0.8405 - val_loss: 0.9675 - val_mean_squared_error: 0.9675\n",
      "Epoch 1030/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8426 - mean_squared_error: 0.8426 - val_loss: 0.9552 - val_mean_squared_error: 0.9552\n",
      "Epoch 1031/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8410 - mean_squared_error: 0.8410 - val_loss: 0.9700 - val_mean_squared_error: 0.9700\n",
      "Epoch 1032/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8406 - mean_squared_error: 0.8406 - val_loss: 0.9690 - val_mean_squared_error: 0.9690\n",
      "Epoch 1033/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8406 - mean_squared_error: 0.8406 - val_loss: 0.9705 - val_mean_squared_error: 0.9705\n",
      "Epoch 1034/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8401 - mean_squared_error: 0.8401 - val_loss: 0.9661 - val_mean_squared_error: 0.9661\n",
      "Epoch 1035/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8405 - mean_squared_error: 0.8405 - val_loss: 0.9649 - val_mean_squared_error: 0.9649\n",
      "Epoch 1036/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8405 - mean_squared_error: 0.8405 - val_loss: 0.9662 - val_mean_squared_error: 0.9662\n",
      "Epoch 1037/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8405 - mean_squared_error: 0.8405 - val_loss: 0.9688 - val_mean_squared_error: 0.9688\n",
      "Epoch 1038/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8402 - mean_squared_error: 0.8402 - val_loss: 0.9621 - val_mean_squared_error: 0.9621\n",
      "Epoch 1039/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8405 - mean_squared_error: 0.8405 - val_loss: 0.9694 - val_mean_squared_error: 0.9694\n",
      "Epoch 1040/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8404 - mean_squared_error: 0.8404 - val_loss: 0.9642 - val_mean_squared_error: 0.9642\n",
      "Epoch 1041/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8405 - mean_squared_error: 0.8405 - val_loss: 0.9693 - val_mean_squared_error: 0.9693\n",
      "Epoch 1042/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8401 - mean_squared_error: 0.8401 - val_loss: 0.9650 - val_mean_squared_error: 0.9650\n",
      "Epoch 1043/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8401 - mean_squared_error: 0.8401 - val_loss: 0.9613 - val_mean_squared_error: 0.9613\n",
      "Epoch 1044/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8400 - mean_squared_error: 0.8400 - val_loss: 0.9640 - val_mean_squared_error: 0.9640\n",
      "Epoch 1045/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8404 - mean_squared_error: 0.8404 - val_loss: 0.9705 - val_mean_squared_error: 0.9705\n",
      "Epoch 1046/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8402 - mean_squared_error: 0.8402 - val_loss: 0.9668 - val_mean_squared_error: 0.9668\n",
      "Epoch 1047/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8434 - mean_squared_error: 0.8434 - val_loss: 0.9510 - val_mean_squared_error: 0.9510\n",
      "Epoch 1048/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8397 - mean_squared_error: 0.8397 - val_loss: 0.9710 - val_mean_squared_error: 0.9710\n",
      "Epoch 1049/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8415 - mean_squared_error: 0.8415 - val_loss: 0.9747 - val_mean_squared_error: 0.9747\n",
      "Epoch 1050/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8403 - mean_squared_error: 0.8403 - val_loss: 0.9685 - val_mean_squared_error: 0.9685\n",
      "Epoch 1051/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8398 - mean_squared_error: 0.8398 - val_loss: 0.9626 - val_mean_squared_error: 0.9626\n",
      "Epoch 1052/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8405 - mean_squared_error: 0.8405 - val_loss: 0.9622 - val_mean_squared_error: 0.9622\n",
      "Epoch 1053/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8399 - mean_squared_error: 0.8399 - val_loss: 0.9644 - val_mean_squared_error: 0.9644\n",
      "Epoch 1054/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8413 - mean_squared_error: 0.8413 - val_loss: 0.9709 - val_mean_squared_error: 0.9709\n",
      "Epoch 1055/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8399 - mean_squared_error: 0.8399 - val_loss: 0.9608 - val_mean_squared_error: 0.9608\n",
      "Epoch 1056/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8403 - mean_squared_error: 0.8403 - val_loss: 0.9677 - val_mean_squared_error: 0.9677\n",
      "Epoch 1057/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8398 - mean_squared_error: 0.8398 - val_loss: 0.9643 - val_mean_squared_error: 0.9643\n",
      "Epoch 1058/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8404 - mean_squared_error: 0.8404 - val_loss: 0.9539 - val_mean_squared_error: 0.9539\n",
      "Epoch 1059/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8402 - mean_squared_error: 0.8402 - val_loss: 0.9656 - val_mean_squared_error: 0.9656\n",
      "Epoch 1060/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8400 - mean_squared_error: 0.8400 - val_loss: 0.9713 - val_mean_squared_error: 0.9713\n",
      "Epoch 1061/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8402 - mean_squared_error: 0.8402 - val_loss: 0.9699 - val_mean_squared_error: 0.9699\n",
      "Epoch 1062/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8398 - mean_squared_error: 0.8398 - val_loss: 0.9634 - val_mean_squared_error: 0.9634\n",
      "Epoch 1063/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8399 - mean_squared_error: 0.8399 - val_loss: 0.9592 - val_mean_squared_error: 0.9592\n",
      "Epoch 1064/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8397 - mean_squared_error: 0.8397 - val_loss: 0.9667 - val_mean_squared_error: 0.9667\n",
      "Epoch 1065/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8401 - mean_squared_error: 0.8401 - val_loss: 0.9642 - val_mean_squared_error: 0.9642\n",
      "Epoch 1066/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8398 - mean_squared_error: 0.8398 - val_loss: 0.9688 - val_mean_squared_error: 0.9688\n",
      "Epoch 1067/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8397 - mean_squared_error: 0.8397 - val_loss: 0.9630 - val_mean_squared_error: 0.9630\n",
      "Epoch 1068/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8395 - mean_squared_error: 0.8395 - val_loss: 0.9661 - val_mean_squared_error: 0.9661\n",
      "Epoch 1069/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8407 - mean_squared_error: 0.8407 - val_loss: 0.9585 - val_mean_squared_error: 0.9585\n",
      "Epoch 1070/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8398 - mean_squared_error: 0.8398 - val_loss: 0.9685 - val_mean_squared_error: 0.9685\n",
      "Epoch 1071/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8397 - mean_squared_error: 0.8397 - val_loss: 0.9697 - val_mean_squared_error: 0.9697\n",
      "Epoch 1072/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8399 - mean_squared_error: 0.8399 - val_loss: 0.9602 - val_mean_squared_error: 0.9602\n",
      "Epoch 1073/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8395 - mean_squared_error: 0.8395 - val_loss: 0.9636 - val_mean_squared_error: 0.9636\n",
      "Epoch 1074/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8399 - mean_squared_error: 0.8399 - val_loss: 0.9667 - val_mean_squared_error: 0.9667\n",
      "Epoch 1075/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8393 - mean_squared_error: 0.8393 - val_loss: 0.9653 - val_mean_squared_error: 0.9653\n",
      "Epoch 1076/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8396 - mean_squared_error: 0.8396 - val_loss: 0.9611 - val_mean_squared_error: 0.9611\n",
      "Epoch 1077/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8400 - mean_squared_error: 0.8400 - val_loss: 0.9593 - val_mean_squared_error: 0.9593\n",
      "Epoch 1078/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8393 - mean_squared_error: 0.8393 - val_loss: 0.9660 - val_mean_squared_error: 0.9660\n",
      "Epoch 1079/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8402 - mean_squared_error: 0.8402 - val_loss: 0.9702 - val_mean_squared_error: 0.9702\n",
      "Epoch 1080/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8394 - mean_squared_error: 0.8394 - val_loss: 0.9640 - val_mean_squared_error: 0.9640\n",
      "Epoch 1081/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8395 - mean_squared_error: 0.8395 - val_loss: 0.9597 - val_mean_squared_error: 0.9597\n",
      "Epoch 1082/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8400 - mean_squared_error: 0.8400 - val_loss: 0.9572 - val_mean_squared_error: 0.9572\n",
      "Epoch 1083/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8394 - mean_squared_error: 0.8394 - val_loss: 0.9647 - val_mean_squared_error: 0.9647\n",
      "Epoch 1084/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8394 - mean_squared_error: 0.8394 - val_loss: 0.9666 - val_mean_squared_error: 0.9666\n",
      "Epoch 1085/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8408 - mean_squared_error: 0.8408 - val_loss: 0.9649 - val_mean_squared_error: 0.9649\n",
      "Epoch 1086/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8395 - mean_squared_error: 0.8395 - val_loss: 0.9719 - val_mean_squared_error: 0.9719\n",
      "Epoch 1087/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8394 - mean_squared_error: 0.8394 - val_loss: 0.9667 - val_mean_squared_error: 0.9667\n",
      "Epoch 1088/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8417 - mean_squared_error: 0.8417 - val_loss: 0.9505 - val_mean_squared_error: 0.9505\n",
      "Epoch 1089/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8398 - mean_squared_error: 0.8398 - val_loss: 0.9650 - val_mean_squared_error: 0.9650\n",
      "Epoch 1090/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8395 - mean_squared_error: 0.8395 - val_loss: 0.9694 - val_mean_squared_error: 0.9694\n",
      "Epoch 1091/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8392 - mean_squared_error: 0.8392 - val_loss: 0.9564 - val_mean_squared_error: 0.9564\n",
      "Epoch 1092/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8400 - mean_squared_error: 0.8400 - val_loss: 0.9665 - val_mean_squared_error: 0.9665\n",
      "Epoch 1093/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8391 - mean_squared_error: 0.8391 - val_loss: 0.9615 - val_mean_squared_error: 0.9615\n",
      "Epoch 1094/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8416 - mean_squared_error: 0.8416 - val_loss: 0.9502 - val_mean_squared_error: 0.9502\n",
      "Epoch 1095/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8388 - mean_squared_error: 0.8388 - val_loss: 0.9649 - val_mean_squared_error: 0.9649\n",
      "Epoch 1096/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8392 - mean_squared_error: 0.8392 - val_loss: 0.9727 - val_mean_squared_error: 0.9727\n",
      "Epoch 1097/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8394 - mean_squared_error: 0.8394 - val_loss: 0.9718 - val_mean_squared_error: 0.9718\n",
      "Epoch 1098/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8394 - mean_squared_error: 0.8394 - val_loss: 0.9577 - val_mean_squared_error: 0.9577\n",
      "Epoch 1099/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8393 - mean_squared_error: 0.8393 - val_loss: 0.9551 - val_mean_squared_error: 0.9551\n",
      "Epoch 1100/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8394 - mean_squared_error: 0.8394 - val_loss: 0.9677 - val_mean_squared_error: 0.9677\n",
      "Epoch 1101/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8390 - mean_squared_error: 0.8390 - val_loss: 0.9658 - val_mean_squared_error: 0.9658\n",
      "Epoch 1102/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8389 - mean_squared_error: 0.8389 - val_loss: 0.9599 - val_mean_squared_error: 0.9599\n",
      "Epoch 1103/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8399 - mean_squared_error: 0.8399 - val_loss: 0.9543 - val_mean_squared_error: 0.9543\n",
      "Epoch 1104/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8385 - mean_squared_error: 0.8385 - val_loss: 0.9637 - val_mean_squared_error: 0.9637\n",
      "Epoch 1105/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8388 - mean_squared_error: 0.8388 - val_loss: 0.9682 - val_mean_squared_error: 0.9682\n",
      "Epoch 1106/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8393 - mean_squared_error: 0.8393 - val_loss: 0.9656 - val_mean_squared_error: 0.9656\n",
      "Epoch 1107/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8399 - mean_squared_error: 0.8399 - val_loss: 0.9548 - val_mean_squared_error: 0.9548\n",
      "Epoch 1108/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8393 - mean_squared_error: 0.8393 - val_loss: 0.9649 - val_mean_squared_error: 0.9649\n",
      "Epoch 1109/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8393 - mean_squared_error: 0.8393 - val_loss: 0.9657 - val_mean_squared_error: 0.9657\n",
      "Epoch 1110/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8394 - mean_squared_error: 0.8394 - val_loss: 0.9622 - val_mean_squared_error: 0.9622\n",
      "Epoch 1111/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8399 - mean_squared_error: 0.8399 - val_loss: 0.9523 - val_mean_squared_error: 0.9523\n",
      "Epoch 1112/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8385 - mean_squared_error: 0.8385 - val_loss: 0.9607 - val_mean_squared_error: 0.9607\n",
      "Epoch 1113/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8385 - mean_squared_error: 0.8385 - val_loss: 0.9708 - val_mean_squared_error: 0.9708\n",
      "Epoch 1114/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8392 - mean_squared_error: 0.8392 - val_loss: 0.9680 - val_mean_squared_error: 0.9680\n",
      "Epoch 1115/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8397 - mean_squared_error: 0.8397 - val_loss: 0.9541 - val_mean_squared_error: 0.9541\n",
      "Epoch 1116/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8397 - mean_squared_error: 0.8397 - val_loss: 0.9652 - val_mean_squared_error: 0.9652\n",
      "Epoch 1117/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8387 - mean_squared_error: 0.8387 - val_loss: 0.9594 - val_mean_squared_error: 0.9594\n",
      "Epoch 1118/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8388 - mean_squared_error: 0.8388 - val_loss: 0.9649 - val_mean_squared_error: 0.9649\n",
      "Epoch 1119/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8390 - mean_squared_error: 0.8390 - val_loss: 0.9625 - val_mean_squared_error: 0.9625\n",
      "Epoch 1120/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8391 - mean_squared_error: 0.8391 - val_loss: 0.9625 - val_mean_squared_error: 0.9625\n",
      "Epoch 1121/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8387 - mean_squared_error: 0.8387 - val_loss: 0.9589 - val_mean_squared_error: 0.9589\n",
      "Epoch 1122/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8387 - mean_squared_error: 0.8387 - val_loss: 0.9602 - val_mean_squared_error: 0.9602\n",
      "Epoch 1123/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8388 - mean_squared_error: 0.8388 - val_loss: 0.9620 - val_mean_squared_error: 0.9620\n",
      "Epoch 1124/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8390 - mean_squared_error: 0.8390 - val_loss: 0.9544 - val_mean_squared_error: 0.9544\n",
      "Epoch 1125/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8388 - mean_squared_error: 0.8388 - val_loss: 0.9629 - val_mean_squared_error: 0.9629\n",
      "Epoch 1126/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8389 - mean_squared_error: 0.8389 - val_loss: 0.9563 - val_mean_squared_error: 0.9563\n",
      "Epoch 1127/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8412 - mean_squared_error: 0.8412 - val_loss: 0.9728 - val_mean_squared_error: 0.9728\n",
      "Epoch 1128/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8385 - mean_squared_error: 0.8385 - val_loss: 0.9564 - val_mean_squared_error: 0.9564\n",
      "Epoch 1129/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8390 - mean_squared_error: 0.8390 - val_loss: 0.9503 - val_mean_squared_error: 0.9503\n",
      "Epoch 1130/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8388 - mean_squared_error: 0.8388 - val_loss: 0.9572 - val_mean_squared_error: 0.9572\n",
      "Epoch 1131/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8380 - mean_squared_error: 0.8380 - val_loss: 0.9673 - val_mean_squared_error: 0.9673\n",
      "Epoch 1132/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8388 - mean_squared_error: 0.8388 - val_loss: 0.9682 - val_mean_squared_error: 0.9682\n",
      "Epoch 1133/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8401 - mean_squared_error: 0.8401 - val_loss: 0.9559 - val_mean_squared_error: 0.9559\n",
      "Epoch 1134/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8384 - mean_squared_error: 0.8384 - val_loss: 0.9626 - val_mean_squared_error: 0.9626\n",
      "Epoch 1135/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8386 - mean_squared_error: 0.8386 - val_loss: 0.9642 - val_mean_squared_error: 0.9642\n",
      "Epoch 1136/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8382 - mean_squared_error: 0.8382 - val_loss: 0.9557 - val_mean_squared_error: 0.9557\n",
      "Epoch 1137/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8384 - mean_squared_error: 0.8384 - val_loss: 0.9593 - val_mean_squared_error: 0.9593\n",
      "Epoch 1138/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8388 - mean_squared_error: 0.8388 - val_loss: 0.9536 - val_mean_squared_error: 0.9536\n",
      "Epoch 1139/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8383 - mean_squared_error: 0.8383 - val_loss: 0.9614 - val_mean_squared_error: 0.9614\n",
      "Epoch 1140/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8383 - mean_squared_error: 0.8383 - val_loss: 0.9630 - val_mean_squared_error: 0.9630\n",
      "Epoch 1141/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8383 - mean_squared_error: 0.8383 - val_loss: 0.9595 - val_mean_squared_error: 0.9595\n",
      "Epoch 1142/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8385 - mean_squared_error: 0.8385 - val_loss: 0.9599 - val_mean_squared_error: 0.9599\n",
      "Epoch 1143/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8383 - mean_squared_error: 0.8383 - val_loss: 0.9581 - val_mean_squared_error: 0.9581\n",
      "Epoch 1144/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8381 - mean_squared_error: 0.8381 - val_loss: 0.9562 - val_mean_squared_error: 0.9562\n",
      "Epoch 1145/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8402 - mean_squared_error: 0.8402 - val_loss: 0.9669 - val_mean_squared_error: 0.9669\n",
      "Epoch 1146/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8387 - mean_squared_error: 0.8387 - val_loss: 0.9513 - val_mean_squared_error: 0.9513\n",
      "Epoch 1147/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8385 - mean_squared_error: 0.8385 - val_loss: 0.9574 - val_mean_squared_error: 0.9574\n",
      "Epoch 1148/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8395 - mean_squared_error: 0.8395 - val_loss: 0.9671 - val_mean_squared_error: 0.9671\n",
      "Epoch 1149/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8389 - mean_squared_error: 0.8389 - val_loss: 0.9491 - val_mean_squared_error: 0.9491\n",
      "Epoch 1150/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8397 - mean_squared_error: 0.8397 - val_loss: 0.9626 - val_mean_squared_error: 0.9626\n",
      "Epoch 1151/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8383 - mean_squared_error: 0.8383 - val_loss: 0.9559 - val_mean_squared_error: 0.9559\n",
      "Epoch 1152/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8387 - mean_squared_error: 0.8387 - val_loss: 0.9601 - val_mean_squared_error: 0.9601\n",
      "Epoch 1153/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8383 - mean_squared_error: 0.8383 - val_loss: 0.9616 - val_mean_squared_error: 0.9616\n",
      "Epoch 1154/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8379 - mean_squared_error: 0.8379 - val_loss: 0.9623 - val_mean_squared_error: 0.9623\n",
      "Epoch 1155/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8381 - mean_squared_error: 0.8381 - val_loss: 0.9600 - val_mean_squared_error: 0.9600\n",
      "Epoch 1156/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8381 - mean_squared_error: 0.8381 - val_loss: 0.9532 - val_mean_squared_error: 0.9532\n",
      "Epoch 1157/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8382 - mean_squared_error: 0.8382 - val_loss: 0.9560 - val_mean_squared_error: 0.9560\n",
      "Epoch 1158/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8381 - mean_squared_error: 0.8381 - val_loss: 0.9598 - val_mean_squared_error: 0.9598\n",
      "Epoch 1159/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8380 - mean_squared_error: 0.8380 - val_loss: 0.9594 - val_mean_squared_error: 0.9594\n",
      "Epoch 1160/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8384 - mean_squared_error: 0.8384 - val_loss: 0.9628 - val_mean_squared_error: 0.9628\n",
      "Epoch 1161/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8385 - mean_squared_error: 0.8385 - val_loss: 0.9632 - val_mean_squared_error: 0.9632\n",
      "Epoch 1162/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8376 - mean_squared_error: 0.8376 - val_loss: 0.9541 - val_mean_squared_error: 0.9541\n",
      "Epoch 1163/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8383 - mean_squared_error: 0.8383 - val_loss: 0.9503 - val_mean_squared_error: 0.9503\n",
      "Epoch 1164/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8381 - mean_squared_error: 0.8381 - val_loss: 0.9615 - val_mean_squared_error: 0.9615\n",
      "Epoch 1165/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8384 - mean_squared_error: 0.8384 - val_loss: 0.9598 - val_mean_squared_error: 0.9598\n",
      "Epoch 1166/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8378 - mean_squared_error: 0.8378 - val_loss: 0.9566 - val_mean_squared_error: 0.9566\n",
      "Epoch 1167/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8379 - mean_squared_error: 0.8379 - val_loss: 0.9566 - val_mean_squared_error: 0.9566\n",
      "Epoch 1168/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8378 - mean_squared_error: 0.8378 - val_loss: 0.9573 - val_mean_squared_error: 0.9573\n",
      "Epoch 1169/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8384 - mean_squared_error: 0.8384 - val_loss: 0.9535 - val_mean_squared_error: 0.9535\n",
      "Epoch 1170/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8377 - mean_squared_error: 0.8377 - val_loss: 0.9624 - val_mean_squared_error: 0.9624\n",
      "Epoch 1171/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8377 - mean_squared_error: 0.8377 - val_loss: 0.9595 - val_mean_squared_error: 0.9595\n",
      "Epoch 1172/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8383 - mean_squared_error: 0.8383 - val_loss: 0.9633 - val_mean_squared_error: 0.9633\n",
      "Epoch 1173/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8387 - mean_squared_error: 0.8387 - val_loss: 0.9524 - val_mean_squared_error: 0.9524\n",
      "Epoch 1174/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8379 - mean_squared_error: 0.8379 - val_loss: 0.9550 - val_mean_squared_error: 0.9550\n",
      "Epoch 1175/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8378 - mean_squared_error: 0.8378 - val_loss: 0.9555 - val_mean_squared_error: 0.9555\n",
      "Epoch 1176/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8378 - mean_squared_error: 0.8378 - val_loss: 0.9639 - val_mean_squared_error: 0.9639\n",
      "Epoch 1177/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8378 - mean_squared_error: 0.8378 - val_loss: 0.9612 - val_mean_squared_error: 0.9612\n",
      "Epoch 1178/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8375 - mean_squared_error: 0.8375 - val_loss: 0.9556 - val_mean_squared_error: 0.9556\n",
      "Epoch 1179/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8383 - mean_squared_error: 0.8383 - val_loss: 0.9432 - val_mean_squared_error: 0.9432\n",
      "Epoch 1180/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8386 - mean_squared_error: 0.8386 - val_loss: 0.9561 - val_mean_squared_error: 0.9561\n",
      "Epoch 1181/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8377 - mean_squared_error: 0.8377 - val_loss: 0.9551 - val_mean_squared_error: 0.9551\n",
      "Epoch 1182/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8379 - mean_squared_error: 0.8379 - val_loss: 0.9584 - val_mean_squared_error: 0.9584\n",
      "Epoch 1183/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8379 - mean_squared_error: 0.8379 - val_loss: 0.9605 - val_mean_squared_error: 0.9605\n",
      "Epoch 1184/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8385 - mean_squared_error: 0.8385 - val_loss: 0.9586 - val_mean_squared_error: 0.9586\n",
      "Epoch 1185/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8379 - mean_squared_error: 0.8379 - val_loss: 0.9460 - val_mean_squared_error: 0.9460\n",
      "Epoch 1186/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8378 - mean_squared_error: 0.8378 - val_loss: 0.9523 - val_mean_squared_error: 0.9523\n",
      "Epoch 1187/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8376 - mean_squared_error: 0.8376 - val_loss: 0.9594 - val_mean_squared_error: 0.9594\n",
      "Epoch 1188/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8379 - mean_squared_error: 0.8379 - val_loss: 0.9608 - val_mean_squared_error: 0.9608\n",
      "Epoch 1189/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8373 - mean_squared_error: 0.8373 - val_loss: 0.9558 - val_mean_squared_error: 0.9558\n",
      "Epoch 1190/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8374 - mean_squared_error: 0.8374 - val_loss: 0.9518 - val_mean_squared_error: 0.9518\n",
      "Epoch 1191/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8380 - mean_squared_error: 0.8380 - val_loss: 0.9509 - val_mean_squared_error: 0.9509\n",
      "Epoch 1192/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8369 - mean_squared_error: 0.8369 - val_loss: 0.9576 - val_mean_squared_error: 0.9576\n",
      "Epoch 1193/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8391 - mean_squared_error: 0.8391 - val_loss: 0.9615 - val_mean_squared_error: 0.9615\n",
      "Epoch 1194/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8379 - mean_squared_error: 0.8379 - val_loss: 0.9542 - val_mean_squared_error: 0.9542\n",
      "Epoch 1195/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8387 - mean_squared_error: 0.8387 - val_loss: 0.9535 - val_mean_squared_error: 0.9535\n",
      "Epoch 1196/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8374 - mean_squared_error: 0.8374 - val_loss: 0.9609 - val_mean_squared_error: 0.9609\n",
      "Epoch 1197/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8384 - mean_squared_error: 0.8384 - val_loss: 0.9512 - val_mean_squared_error: 0.9512\n",
      "Epoch 1198/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8376 - mean_squared_error: 0.8376 - val_loss: 0.9583 - val_mean_squared_error: 0.9583\n",
      "Epoch 1199/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8374 - mean_squared_error: 0.8374 - val_loss: 0.9579 - val_mean_squared_error: 0.9579\n",
      "Epoch 1200/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8373 - mean_squared_error: 0.8373 - val_loss: 0.9510 - val_mean_squared_error: 0.9510\n",
      "Epoch 1201/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8375 - mean_squared_error: 0.8375 - val_loss: 0.9504 - val_mean_squared_error: 0.9504\n",
      "Epoch 1202/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8370 - mean_squared_error: 0.8370 - val_loss: 0.9558 - val_mean_squared_error: 0.9558\n",
      "Epoch 1203/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8371 - mean_squared_error: 0.8371 - val_loss: 0.9589 - val_mean_squared_error: 0.9589\n",
      "Epoch 1204/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8372 - mean_squared_error: 0.8372 - val_loss: 0.9562 - val_mean_squared_error: 0.9562\n",
      "Epoch 1205/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8376 - mean_squared_error: 0.8376 - val_loss: 0.9590 - val_mean_squared_error: 0.9590\n",
      "Epoch 1206/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8370 - mean_squared_error: 0.8370 - val_loss: 0.9531 - val_mean_squared_error: 0.9531\n",
      "Epoch 1207/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8370 - mean_squared_error: 0.8370 - val_loss: 0.9516 - val_mean_squared_error: 0.9516\n",
      "Epoch 1208/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8374 - mean_squared_error: 0.8374 - val_loss: 0.9553 - val_mean_squared_error: 0.9553\n",
      "Epoch 1209/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8372 - mean_squared_error: 0.8372 - val_loss: 0.9545 - val_mean_squared_error: 0.9545\n",
      "Epoch 1210/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8372 - mean_squared_error: 0.8372 - val_loss: 0.9579 - val_mean_squared_error: 0.9579\n",
      "Epoch 1211/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8377 - mean_squared_error: 0.8377 - val_loss: 0.9578 - val_mean_squared_error: 0.9578\n",
      "Epoch 1212/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8371 - mean_squared_error: 0.8371 - val_loss: 0.9554 - val_mean_squared_error: 0.9554\n",
      "Epoch 1213/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8380 - mean_squared_error: 0.8380 - val_loss: 0.9437 - val_mean_squared_error: 0.9437\n",
      "Epoch 1214/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8377 - mean_squared_error: 0.8377 - val_loss: 0.9529 - val_mean_squared_error: 0.9529\n",
      "Epoch 1215/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8376 - mean_squared_error: 0.8376 - val_loss: 0.9509 - val_mean_squared_error: 0.9509\n",
      "Epoch 1216/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8365 - mean_squared_error: 0.8365 - val_loss: 0.9599 - val_mean_squared_error: 0.9599\n",
      "Epoch 1217/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8369 - mean_squared_error: 0.8369 - val_loss: 0.9594 - val_mean_squared_error: 0.9594\n",
      "Epoch 1218/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8378 - mean_squared_error: 0.8378 - val_loss: 0.9601 - val_mean_squared_error: 0.9601\n",
      "Epoch 1219/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8385 - mean_squared_error: 0.8385 - val_loss: 0.9623 - val_mean_squared_error: 0.9623\n",
      "Epoch 1220/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8369 - mean_squared_error: 0.8369 - val_loss: 0.9688 - val_mean_squared_error: 0.9688\n",
      "Epoch 1221/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8369 - mean_squared_error: 0.8369 - val_loss: 0.9663 - val_mean_squared_error: 0.9663\n",
      "Epoch 1222/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8364 - mean_squared_error: 0.8364 - val_loss: 0.9648 - val_mean_squared_error: 0.9648\n",
      "Epoch 1223/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8366 - mean_squared_error: 0.8366 - val_loss: 0.9599 - val_mean_squared_error: 0.9599\n",
      "Epoch 1224/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8366 - mean_squared_error: 0.8366 - val_loss: 0.9609 - val_mean_squared_error: 0.9609\n",
      "Epoch 1225/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.8375 - mean_squared_error: 0.8375 - val_loss: 0.9675 - val_mean_squared_error: 0.9675\n",
      "Epoch 1226/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8381 - mean_squared_error: 0.8381 - val_loss: 0.9857 - val_mean_squared_error: 0.9857\n",
      "Epoch 1227/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8363 - mean_squared_error: 0.8363 - val_loss: 0.9511 - val_mean_squared_error: 0.9511\n",
      "Epoch 1228/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8370 - mean_squared_error: 0.8370 - val_loss: 0.9504 - val_mean_squared_error: 0.9504\n",
      "Epoch 1229/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8370 - mean_squared_error: 0.8370 - val_loss: 0.9616 - val_mean_squared_error: 0.9616\n",
      "Epoch 1230/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8366 - mean_squared_error: 0.8366 - val_loss: 0.9632 - val_mean_squared_error: 0.9632\n",
      "Epoch 1231/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8365 - mean_squared_error: 0.8365 - val_loss: 0.9686 - val_mean_squared_error: 0.9686\n",
      "Epoch 1232/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8373 - mean_squared_error: 0.8373 - val_loss: 0.9701 - val_mean_squared_error: 0.9701\n",
      "Epoch 1233/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8364 - mean_squared_error: 0.8364 - val_loss: 0.9535 - val_mean_squared_error: 0.9535\n",
      "Epoch 1234/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8368 - mean_squared_error: 0.8368 - val_loss: 0.9600 - val_mean_squared_error: 0.9600\n",
      "Epoch 1235/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8365 - mean_squared_error: 0.8365 - val_loss: 0.9592 - val_mean_squared_error: 0.9592\n",
      "Epoch 1236/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8366 - mean_squared_error: 0.8366 - val_loss: 0.9683 - val_mean_squared_error: 0.9683\n",
      "Epoch 1237/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8366 - mean_squared_error: 0.8366 - val_loss: 0.9592 - val_mean_squared_error: 0.9592\n",
      "Epoch 1238/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8366 - mean_squared_error: 0.8366 - val_loss: 0.9590 - val_mean_squared_error: 0.9590\n",
      "Epoch 1239/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8367 - mean_squared_error: 0.8367 - val_loss: 0.9683 - val_mean_squared_error: 0.9683\n",
      "Epoch 1240/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8362 - mean_squared_error: 0.8362 - val_loss: 0.9650 - val_mean_squared_error: 0.9650\n",
      "Epoch 1241/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8370 - mean_squared_error: 0.8370 - val_loss: 0.9542 - val_mean_squared_error: 0.9542\n",
      "Epoch 1242/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8366 - mean_squared_error: 0.8366 - val_loss: 0.9617 - val_mean_squared_error: 0.9617\n",
      "Epoch 1243/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8365 - mean_squared_error: 0.8365 - val_loss: 0.9615 - val_mean_squared_error: 0.9615\n",
      "Epoch 1244/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8367 - mean_squared_error: 0.8367 - val_loss: 0.9606 - val_mean_squared_error: 0.9606\n",
      "Epoch 1245/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8362 - mean_squared_error: 0.8362 - val_loss: 0.9615 - val_mean_squared_error: 0.9615\n",
      "Epoch 1246/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8362 - mean_squared_error: 0.8362 - val_loss: 0.9582 - val_mean_squared_error: 0.9582\n",
      "Epoch 1247/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8366 - mean_squared_error: 0.8366 - val_loss: 0.9578 - val_mean_squared_error: 0.9578\n",
      "Epoch 1248/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8363 - mean_squared_error: 0.8363 - val_loss: 0.9664 - val_mean_squared_error: 0.9664\n",
      "Epoch 1249/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8367 - mean_squared_error: 0.8367 - val_loss: 0.9571 - val_mean_squared_error: 0.9571\n",
      "Epoch 1250/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8365 - mean_squared_error: 0.8365 - val_loss: 0.9679 - val_mean_squared_error: 0.9679\n",
      "Epoch 1251/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8365 - mean_squared_error: 0.8365 - val_loss: 0.9591 - val_mean_squared_error: 0.9591\n",
      "Epoch 1252/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8358 - mean_squared_error: 0.8358 - val_loss: 0.9596 - val_mean_squared_error: 0.9596\n",
      "Epoch 1253/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8359 - mean_squared_error: 0.8359 - val_loss: 0.9655 - val_mean_squared_error: 0.9655\n",
      "Epoch 1254/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8365 - mean_squared_error: 0.8365 - val_loss: 0.9665 - val_mean_squared_error: 0.9665\n",
      "Epoch 1255/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8381 - mean_squared_error: 0.8381 - val_loss: 0.9480 - val_mean_squared_error: 0.9480\n",
      "Epoch 1256/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8362 - mean_squared_error: 0.8362 - val_loss: 0.9512 - val_mean_squared_error: 0.9512\n",
      "Epoch 1257/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8364 - mean_squared_error: 0.8364 - val_loss: 0.9663 - val_mean_squared_error: 0.9663\n",
      "Epoch 1258/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8361 - mean_squared_error: 0.8361 - val_loss: 0.9649 - val_mean_squared_error: 0.9649\n",
      "Epoch 1259/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8361 - mean_squared_error: 0.8361 - val_loss: 0.9538 - val_mean_squared_error: 0.9538\n",
      "Epoch 1260/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8365 - mean_squared_error: 0.8365 - val_loss: 0.9654 - val_mean_squared_error: 0.9654\n",
      "Epoch 1261/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8358 - mean_squared_error: 0.8358 - val_loss: 0.9596 - val_mean_squared_error: 0.9596\n",
      "Epoch 1262/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8356 - mean_squared_error: 0.8356 - val_loss: 0.9511 - val_mean_squared_error: 0.9511\n",
      "Epoch 1263/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8366 - mean_squared_error: 0.8366 - val_loss: 0.9581 - val_mean_squared_error: 0.9581\n",
      "Epoch 1264/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8367 - mean_squared_error: 0.8367 - val_loss: 0.9464 - val_mean_squared_error: 0.9464\n",
      "Epoch 1265/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8357 - mean_squared_error: 0.8357 - val_loss: 0.9546 - val_mean_squared_error: 0.9546\n",
      "Epoch 1266/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8367 - mean_squared_error: 0.8367 - val_loss: 0.9725 - val_mean_squared_error: 0.9725\n",
      "Epoch 1267/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8359 - mean_squared_error: 0.8359 - val_loss: 0.9574 - val_mean_squared_error: 0.9574\n",
      "Epoch 1268/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8357 - mean_squared_error: 0.8357 - val_loss: 0.9584 - val_mean_squared_error: 0.9584\n",
      "Epoch 1269/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8356 - mean_squared_error: 0.8356 - val_loss: 0.9573 - val_mean_squared_error: 0.9573\n",
      "Epoch 1270/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8356 - mean_squared_error: 0.8356 - val_loss: 0.9602 - val_mean_squared_error: 0.9602\n",
      "Epoch 1271/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8357 - mean_squared_error: 0.8357 - val_loss: 0.9596 - val_mean_squared_error: 0.9596\n",
      "Epoch 1272/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8354 - mean_squared_error: 0.8354 - val_loss: 0.9534 - val_mean_squared_error: 0.9534\n",
      "Epoch 1273/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8362 - mean_squared_error: 0.8362 - val_loss: 0.9571 - val_mean_squared_error: 0.9571\n",
      "Epoch 1274/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8354 - mean_squared_error: 0.8354 - val_loss: 0.9521 - val_mean_squared_error: 0.9521\n",
      "Epoch 1275/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8358 - mean_squared_error: 0.8358 - val_loss: 0.9524 - val_mean_squared_error: 0.9524\n",
      "Epoch 1276/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8355 - mean_squared_error: 0.8355 - val_loss: 0.9647 - val_mean_squared_error: 0.9647\n",
      "Epoch 1277/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8364 - mean_squared_error: 0.8364 - val_loss: 0.9561 - val_mean_squared_error: 0.9561\n",
      "Epoch 1278/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8355 - mean_squared_error: 0.8355 - val_loss: 0.9618 - val_mean_squared_error: 0.9618\n",
      "Epoch 1279/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8356 - mean_squared_error: 0.8356 - val_loss: 0.9556 - val_mean_squared_error: 0.9556\n",
      "Epoch 1280/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8356 - mean_squared_error: 0.8356 - val_loss: 0.9550 - val_mean_squared_error: 0.9550\n",
      "Epoch 1281/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8355 - mean_squared_error: 0.8355 - val_loss: 0.9559 - val_mean_squared_error: 0.9559\n",
      "Epoch 1282/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8359 - mean_squared_error: 0.8359 - val_loss: 0.9525 - val_mean_squared_error: 0.9525\n",
      "Epoch 1283/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8357 - mean_squared_error: 0.8357 - val_loss: 0.9551 - val_mean_squared_error: 0.9551\n",
      "Epoch 1284/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8359 - mean_squared_error: 0.8359 - val_loss: 0.9472 - val_mean_squared_error: 0.9472\n",
      "Epoch 1285/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8357 - mean_squared_error: 0.8357 - val_loss: 0.9609 - val_mean_squared_error: 0.9609\n",
      "Epoch 1286/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8357 - mean_squared_error: 0.8357 - val_loss: 0.9606 - val_mean_squared_error: 0.9606\n",
      "Epoch 1287/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8353 - mean_squared_error: 0.8353 - val_loss: 0.9512 - val_mean_squared_error: 0.9512\n",
      "Epoch 1288/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8355 - mean_squared_error: 0.8355 - val_loss: 0.9519 - val_mean_squared_error: 0.9519\n",
      "Epoch 1289/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8354 - mean_squared_error: 0.8354 - val_loss: 0.9530 - val_mean_squared_error: 0.9530\n",
      "Epoch 1290/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8350 - mean_squared_error: 0.8350 - val_loss: 0.9598 - val_mean_squared_error: 0.9598\n",
      "Epoch 1291/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8353 - mean_squared_error: 0.8353 - val_loss: 0.9580 - val_mean_squared_error: 0.9580\n",
      "Epoch 1292/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8357 - mean_squared_error: 0.8357 - val_loss: 0.9503 - val_mean_squared_error: 0.9503\n",
      "Epoch 1293/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8356 - mean_squared_error: 0.8356 - val_loss: 0.9636 - val_mean_squared_error: 0.9636\n",
      "Epoch 1294/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8358 - mean_squared_error: 0.8358 - val_loss: 0.9534 - val_mean_squared_error: 0.9534\n",
      "Epoch 1295/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8349 - mean_squared_error: 0.8349 - val_loss: 0.9553 - val_mean_squared_error: 0.9553\n",
      "Epoch 1296/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8353 - mean_squared_error: 0.8353 - val_loss: 0.9505 - val_mean_squared_error: 0.9505\n",
      "Epoch 1297/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8355 - mean_squared_error: 0.8355 - val_loss: 0.9530 - val_mean_squared_error: 0.9530\n",
      "Epoch 1298/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8349 - mean_squared_error: 0.8349 - val_loss: 0.9595 - val_mean_squared_error: 0.9595\n",
      "Epoch 1299/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8354 - mean_squared_error: 0.8354 - val_loss: 0.9557 - val_mean_squared_error: 0.9557\n",
      "Epoch 1300/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8362 - mean_squared_error: 0.8362 - val_loss: 0.9609 - val_mean_squared_error: 0.9609\n",
      "Epoch 1301/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8373 - mean_squared_error: 0.8373 - val_loss: 0.9435 - val_mean_squared_error: 0.9435\n",
      "Epoch 1302/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8353 - mean_squared_error: 0.8353 - val_loss: 0.9553 - val_mean_squared_error: 0.9553\n",
      "Epoch 1303/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8349 - mean_squared_error: 0.8349 - val_loss: 0.9533 - val_mean_squared_error: 0.9533\n",
      "Epoch 1304/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8357 - mean_squared_error: 0.8357 - val_loss: 0.9535 - val_mean_squared_error: 0.9535\n",
      "Epoch 1305/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8361 - mean_squared_error: 0.8361 - val_loss: 0.9653 - val_mean_squared_error: 0.9653\n",
      "Epoch 1306/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8350 - mean_squared_error: 0.8350 - val_loss: 0.9513 - val_mean_squared_error: 0.9513\n",
      "Epoch 1307/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8348 - mean_squared_error: 0.8348 - val_loss: 0.9489 - val_mean_squared_error: 0.9489\n",
      "Epoch 1308/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8349 - mean_squared_error: 0.8349 - val_loss: 0.9493 - val_mean_squared_error: 0.9493\n",
      "Epoch 1309/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8353 - mean_squared_error: 0.8353 - val_loss: 0.9586 - val_mean_squared_error: 0.9586\n",
      "Epoch 1310/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8355 - mean_squared_error: 0.8355 - val_loss: 0.9510 - val_mean_squared_error: 0.9510\n",
      "Epoch 1311/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8348 - mean_squared_error: 0.8348 - val_loss: 0.9516 - val_mean_squared_error: 0.9516\n",
      "Epoch 1312/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8352 - mean_squared_error: 0.8352 - val_loss: 0.9529 - val_mean_squared_error: 0.9529\n",
      "Epoch 1313/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8347 - mean_squared_error: 0.8347 - val_loss: 0.9508 - val_mean_squared_error: 0.9508\n",
      "Epoch 1314/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8346 - mean_squared_error: 0.8346 - val_loss: 0.9549 - val_mean_squared_error: 0.9549\n",
      "Epoch 1315/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8348 - mean_squared_error: 0.8348 - val_loss: 0.9546 - val_mean_squared_error: 0.9546\n",
      "Epoch 1316/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8353 - mean_squared_error: 0.8353 - val_loss: 0.9444 - val_mean_squared_error: 0.9444\n",
      "Epoch 1317/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8352 - mean_squared_error: 0.8352 - val_loss: 0.9533 - val_mean_squared_error: 0.9533\n",
      "Epoch 1318/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8358 - mean_squared_error: 0.8358 - val_loss: 0.9452 - val_mean_squared_error: 0.9452\n",
      "Epoch 1319/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8355 - mean_squared_error: 0.8355 - val_loss: 0.9589 - val_mean_squared_error: 0.9589\n",
      "Epoch 1320/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8354 - mean_squared_error: 0.8354 - val_loss: 0.9541 - val_mean_squared_error: 0.9541\n",
      "Epoch 1321/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8345 - mean_squared_error: 0.8345 - val_loss: 0.9455 - val_mean_squared_error: 0.9455\n",
      "Epoch 1322/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8348 - mean_squared_error: 0.8348 - val_loss: 0.9477 - val_mean_squared_error: 0.9477\n",
      "Epoch 1323/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8351 - mean_squared_error: 0.8351 - val_loss: 0.9515 - val_mean_squared_error: 0.9515\n",
      "Epoch 1324/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8346 - mean_squared_error: 0.8346 - val_loss: 0.9533 - val_mean_squared_error: 0.9533\n",
      "Epoch 1325/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8346 - mean_squared_error: 0.8346 - val_loss: 0.9539 - val_mean_squared_error: 0.9539\n",
      "Epoch 1326/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8345 - mean_squared_error: 0.8345 - val_loss: 0.9566 - val_mean_squared_error: 0.9566\n",
      "Epoch 1327/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8349 - mean_squared_error: 0.8349 - val_loss: 0.9470 - val_mean_squared_error: 0.9470\n",
      "Epoch 1328/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8346 - mean_squared_error: 0.8346 - val_loss: 0.9467 - val_mean_squared_error: 0.9467\n",
      "Epoch 1329/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8349 - mean_squared_error: 0.8349 - val_loss: 0.9565 - val_mean_squared_error: 0.9565\n",
      "Epoch 1330/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8353 - mean_squared_error: 0.8353 - val_loss: 0.9544 - val_mean_squared_error: 0.9544\n",
      "Epoch 1331/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8346 - mean_squared_error: 0.8346 - val_loss: 0.9399 - val_mean_squared_error: 0.9399\n",
      "Epoch 1332/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8347 - mean_squared_error: 0.8347 - val_loss: 0.9506 - val_mean_squared_error: 0.9506\n",
      "Epoch 1333/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8346 - mean_squared_error: 0.8346 - val_loss: 0.9531 - val_mean_squared_error: 0.9531\n",
      "Epoch 1334/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8346 - mean_squared_error: 0.8346 - val_loss: 0.9473 - val_mean_squared_error: 0.9473\n",
      "Epoch 1335/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8344 - mean_squared_error: 0.8344 - val_loss: 0.9497 - val_mean_squared_error: 0.9497\n",
      "Epoch 1336/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8344 - mean_squared_error: 0.8344 - val_loss: 0.9479 - val_mean_squared_error: 0.9479\n",
      "Epoch 1337/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8342 - mean_squared_error: 0.8342 - val_loss: 0.9536 - val_mean_squared_error: 0.9536\n",
      "Epoch 1338/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8344 - mean_squared_error: 0.8344 - val_loss: 0.9541 - val_mean_squared_error: 0.9541\n",
      "Epoch 1339/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8348 - mean_squared_error: 0.8348 - val_loss: 0.9535 - val_mean_squared_error: 0.9535\n",
      "Epoch 1340/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8347 - mean_squared_error: 0.8347 - val_loss: 0.9439 - val_mean_squared_error: 0.9439\n",
      "Epoch 1341/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8342 - mean_squared_error: 0.8342 - val_loss: 0.9499 - val_mean_squared_error: 0.9499\n",
      "Epoch 1342/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8341 - mean_squared_error: 0.8341 - val_loss: 0.9486 - val_mean_squared_error: 0.9486\n",
      "Epoch 1343/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8351 - mean_squared_error: 0.8351 - val_loss: 0.9520 - val_mean_squared_error: 0.9520\n",
      "Epoch 1344/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8344 - mean_squared_error: 0.8344 - val_loss: 0.9524 - val_mean_squared_error: 0.9524\n",
      "Epoch 1345/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8351 - mean_squared_error: 0.8351 - val_loss: 0.9559 - val_mean_squared_error: 0.9559\n",
      "Epoch 1346/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8341 - mean_squared_error: 0.8341 - val_loss: 0.9512 - val_mean_squared_error: 0.9512\n",
      "Epoch 1347/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8342 - mean_squared_error: 0.8342 - val_loss: 0.9384 - val_mean_squared_error: 0.9384\n",
      "Epoch 1348/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8352 - mean_squared_error: 0.8352 - val_loss: 0.9487 - val_mean_squared_error: 0.9487\n",
      "Epoch 1349/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8351 - mean_squared_error: 0.8351 - val_loss: 0.9460 - val_mean_squared_error: 0.9460\n",
      "Epoch 1350/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8342 - mean_squared_error: 0.8342 - val_loss: 0.9470 - val_mean_squared_error: 0.9470\n",
      "Epoch 1351/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8343 - mean_squared_error: 0.8343 - val_loss: 0.9472 - val_mean_squared_error: 0.9472\n",
      "Epoch 1352/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8353 - mean_squared_error: 0.8353 - val_loss: 0.9451 - val_mean_squared_error: 0.9451\n",
      "Epoch 1353/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8344 - mean_squared_error: 0.8344 - val_loss: 0.9431 - val_mean_squared_error: 0.9431\n",
      "Epoch 1354/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8353 - mean_squared_error: 0.8353 - val_loss: 0.9600 - val_mean_squared_error: 0.9600\n",
      "Epoch 1355/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8343 - mean_squared_error: 0.8343 - val_loss: 0.9539 - val_mean_squared_error: 0.9539\n",
      "Epoch 1356/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8343 - mean_squared_error: 0.8343 - val_loss: 0.9458 - val_mean_squared_error: 0.9458\n",
      "Epoch 1357/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8349 - mean_squared_error: 0.8349 - val_loss: 0.9380 - val_mean_squared_error: 0.9380\n",
      "Epoch 1358/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8348 - mean_squared_error: 0.8348 - val_loss: 0.9569 - val_mean_squared_error: 0.9569\n",
      "Epoch 1359/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8348 - mean_squared_error: 0.8348 - val_loss: 0.9469 - val_mean_squared_error: 0.9469\n",
      "Epoch 1360/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8340 - mean_squared_error: 0.8340 - val_loss: 0.9503 - val_mean_squared_error: 0.9503\n",
      "Epoch 1361/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8340 - mean_squared_error: 0.8340 - val_loss: 0.9498 - val_mean_squared_error: 0.9498\n",
      "Epoch 1362/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8345 - mean_squared_error: 0.8345 - val_loss: 0.9367 - val_mean_squared_error: 0.9367\n",
      "Epoch 1363/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8350 - mean_squared_error: 0.8350 - val_loss: 0.9391 - val_mean_squared_error: 0.9391\n",
      "Epoch 1364/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8346 - mean_squared_error: 0.8346 - val_loss: 0.9522 - val_mean_squared_error: 0.9522\n",
      "Epoch 1365/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8339 - mean_squared_error: 0.8339 - val_loss: 0.9432 - val_mean_squared_error: 0.9432\n",
      "Epoch 1366/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8341 - mean_squared_error: 0.8341 - val_loss: 0.9400 - val_mean_squared_error: 0.9400\n",
      "Epoch 1367/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8337 - mean_squared_error: 0.8337 - val_loss: 0.9437 - val_mean_squared_error: 0.9437\n",
      "Epoch 1368/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8338 - mean_squared_error: 0.8338 - val_loss: 0.9527 - val_mean_squared_error: 0.9527\n",
      "Epoch 1369/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8345 - mean_squared_error: 0.8345 - val_loss: 0.9432 - val_mean_squared_error: 0.9432\n",
      "Epoch 1370/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8351 - mean_squared_error: 0.8351 - val_loss: 0.9571 - val_mean_squared_error: 0.9571\n",
      "Epoch 1371/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8339 - mean_squared_error: 0.8339 - val_loss: 0.9444 - val_mean_squared_error: 0.9444\n",
      "Epoch 1372/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8350 - mean_squared_error: 0.8350 - val_loss: 0.9409 - val_mean_squared_error: 0.9409\n",
      "Epoch 1373/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8338 - mean_squared_error: 0.8338 - val_loss: 0.9430 - val_mean_squared_error: 0.9430\n",
      "Epoch 1374/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8342 - mean_squared_error: 0.8342 - val_loss: 0.9528 - val_mean_squared_error: 0.9528\n",
      "Epoch 1375/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8338 - mean_squared_error: 0.8338 - val_loss: 0.9407 - val_mean_squared_error: 0.9407\n",
      "Epoch 1376/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8342 - mean_squared_error: 0.8342 - val_loss: 0.9374 - val_mean_squared_error: 0.9374\n",
      "Epoch 1377/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8352 - mean_squared_error: 0.8352 - val_loss: 0.9573 - val_mean_squared_error: 0.9573\n",
      "Epoch 1378/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8338 - mean_squared_error: 0.8338 - val_loss: 0.9463 - val_mean_squared_error: 0.9463\n",
      "Epoch 1379/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8338 - mean_squared_error: 0.8338 - val_loss: 0.9348 - val_mean_squared_error: 0.9348\n",
      "Epoch 1380/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8334 - mean_squared_error: 0.8334 - val_loss: 0.9400 - val_mean_squared_error: 0.9400\n",
      "Epoch 1381/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8335 - mean_squared_error: 0.8335 - val_loss: 0.9493 - val_mean_squared_error: 0.9493\n",
      "Epoch 1382/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8332 - mean_squared_error: 0.8332 - val_loss: 0.9449 - val_mean_squared_error: 0.9449\n",
      "Epoch 1383/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8339 - mean_squared_error: 0.8339 - val_loss: 0.9432 - val_mean_squared_error: 0.9432\n",
      "Epoch 1384/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8344 - mean_squared_error: 0.8344 - val_loss: 0.9364 - val_mean_squared_error: 0.9364\n",
      "Epoch 1385/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8347 - mean_squared_error: 0.8347 - val_loss: 0.9559 - val_mean_squared_error: 0.9559\n",
      "Epoch 1386/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8338 - mean_squared_error: 0.8338 - val_loss: 0.9453 - val_mean_squared_error: 0.9453\n",
      "Epoch 1387/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8340 - mean_squared_error: 0.8340 - val_loss: 0.9428 - val_mean_squared_error: 0.9428\n",
      "Epoch 1388/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8339 - mean_squared_error: 0.8339 - val_loss: 0.9378 - val_mean_squared_error: 0.9378\n",
      "Epoch 1389/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8382 - mean_squared_error: 0.8382 - val_loss: 0.9617 - val_mean_squared_error: 0.9617\n",
      "Epoch 1390/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8339 - mean_squared_error: 0.8339 - val_loss: 0.9390 - val_mean_squared_error: 0.9390\n",
      "Epoch 1391/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8334 - mean_squared_error: 0.8334 - val_loss: 0.9414 - val_mean_squared_error: 0.9414\n",
      "Epoch 1392/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8336 - mean_squared_error: 0.8336 - val_loss: 0.9398 - val_mean_squared_error: 0.9398\n",
      "Epoch 1393/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8339 - mean_squared_error: 0.8339 - val_loss: 0.9396 - val_mean_squared_error: 0.9396\n",
      "Epoch 1394/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8340 - mean_squared_error: 0.8340 - val_loss: 0.9355 - val_mean_squared_error: 0.9355\n",
      "Epoch 1395/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8341 - mean_squared_error: 0.8341 - val_loss: 0.9468 - val_mean_squared_error: 0.9468\n",
      "Epoch 1396/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8333 - mean_squared_error: 0.8333 - val_loss: 0.9401 - val_mean_squared_error: 0.9401\n",
      "Epoch 1397/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8332 - mean_squared_error: 0.8332 - val_loss: 0.9389 - val_mean_squared_error: 0.9389\n",
      "Epoch 1398/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8337 - mean_squared_error: 0.8337 - val_loss: 0.9452 - val_mean_squared_error: 0.9452\n",
      "Epoch 1399/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8330 - mean_squared_error: 0.8330 - val_loss: 0.9405 - val_mean_squared_error: 0.9405\n",
      "Epoch 1400/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8330 - mean_squared_error: 0.8330 - val_loss: 0.9407 - val_mean_squared_error: 0.9407\n",
      "Epoch 1401/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8333 - mean_squared_error: 0.8333 - val_loss: 0.9454 - val_mean_squared_error: 0.9454\n",
      "Epoch 1402/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8328 - mean_squared_error: 0.8328 - val_loss: 0.9409 - val_mean_squared_error: 0.9409\n",
      "Epoch 1403/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8333 - mean_squared_error: 0.8333 - val_loss: 0.9358 - val_mean_squared_error: 0.9358\n",
      "Epoch 1404/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8342 - mean_squared_error: 0.8342 - val_loss: 0.9503 - val_mean_squared_error: 0.9503\n",
      "Epoch 1405/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8347 - mean_squared_error: 0.8347 - val_loss: 0.9360 - val_mean_squared_error: 0.9360\n",
      "Epoch 1406/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8331 - mean_squared_error: 0.8331 - val_loss: 0.9416 - val_mean_squared_error: 0.9416\n",
      "Epoch 1407/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8332 - mean_squared_error: 0.8332 - val_loss: 0.9403 - val_mean_squared_error: 0.9403\n",
      "Epoch 1408/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8335 - mean_squared_error: 0.8335 - val_loss: 0.9434 - val_mean_squared_error: 0.9434\n",
      "Epoch 1409/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8326 - mean_squared_error: 0.8326 - val_loss: 0.9369 - val_mean_squared_error: 0.9369\n",
      "Epoch 1410/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8338 - mean_squared_error: 0.8338 - val_loss: 0.9312 - val_mean_squared_error: 0.9312\n",
      "Epoch 1411/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8325 - mean_squared_error: 0.8325 - val_loss: 0.9472 - val_mean_squared_error: 0.9472\n",
      "Epoch 1412/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8335 - mean_squared_error: 0.8335 - val_loss: 0.9424 - val_mean_squared_error: 0.9424\n",
      "Epoch 1413/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8330 - mean_squared_error: 0.8330 - val_loss: 0.9478 - val_mean_squared_error: 0.9478\n",
      "Epoch 1414/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8331 - mean_squared_error: 0.8331 - val_loss: 0.9394 - val_mean_squared_error: 0.9394\n",
      "Epoch 1415/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.8337 - mean_squared_error: 0.8337 - val_loss: 0.9388 - val_mean_squared_error: 0.9388\n",
      "Epoch 1416/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8328 - mean_squared_error: 0.8328 - val_loss: 0.9401 - val_mean_squared_error: 0.9401\n",
      "Epoch 1417/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8328 - mean_squared_error: 0.8328 - val_loss: 0.9404 - val_mean_squared_error: 0.9404\n",
      "Epoch 1418/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8329 - mean_squared_error: 0.8329 - val_loss: 0.9377 - val_mean_squared_error: 0.9377\n",
      "Epoch 1419/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8335 - mean_squared_error: 0.8335 - val_loss: 0.9357 - val_mean_squared_error: 0.9357\n",
      "Epoch 1420/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8324 - mean_squared_error: 0.8324 - val_loss: 0.9406 - val_mean_squared_error: 0.9406\n",
      "Epoch 1421/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8330 - mean_squared_error: 0.8330 - val_loss: 0.9460 - val_mean_squared_error: 0.9460\n",
      "Epoch 1422/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8333 - mean_squared_error: 0.8333 - val_loss: 0.9372 - val_mean_squared_error: 0.9372\n",
      "Epoch 1423/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8326 - mean_squared_error: 0.8326 - val_loss: 0.9405 - val_mean_squared_error: 0.9405\n",
      "Epoch 1424/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8326 - mean_squared_error: 0.8326 - val_loss: 0.9375 - val_mean_squared_error: 0.9375\n",
      "Epoch 1425/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8331 - mean_squared_error: 0.8331 - val_loss: 0.9411 - val_mean_squared_error: 0.9411\n",
      "Epoch 1426/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8328 - mean_squared_error: 0.8328 - val_loss: 0.9351 - val_mean_squared_error: 0.9351\n",
      "Epoch 1427/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8329 - mean_squared_error: 0.8329 - val_loss: 0.9430 - val_mean_squared_error: 0.9430\n",
      "Epoch 1428/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8332 - mean_squared_error: 0.8332 - val_loss: 0.9435 - val_mean_squared_error: 0.9435\n",
      "Epoch 1429/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8325 - mean_squared_error: 0.8325 - val_loss: 0.9348 - val_mean_squared_error: 0.9348\n",
      "Epoch 1430/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8328 - mean_squared_error: 0.8328 - val_loss: 0.9346 - val_mean_squared_error: 0.9346\n",
      "Epoch 1431/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8329 - mean_squared_error: 0.8329 - val_loss: 0.9334 - val_mean_squared_error: 0.9334\n",
      "Epoch 1432/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8344 - mean_squared_error: 0.8344 - val_loss: 0.9477 - val_mean_squared_error: 0.9477\n",
      "Epoch 1433/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8330 - mean_squared_error: 0.8330 - val_loss: 0.9365 - val_mean_squared_error: 0.9365\n",
      "Epoch 1434/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8325 - mean_squared_error: 0.8325 - val_loss: 0.9382 - val_mean_squared_error: 0.9382\n",
      "Epoch 1435/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8325 - mean_squared_error: 0.8325 - val_loss: 0.9367 - val_mean_squared_error: 0.9367\n",
      "Epoch 1436/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8345 - mean_squared_error: 0.8345 - val_loss: 0.9496 - val_mean_squared_error: 0.9496\n",
      "Epoch 1437/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8322 - mean_squared_error: 0.8322 - val_loss: 0.9368 - val_mean_squared_error: 0.9368\n",
      "Epoch 1438/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8326 - mean_squared_error: 0.8326 - val_loss: 0.9331 - val_mean_squared_error: 0.9331\n",
      "Epoch 1439/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8323 - mean_squared_error: 0.8323 - val_loss: 0.9339 - val_mean_squared_error: 0.9339\n",
      "Epoch 1440/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8327 - mean_squared_error: 0.8327 - val_loss: 0.9395 - val_mean_squared_error: 0.9395\n",
      "Epoch 1441/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8326 - mean_squared_error: 0.8326 - val_loss: 0.9368 - val_mean_squared_error: 0.9368\n",
      "Epoch 1442/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8326 - mean_squared_error: 0.8326 - val_loss: 0.9378 - val_mean_squared_error: 0.9378\n",
      "Epoch 1443/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8324 - mean_squared_error: 0.8324 - val_loss: 0.9403 - val_mean_squared_error: 0.9403\n",
      "Epoch 1444/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8326 - mean_squared_error: 0.8326 - val_loss: 0.9396 - val_mean_squared_error: 0.9396\n",
      "Epoch 1445/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8337 - mean_squared_error: 0.8337 - val_loss: 0.9280 - val_mean_squared_error: 0.9280\n",
      "Epoch 1446/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8346 - mean_squared_error: 0.8346 - val_loss: 0.9434 - val_mean_squared_error: 0.9434\n",
      "Epoch 1447/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8325 - mean_squared_error: 0.8325 - val_loss: 0.9386 - val_mean_squared_error: 0.9386\n",
      "Epoch 1448/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8325 - mean_squared_error: 0.8325 - val_loss: 0.9341 - val_mean_squared_error: 0.9341\n",
      "Epoch 1449/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8323 - mean_squared_error: 0.8323 - val_loss: 0.9343 - val_mean_squared_error: 0.9343\n",
      "Epoch 1450/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8321 - mean_squared_error: 0.8321 - val_loss: 0.9328 - val_mean_squared_error: 0.9328\n",
      "Epoch 1451/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8325 - mean_squared_error: 0.8325 - val_loss: 0.9321 - val_mean_squared_error: 0.9321\n",
      "Epoch 1452/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8323 - mean_squared_error: 0.8323 - val_loss: 0.9371 - val_mean_squared_error: 0.9371\n",
      "Epoch 1453/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8329 - mean_squared_error: 0.8329 - val_loss: 0.9450 - val_mean_squared_error: 0.9450\n",
      "Epoch 1454/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8320 - mean_squared_error: 0.8320 - val_loss: 0.9390 - val_mean_squared_error: 0.9390\n",
      "Epoch 1455/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8321 - mean_squared_error: 0.8321 - val_loss: 0.9348 - val_mean_squared_error: 0.9348\n",
      "Epoch 1456/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8325 - mean_squared_error: 0.8325 - val_loss: 0.9270 - val_mean_squared_error: 0.9270\n",
      "Epoch 1457/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8323 - mean_squared_error: 0.8323 - val_loss: 0.9341 - val_mean_squared_error: 0.9341\n",
      "Epoch 1458/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8329 - mean_squared_error: 0.8329 - val_loss: 0.9321 - val_mean_squared_error: 0.9321\n",
      "Epoch 1459/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8336 - mean_squared_error: 0.8336 - val_loss: 0.9445 - val_mean_squared_error: 0.9445\n",
      "Epoch 1460/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8326 - mean_squared_error: 0.8326 - val_loss: 0.9368 - val_mean_squared_error: 0.9368\n",
      "Epoch 1461/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8317 - mean_squared_error: 0.8317 - val_loss: 0.9319 - val_mean_squared_error: 0.9319\n",
      "Epoch 1462/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8326 - mean_squared_error: 0.8326 - val_loss: 0.9282 - val_mean_squared_error: 0.9282\n",
      "Epoch 1463/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8334 - mean_squared_error: 0.8334 - val_loss: 0.9391 - val_mean_squared_error: 0.9391\n",
      "Epoch 1464/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8331 - mean_squared_error: 0.8331 - val_loss: 0.9328 - val_mean_squared_error: 0.9328\n",
      "Epoch 1465/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8318 - mean_squared_error: 0.8318 - val_loss: 0.9383 - val_mean_squared_error: 0.9383\n",
      "Epoch 1466/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8321 - mean_squared_error: 0.8321 - val_loss: 0.9395 - val_mean_squared_error: 0.9395\n",
      "Epoch 1467/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8322 - mean_squared_error: 0.8322 - val_loss: 0.9403 - val_mean_squared_error: 0.9403\n",
      "Epoch 1468/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8318 - mean_squared_error: 0.8318 - val_loss: 0.9338 - val_mean_squared_error: 0.9338\n",
      "Epoch 1469/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8330 - mean_squared_error: 0.8330 - val_loss: 0.9286 - val_mean_squared_error: 0.9286\n",
      "Epoch 1470/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8343 - mean_squared_error: 0.8343 - val_loss: 0.9400 - val_mean_squared_error: 0.9400\n",
      "Epoch 1471/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8317 - mean_squared_error: 0.8317 - val_loss: 0.9360 - val_mean_squared_error: 0.9360\n",
      "Epoch 1472/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8323 - mean_squared_error: 0.8323 - val_loss: 0.9248 - val_mean_squared_error: 0.9248\n",
      "Epoch 1473/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8334 - mean_squared_error: 0.8334 - val_loss: 0.9258 - val_mean_squared_error: 0.9258\n",
      "Epoch 1474/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8323 - mean_squared_error: 0.8323 - val_loss: 0.9424 - val_mean_squared_error: 0.9424\n",
      "Epoch 1475/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8326 - mean_squared_error: 0.8326 - val_loss: 0.9350 - val_mean_squared_error: 0.9350\n",
      "Epoch 1476/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8325 - mean_squared_error: 0.8325 - val_loss: 0.9324 - val_mean_squared_error: 0.9324\n",
      "Epoch 1477/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8317 - mean_squared_error: 0.8317 - val_loss: 0.9386 - val_mean_squared_error: 0.9386\n",
      "Epoch 1478/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8319 - mean_squared_error: 0.8319 - val_loss: 0.9360 - val_mean_squared_error: 0.9360\n",
      "Epoch 1479/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8323 - mean_squared_error: 0.8323 - val_loss: 0.9414 - val_mean_squared_error: 0.9414\n",
      "Epoch 1480/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8320 - mean_squared_error: 0.8320 - val_loss: 0.9306 - val_mean_squared_error: 0.9306\n",
      "Epoch 1481/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8319 - mean_squared_error: 0.8319 - val_loss: 0.9363 - val_mean_squared_error: 0.9363\n",
      "Epoch 1482/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8317 - mean_squared_error: 0.8317 - val_loss: 0.9334 - val_mean_squared_error: 0.9334\n",
      "Epoch 1483/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8319 - mean_squared_error: 0.8319 - val_loss: 0.9336 - val_mean_squared_error: 0.9336\n",
      "Epoch 1484/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8321 - mean_squared_error: 0.8321 - val_loss: 0.9301 - val_mean_squared_error: 0.9301\n",
      "Epoch 1485/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8321 - mean_squared_error: 0.8321 - val_loss: 0.9369 - val_mean_squared_error: 0.9369\n",
      "Epoch 1486/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8318 - mean_squared_error: 0.8318 - val_loss: 0.9323 - val_mean_squared_error: 0.9323\n",
      "Epoch 1487/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8316 - mean_squared_error: 0.8316 - val_loss: 0.9275 - val_mean_squared_error: 0.9275\n",
      "Epoch 1488/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8318 - mean_squared_error: 0.8318 - val_loss: 0.9334 - val_mean_squared_error: 0.9334\n",
      "Epoch 1489/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8320 - mean_squared_error: 0.8320 - val_loss: 0.9313 - val_mean_squared_error: 0.9313\n",
      "Epoch 1490/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8316 - mean_squared_error: 0.8316 - val_loss: 0.9319 - val_mean_squared_error: 0.9319\n",
      "Epoch 1491/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8318 - mean_squared_error: 0.8318 - val_loss: 0.9272 - val_mean_squared_error: 0.9272\n",
      "Epoch 1492/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8323 - mean_squared_error: 0.8323 - val_loss: 0.9365 - val_mean_squared_error: 0.9365\n",
      "Epoch 1493/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8316 - mean_squared_error: 0.8316 - val_loss: 0.9281 - val_mean_squared_error: 0.9281\n",
      "Epoch 1494/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8335 - mean_squared_error: 0.8335 - val_loss: 0.9235 - val_mean_squared_error: 0.9235\n",
      "Epoch 1495/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8315 - mean_squared_error: 0.8315 - val_loss: 0.9377 - val_mean_squared_error: 0.9377\n",
      "Epoch 1496/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8320 - mean_squared_error: 0.8320 - val_loss: 0.9410 - val_mean_squared_error: 0.9410\n",
      "Epoch 1497/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8313 - mean_squared_error: 0.8313 - val_loss: 0.9330 - val_mean_squared_error: 0.9330\n",
      "Epoch 1498/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8317 - mean_squared_error: 0.8317 - val_loss: 0.9265 - val_mean_squared_error: 0.9265\n",
      "Epoch 1499/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8318 - mean_squared_error: 0.8318 - val_loss: 0.9294 - val_mean_squared_error: 0.9294\n",
      "Epoch 1500/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8322 - mean_squared_error: 0.8322 - val_loss: 0.9404 - val_mean_squared_error: 0.9404\n",
      "Epoch 1501/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8314 - mean_squared_error: 0.8314 - val_loss: 0.9327 - val_mean_squared_error: 0.9327\n",
      "Epoch 1502/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8315 - mean_squared_error: 0.8315 - val_loss: 0.9277 - val_mean_squared_error: 0.9277\n",
      "Epoch 1503/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8318 - mean_squared_error: 0.8318 - val_loss: 0.9271 - val_mean_squared_error: 0.9271\n",
      "Epoch 1504/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8318 - mean_squared_error: 0.8318 - val_loss: 0.9302 - val_mean_squared_error: 0.9302\n",
      "Epoch 1505/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8325 - mean_squared_error: 0.8325 - val_loss: 0.9256 - val_mean_squared_error: 0.9256\n",
      "Epoch 1506/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8316 - mean_squared_error: 0.8316 - val_loss: 0.9303 - val_mean_squared_error: 0.9303\n",
      "Epoch 1507/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8316 - mean_squared_error: 0.8316 - val_loss: 0.9370 - val_mean_squared_error: 0.9370\n",
      "Epoch 1508/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8321 - mean_squared_error: 0.8321 - val_loss: 0.9327 - val_mean_squared_error: 0.9327\n",
      "Epoch 1509/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8315 - mean_squared_error: 0.8315 - val_loss: 0.9312 - val_mean_squared_error: 0.9312\n",
      "Epoch 1510/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8314 - mean_squared_error: 0.8314 - val_loss: 0.9310 - val_mean_squared_error: 0.9310\n",
      "Epoch 1511/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8328 - mean_squared_error: 0.8328 - val_loss: 0.9250 - val_mean_squared_error: 0.9250\n",
      "Epoch 1512/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8320 - mean_squared_error: 0.8320 - val_loss: 0.9357 - val_mean_squared_error: 0.9357\n",
      "Epoch 1513/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8316 - mean_squared_error: 0.8316 - val_loss: 0.9306 - val_mean_squared_error: 0.9306\n",
      "Epoch 1514/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8315 - mean_squared_error: 0.8315 - val_loss: 0.9312 - val_mean_squared_error: 0.9312\n",
      "Epoch 1515/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8313 - mean_squared_error: 0.8313 - val_loss: 0.9332 - val_mean_squared_error: 0.9332\n",
      "Epoch 1516/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8313 - mean_squared_error: 0.8313 - val_loss: 0.9341 - val_mean_squared_error: 0.9341\n",
      "Epoch 1517/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8328 - mean_squared_error: 0.8328 - val_loss: 0.9377 - val_mean_squared_error: 0.9377\n",
      "Epoch 1518/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8313 - mean_squared_error: 0.8313 - val_loss: 0.9230 - val_mean_squared_error: 0.9230\n",
      "Epoch 1519/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8318 - mean_squared_error: 0.8318 - val_loss: 0.9243 - val_mean_squared_error: 0.9243\n",
      "Epoch 1520/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8320 - mean_squared_error: 0.8320 - val_loss: 0.9296 - val_mean_squared_error: 0.9296\n",
      "Epoch 1521/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8318 - mean_squared_error: 0.8318 - val_loss: 0.9243 - val_mean_squared_error: 0.9243\n",
      "Epoch 1522/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8313 - mean_squared_error: 0.8313 - val_loss: 0.9279 - val_mean_squared_error: 0.9279\n",
      "Epoch 1523/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8332 - mean_squared_error: 0.8332 - val_loss: 0.9380 - val_mean_squared_error: 0.9380\n",
      "Epoch 1524/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8310 - mean_squared_error: 0.8310 - val_loss: 0.9284 - val_mean_squared_error: 0.9284\n",
      "Epoch 1525/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8317 - mean_squared_error: 0.8317 - val_loss: 0.9228 - val_mean_squared_error: 0.9228\n",
      "Epoch 1526/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8316 - mean_squared_error: 0.8316 - val_loss: 0.9301 - val_mean_squared_error: 0.9301\n",
      "Epoch 1527/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8311 - mean_squared_error: 0.8311 - val_loss: 0.9286 - val_mean_squared_error: 0.9286\n",
      "Epoch 1528/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8311 - mean_squared_error: 0.8311 - val_loss: 0.9325 - val_mean_squared_error: 0.9325\n",
      "Epoch 1529/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8323 - mean_squared_error: 0.8323 - val_loss: 0.9242 - val_mean_squared_error: 0.9242\n",
      "Epoch 1530/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8310 - mean_squared_error: 0.8310 - val_loss: 0.9291 - val_mean_squared_error: 0.9291\n",
      "Epoch 1531/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8313 - mean_squared_error: 0.8313 - val_loss: 0.9300 - val_mean_squared_error: 0.9300\n",
      "Epoch 1532/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8312 - mean_squared_error: 0.8312 - val_loss: 0.9321 - val_mean_squared_error: 0.9321\n",
      "Epoch 1533/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8317 - mean_squared_error: 0.8317 - val_loss: 0.9298 - val_mean_squared_error: 0.9298\n",
      "Epoch 1534/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8320 - mean_squared_error: 0.8320 - val_loss: 0.9355 - val_mean_squared_error: 0.9355\n",
      "Epoch 1535/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8312 - mean_squared_error: 0.8312 - val_loss: 0.9300 - val_mean_squared_error: 0.9300\n",
      "Epoch 1536/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8313 - mean_squared_error: 0.8313 - val_loss: 0.9234 - val_mean_squared_error: 0.9234\n",
      "Epoch 1537/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8310 - mean_squared_error: 0.8310 - val_loss: 0.9293 - val_mean_squared_error: 0.9293\n",
      "Epoch 1538/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8309 - mean_squared_error: 0.8309 - val_loss: 0.9319 - val_mean_squared_error: 0.9319\n",
      "Epoch 1539/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8317 - mean_squared_error: 0.8317 - val_loss: 0.9347 - val_mean_squared_error: 0.9347\n",
      "Epoch 1540/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8308 - mean_squared_error: 0.8308 - val_loss: 0.9212 - val_mean_squared_error: 0.9212\n",
      "Epoch 1541/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8330 - mean_squared_error: 0.8330 - val_loss: 0.9231 - val_mean_squared_error: 0.9231\n",
      "Epoch 1542/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8312 - mean_squared_error: 0.8312 - val_loss: 0.9356 - val_mean_squared_error: 0.9356\n",
      "Epoch 1543/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8313 - mean_squared_error: 0.8313 - val_loss: 0.9308 - val_mean_squared_error: 0.9308\n",
      "Epoch 1544/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8321 - mean_squared_error: 0.8321 - val_loss: 0.9336 - val_mean_squared_error: 0.9336\n",
      "Epoch 1545/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8309 - mean_squared_error: 0.8309 - val_loss: 0.9252 - val_mean_squared_error: 0.9252\n",
      "Epoch 1546/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8309 - mean_squared_error: 0.8309 - val_loss: 0.9254 - val_mean_squared_error: 0.9254\n",
      "Epoch 1547/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8310 - mean_squared_error: 0.8310 - val_loss: 0.9283 - val_mean_squared_error: 0.9283\n",
      "Epoch 1548/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8316 - mean_squared_error: 0.8316 - val_loss: 0.9236 - val_mean_squared_error: 0.9236\n",
      "Epoch 1549/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8307 - mean_squared_error: 0.8307 - val_loss: 0.9313 - val_mean_squared_error: 0.9313\n",
      "Epoch 1550/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8307 - mean_squared_error: 0.8307 - val_loss: 0.9335 - val_mean_squared_error: 0.9335\n",
      "Epoch 1551/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8309 - mean_squared_error: 0.8309 - val_loss: 0.9303 - val_mean_squared_error: 0.9303\n",
      "Epoch 1552/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8311 - mean_squared_error: 0.8311 - val_loss: 0.9309 - val_mean_squared_error: 0.9309\n",
      "Epoch 1553/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8307 - mean_squared_error: 0.8307 - val_loss: 0.9205 - val_mean_squared_error: 0.9205\n",
      "Epoch 1554/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8319 - mean_squared_error: 0.8319 - val_loss: 0.9166 - val_mean_squared_error: 0.9166\n",
      "Epoch 1555/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8341 - mean_squared_error: 0.8341 - val_loss: 0.9340 - val_mean_squared_error: 0.9340\n",
      "Epoch 1556/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8311 - mean_squared_error: 0.8311 - val_loss: 0.9258 - val_mean_squared_error: 0.9258\n",
      "Epoch 1557/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8310 - mean_squared_error: 0.8310 - val_loss: 0.9218 - val_mean_squared_error: 0.9218\n",
      "Epoch 1558/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8333 - mean_squared_error: 0.8333 - val_loss: 0.9164 - val_mean_squared_error: 0.9164\n",
      "Epoch 1559/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8299 - mean_squared_error: 0.8299 - val_loss: 0.9256 - val_mean_squared_error: 0.9256\n",
      "Epoch 1560/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8327 - mean_squared_error: 0.8327 - val_loss: 0.9436 - val_mean_squared_error: 0.9436\n",
      "Epoch 1561/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8314 - mean_squared_error: 0.8314 - val_loss: 0.9315 - val_mean_squared_error: 0.9315\n",
      "Epoch 1562/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8309 - mean_squared_error: 0.8309 - val_loss: 0.9249 - val_mean_squared_error: 0.9249\n",
      "Epoch 1563/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8311 - mean_squared_error: 0.8311 - val_loss: 0.9226 - val_mean_squared_error: 0.9226\n",
      "Epoch 1564/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8322 - mean_squared_error: 0.8322 - val_loss: 0.9329 - val_mean_squared_error: 0.9329\n",
      "Epoch 1565/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8305 - mean_squared_error: 0.8305 - val_loss: 0.9263 - val_mean_squared_error: 0.9263\n",
      "Epoch 1566/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8309 - mean_squared_error: 0.8309 - val_loss: 0.9221 - val_mean_squared_error: 0.9221\n",
      "Epoch 1567/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8313 - mean_squared_error: 0.8313 - val_loss: 0.9272 - val_mean_squared_error: 0.9272\n",
      "Epoch 1568/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8305 - mean_squared_error: 0.8305 - val_loss: 0.9269 - val_mean_squared_error: 0.9269\n",
      "Epoch 1569/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8307 - mean_squared_error: 0.8307 - val_loss: 0.9221 - val_mean_squared_error: 0.9221\n",
      "Epoch 1570/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8317 - mean_squared_error: 0.8317 - val_loss: 0.9181 - val_mean_squared_error: 0.9181\n",
      "Epoch 1571/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8310 - mean_squared_error: 0.8310 - val_loss: 0.9288 - val_mean_squared_error: 0.9288\n",
      "Epoch 1572/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8310 - mean_squared_error: 0.8310 - val_loss: 0.9280 - val_mean_squared_error: 0.9280\n",
      "Epoch 1573/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8308 - mean_squared_error: 0.8308 - val_loss: 0.9222 - val_mean_squared_error: 0.9222\n",
      "Epoch 1574/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8308 - mean_squared_error: 0.8308 - val_loss: 0.9214 - val_mean_squared_error: 0.9214\n",
      "Epoch 1575/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8304 - mean_squared_error: 0.8304 - val_loss: 0.9229 - val_mean_squared_error: 0.9229\n",
      "Epoch 1576/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8306 - mean_squared_error: 0.8306 - val_loss: 0.9231 - val_mean_squared_error: 0.9231\n",
      "Epoch 1577/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8306 - mean_squared_error: 0.8306 - val_loss: 0.9292 - val_mean_squared_error: 0.9292\n",
      "Epoch 1578/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8306 - mean_squared_error: 0.8306 - val_loss: 0.9232 - val_mean_squared_error: 0.9232\n",
      "Epoch 1579/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8307 - mean_squared_error: 0.8307 - val_loss: 0.9245 - val_mean_squared_error: 0.9245\n",
      "Epoch 1580/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8318 - mean_squared_error: 0.8318 - val_loss: 0.9201 - val_mean_squared_error: 0.9201\n",
      "Epoch 1581/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8310 - mean_squared_error: 0.8310 - val_loss: 0.9323 - val_mean_squared_error: 0.9323\n",
      "Epoch 1582/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8304 - mean_squared_error: 0.8304 - val_loss: 0.9266 - val_mean_squared_error: 0.9266\n",
      "Epoch 1583/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8308 - mean_squared_error: 0.8308 - val_loss: 0.9242 - val_mean_squared_error: 0.9242\n",
      "Epoch 1584/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8307 - mean_squared_error: 0.8307 - val_loss: 0.9226 - val_mean_squared_error: 0.9226\n",
      "Epoch 1585/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8316 - mean_squared_error: 0.8316 - val_loss: 0.9271 - val_mean_squared_error: 0.9271\n",
      "Epoch 1586/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8300 - mean_squared_error: 0.8300 - val_loss: 0.9201 - val_mean_squared_error: 0.9201\n",
      "Epoch 1587/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8306 - mean_squared_error: 0.8306 - val_loss: 0.9169 - val_mean_squared_error: 0.9169\n",
      "Epoch 1588/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8306 - mean_squared_error: 0.8306 - val_loss: 0.9263 - val_mean_squared_error: 0.9263\n",
      "Epoch 1589/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8308 - mean_squared_error: 0.8308 - val_loss: 0.9274 - val_mean_squared_error: 0.9274\n",
      "Epoch 1590/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8312 - mean_squared_error: 0.8312 - val_loss: 0.9304 - val_mean_squared_error: 0.9304\n",
      "Epoch 1591/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8301 - mean_squared_error: 0.8301 - val_loss: 0.9213 - val_mean_squared_error: 0.9213\n",
      "Epoch 1592/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8317 - mean_squared_error: 0.8317 - val_loss: 0.9182 - val_mean_squared_error: 0.9182\n",
      "Epoch 1593/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8310 - mean_squared_error: 0.8310 - val_loss: 0.9298 - val_mean_squared_error: 0.9298\n",
      "Epoch 1594/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8303 - mean_squared_error: 0.8303 - val_loss: 0.9258 - val_mean_squared_error: 0.9258\n",
      "Epoch 1595/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8302 - mean_squared_error: 0.8302 - val_loss: 0.9218 - val_mean_squared_error: 0.9218\n",
      "Epoch 1596/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8302 - mean_squared_error: 0.8302 - val_loss: 0.9178 - val_mean_squared_error: 0.9178\n",
      "Epoch 1597/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8308 - mean_squared_error: 0.8308 - val_loss: 0.9247 - val_mean_squared_error: 0.9247\n",
      "Epoch 1598/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8303 - mean_squared_error: 0.8303 - val_loss: 0.9215 - val_mean_squared_error: 0.9215\n",
      "Epoch 1599/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8302 - mean_squared_error: 0.8302 - val_loss: 0.9225 - val_mean_squared_error: 0.9225\n",
      "Epoch 1600/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8307 - mean_squared_error: 0.8307 - val_loss: 0.9165 - val_mean_squared_error: 0.9165\n",
      "Epoch 1601/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8302 - mean_squared_error: 0.8302 - val_loss: 0.9223 - val_mean_squared_error: 0.9223\n",
      "Epoch 1602/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8301 - mean_squared_error: 0.8301 - val_loss: 0.9268 - val_mean_squared_error: 0.9268\n",
      "Epoch 1603/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8305 - mean_squared_error: 0.8305 - val_loss: 0.9237 - val_mean_squared_error: 0.9237\n",
      "Epoch 1604/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8304 - mean_squared_error: 0.8304 - val_loss: 0.9188 - val_mean_squared_error: 0.9188\n",
      "Epoch 1605/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8322 - mean_squared_error: 0.8322 - val_loss: 0.9306 - val_mean_squared_error: 0.9306\n",
      "Epoch 1606/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8307 - mean_squared_error: 0.8307 - val_loss: 0.9223 - val_mean_squared_error: 0.9223\n",
      "Epoch 1607/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8302 - mean_squared_error: 0.8302 - val_loss: 0.9214 - val_mean_squared_error: 0.9214\n",
      "Epoch 1608/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8301 - mean_squared_error: 0.8301 - val_loss: 0.9221 - val_mean_squared_error: 0.9221\n",
      "Epoch 1609/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8302 - mean_squared_error: 0.8302 - val_loss: 0.9209 - val_mean_squared_error: 0.9209\n",
      "Epoch 1610/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8311 - mean_squared_error: 0.8311 - val_loss: 0.9186 - val_mean_squared_error: 0.9186\n",
      "Epoch 1611/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8303 - mean_squared_error: 0.8303 - val_loss: 0.9218 - val_mean_squared_error: 0.9218\n",
      "Epoch 1612/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8301 - mean_squared_error: 0.8301 - val_loss: 0.9224 - val_mean_squared_error: 0.9224\n",
      "Epoch 1613/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8299 - mean_squared_error: 0.8299 - val_loss: 0.9243 - val_mean_squared_error: 0.9243\n",
      "Epoch 1614/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8297 - mean_squared_error: 0.8297 - val_loss: 0.9217 - val_mean_squared_error: 0.9217\n",
      "Epoch 1615/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8305 - mean_squared_error: 0.8305 - val_loss: 0.9168 - val_mean_squared_error: 0.9168\n",
      "Epoch 1616/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8299 - mean_squared_error: 0.8299 - val_loss: 0.9224 - val_mean_squared_error: 0.9224\n",
      "Epoch 1617/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8305 - mean_squared_error: 0.8305 - val_loss: 0.9237 - val_mean_squared_error: 0.9237\n",
      "Epoch 1618/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8302 - mean_squared_error: 0.8302 - val_loss: 0.9242 - val_mean_squared_error: 0.9242\n",
      "Epoch 1619/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8300 - mean_squared_error: 0.8300 - val_loss: 0.9193 - val_mean_squared_error: 0.9193\n",
      "Epoch 1620/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8303 - mean_squared_error: 0.8303 - val_loss: 0.9168 - val_mean_squared_error: 0.9168\n",
      "Epoch 1621/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8298 - mean_squared_error: 0.8298 - val_loss: 0.9198 - val_mean_squared_error: 0.9198\n",
      "Epoch 1622/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8299 - mean_squared_error: 0.8299 - val_loss: 0.9230 - val_mean_squared_error: 0.9230\n",
      "Epoch 1623/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8310 - mean_squared_error: 0.8310 - val_loss: 0.9250 - val_mean_squared_error: 0.9250\n",
      "Epoch 1624/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8299 - mean_squared_error: 0.8299 - val_loss: 0.9196 - val_mean_squared_error: 0.9196\n",
      "Epoch 1625/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8301 - mean_squared_error: 0.8301 - val_loss: 0.9193 - val_mean_squared_error: 0.9193\n",
      "Epoch 1626/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8299 - mean_squared_error: 0.8299 - val_loss: 0.9176 - val_mean_squared_error: 0.9176\n",
      "Epoch 1627/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8307 - mean_squared_error: 0.8307 - val_loss: 0.9185 - val_mean_squared_error: 0.9185\n",
      "Epoch 1628/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8303 - mean_squared_error: 0.8303 - val_loss: 0.9251 - val_mean_squared_error: 0.9251\n",
      "Epoch 1629/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8298 - mean_squared_error: 0.8298 - val_loss: 0.9215 - val_mean_squared_error: 0.9215\n",
      "Epoch 1630/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8298 - mean_squared_error: 0.8298 - val_loss: 0.9213 - val_mean_squared_error: 0.9213\n",
      "Epoch 1631/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8300 - mean_squared_error: 0.8300 - val_loss: 0.9170 - val_mean_squared_error: 0.9170\n",
      "Epoch 1632/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8300 - mean_squared_error: 0.8300 - val_loss: 0.9177 - val_mean_squared_error: 0.9177\n",
      "Epoch 1633/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8295 - mean_squared_error: 0.8295 - val_loss: 0.9207 - val_mean_squared_error: 0.9207\n",
      "Epoch 1634/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8301 - mean_squared_error: 0.8301 - val_loss: 0.9201 - val_mean_squared_error: 0.9201\n",
      "Epoch 1635/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8298 - mean_squared_error: 0.8298 - val_loss: 0.9168 - val_mean_squared_error: 0.9168\n",
      "Epoch 1636/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8298 - mean_squared_error: 0.8298 - val_loss: 0.9201 - val_mean_squared_error: 0.9201\n",
      "Epoch 1637/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8298 - mean_squared_error: 0.8298 - val_loss: 0.9239 - val_mean_squared_error: 0.9239\n",
      "Epoch 1638/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8298 - mean_squared_error: 0.8298 - val_loss: 0.9229 - val_mean_squared_error: 0.9229\n",
      "Epoch 1639/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8316 - mean_squared_error: 0.8316 - val_loss: 0.9147 - val_mean_squared_error: 0.9147\n",
      "Epoch 1640/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8294 - mean_squared_error: 0.8294 - val_loss: 0.9201 - val_mean_squared_error: 0.9201\n",
      "Epoch 1641/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8310 - mean_squared_error: 0.8310 - val_loss: 0.9298 - val_mean_squared_error: 0.9298\n",
      "Epoch 1642/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8295 - mean_squared_error: 0.8295 - val_loss: 0.9188 - val_mean_squared_error: 0.9188\n",
      "Epoch 1643/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8299 - mean_squared_error: 0.8299 - val_loss: 0.9132 - val_mean_squared_error: 0.9132\n",
      "Epoch 1644/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8296 - mean_squared_error: 0.8296 - val_loss: 0.9171 - val_mean_squared_error: 0.9171\n",
      "Epoch 1645/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8296 - mean_squared_error: 0.8296 - val_loss: 0.9194 - val_mean_squared_error: 0.9194\n",
      "Epoch 1646/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8304 - mean_squared_error: 0.8304 - val_loss: 0.9211 - val_mean_squared_error: 0.9211\n",
      "Epoch 1647/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8306 - mean_squared_error: 0.8306 - val_loss: 0.9136 - val_mean_squared_error: 0.9136\n",
      "Epoch 1648/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8295 - mean_squared_error: 0.8295 - val_loss: 0.9159 - val_mean_squared_error: 0.9159\n",
      "Epoch 1649/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8294 - mean_squared_error: 0.8294 - val_loss: 0.9192 - val_mean_squared_error: 0.9192\n",
      "Epoch 1650/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8294 - mean_squared_error: 0.8294 - val_loss: 0.9206 - val_mean_squared_error: 0.9206\n",
      "Epoch 1651/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8298 - mean_squared_error: 0.8298 - val_loss: 0.9169 - val_mean_squared_error: 0.9169\n",
      "Epoch 1652/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8301 - mean_squared_error: 0.8301 - val_loss: 0.9238 - val_mean_squared_error: 0.9238\n",
      "Epoch 1653/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8299 - mean_squared_error: 0.8299 - val_loss: 0.9172 - val_mean_squared_error: 0.9172\n",
      "Epoch 1654/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8296 - mean_squared_error: 0.8296 - val_loss: 0.9146 - val_mean_squared_error: 0.9146\n",
      "Epoch 1655/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8298 - mean_squared_error: 0.8298 - val_loss: 0.9229 - val_mean_squared_error: 0.9229\n",
      "Epoch 1656/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8295 - mean_squared_error: 0.8295 - val_loss: 0.9193 - val_mean_squared_error: 0.9193\n",
      "Epoch 1657/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8304 - mean_squared_error: 0.8304 - val_loss: 0.9241 - val_mean_squared_error: 0.9241\n",
      "Epoch 1658/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8291 - mean_squared_error: 0.8291 - val_loss: 0.9190 - val_mean_squared_error: 0.9190\n",
      "Epoch 1659/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8293 - mean_squared_error: 0.8293 - val_loss: 0.9143 - val_mean_squared_error: 0.9143\n",
      "Epoch 1660/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8298 - mean_squared_error: 0.8298 - val_loss: 0.9120 - val_mean_squared_error: 0.9120\n",
      "Epoch 1661/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8298 - mean_squared_error: 0.8298 - val_loss: 0.9152 - val_mean_squared_error: 0.9152\n",
      "Epoch 1662/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8325 - mean_squared_error: 0.8325 - val_loss: 0.9299 - val_mean_squared_error: 0.9299\n",
      "Epoch 1663/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8294 - mean_squared_error: 0.8294 - val_loss: 0.9165 - val_mean_squared_error: 0.9165\n",
      "Epoch 1664/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8299 - mean_squared_error: 0.8299 - val_loss: 0.9135 - val_mean_squared_error: 0.9135\n",
      "Epoch 1665/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8307 - mean_squared_error: 0.8307 - val_loss: 0.9211 - val_mean_squared_error: 0.9211\n",
      "Epoch 1666/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8292 - mean_squared_error: 0.8292 - val_loss: 0.9181 - val_mean_squared_error: 0.9181\n",
      "Epoch 1667/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8322 - mean_squared_error: 0.8322 - val_loss: 0.9094 - val_mean_squared_error: 0.9094\n",
      "Epoch 1668/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8298 - mean_squared_error: 0.8298 - val_loss: 0.9188 - val_mean_squared_error: 0.9188\n",
      "Epoch 1669/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8294 - mean_squared_error: 0.8294 - val_loss: 0.9205 - val_mean_squared_error: 0.9205\n",
      "Epoch 1670/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8295 - mean_squared_error: 0.8295 - val_loss: 0.9196 - val_mean_squared_error: 0.9196\n",
      "Epoch 1671/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8292 - mean_squared_error: 0.8292 - val_loss: 0.9191 - val_mean_squared_error: 0.9191\n",
      "Epoch 1672/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8293 - mean_squared_error: 0.8293 - val_loss: 0.9172 - val_mean_squared_error: 0.9172\n",
      "Epoch 1673/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8292 - mean_squared_error: 0.8292 - val_loss: 0.9155 - val_mean_squared_error: 0.9155\n",
      "Epoch 1674/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8298 - mean_squared_error: 0.8298 - val_loss: 0.9163 - val_mean_squared_error: 0.9163\n",
      "Epoch 1675/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8290 - mean_squared_error: 0.8290 - val_loss: 0.9152 - val_mean_squared_error: 0.9152\n",
      "Epoch 1676/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8292 - mean_squared_error: 0.8292 - val_loss: 0.9164 - val_mean_squared_error: 0.9164\n",
      "Epoch 1677/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8300 - mean_squared_error: 0.8300 - val_loss: 0.9223 - val_mean_squared_error: 0.9223\n",
      "Epoch 1678/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8292 - mean_squared_error: 0.8292 - val_loss: 0.9199 - val_mean_squared_error: 0.9199\n",
      "Epoch 1679/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8290 - mean_squared_error: 0.8290 - val_loss: 0.9166 - val_mean_squared_error: 0.9166\n",
      "Epoch 1680/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8292 - mean_squared_error: 0.8292 - val_loss: 0.9132 - val_mean_squared_error: 0.9132\n",
      "Epoch 1681/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8292 - mean_squared_error: 0.8292 - val_loss: 0.9168 - val_mean_squared_error: 0.9168\n",
      "Epoch 1682/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8290 - mean_squared_error: 0.8290 - val_loss: 0.9207 - val_mean_squared_error: 0.9207\n",
      "Epoch 1683/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8297 - mean_squared_error: 0.8297 - val_loss: 0.9221 - val_mean_squared_error: 0.9221\n",
      "Epoch 1684/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8292 - mean_squared_error: 0.8292 - val_loss: 0.9153 - val_mean_squared_error: 0.9153\n",
      "Epoch 1685/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8305 - mean_squared_error: 0.8305 - val_loss: 0.9082 - val_mean_squared_error: 0.9082\n",
      "Epoch 1686/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8290 - mean_squared_error: 0.8290 - val_loss: 0.9175 - val_mean_squared_error: 0.9175\n",
      "Epoch 1687/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8291 - mean_squared_error: 0.8291 - val_loss: 0.9210 - val_mean_squared_error: 0.9210\n",
      "Epoch 1688/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8294 - mean_squared_error: 0.8294 - val_loss: 0.9164 - val_mean_squared_error: 0.9164\n",
      "Epoch 1689/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.8291 - mean_squared_error: 0.8291 - val_loss: 0.9143 - val_mean_squared_error: 0.9143\n",
      "Epoch 1690/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8289 - mean_squared_error: 0.8289 - val_loss: 0.9147 - val_mean_squared_error: 0.9147\n",
      "Epoch 1691/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8294 - mean_squared_error: 0.8294 - val_loss: 0.9158 - val_mean_squared_error: 0.9158\n",
      "Epoch 1692/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8294 - mean_squared_error: 0.8294 - val_loss: 0.9098 - val_mean_squared_error: 0.9098\n",
      "Epoch 1693/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8289 - mean_squared_error: 0.8289 - val_loss: 0.9133 - val_mean_squared_error: 0.9133\n",
      "Epoch 1694/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8289 - mean_squared_error: 0.8289 - val_loss: 0.9165 - val_mean_squared_error: 0.9165\n",
      "Epoch 1695/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8292 - mean_squared_error: 0.8292 - val_loss: 0.9186 - val_mean_squared_error: 0.9186\n",
      "Epoch 1696/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8297 - mean_squared_error: 0.8297 - val_loss: 0.9143 - val_mean_squared_error: 0.9143\n",
      "Epoch 1697/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8295 - mean_squared_error: 0.8295 - val_loss: 0.9144 - val_mean_squared_error: 0.9144\n",
      "Epoch 1698/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8287 - mean_squared_error: 0.8287 - val_loss: 0.9208 - val_mean_squared_error: 0.9208\n",
      "Epoch 1699/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8289 - mean_squared_error: 0.8289 - val_loss: 0.9224 - val_mean_squared_error: 0.9224\n",
      "Epoch 1700/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8287 - mean_squared_error: 0.8287 - val_loss: 0.9164 - val_mean_squared_error: 0.9164\n",
      "Epoch 1701/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8290 - mean_squared_error: 0.8290 - val_loss: 0.9119 - val_mean_squared_error: 0.9119\n",
      "Epoch 1702/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8288 - mean_squared_error: 0.8288 - val_loss: 0.9156 - val_mean_squared_error: 0.9156\n",
      "Epoch 1703/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8286 - mean_squared_error: 0.8286 - val_loss: 0.9143 - val_mean_squared_error: 0.9143\n",
      "Epoch 1704/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8286 - mean_squared_error: 0.8286 - val_loss: 0.9139 - val_mean_squared_error: 0.9139\n",
      "Epoch 1705/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8296 - mean_squared_error: 0.8296 - val_loss: 0.9199 - val_mean_squared_error: 0.9199\n",
      "Epoch 1706/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8313 - mean_squared_error: 0.8313 - val_loss: 0.9054 - val_mean_squared_error: 0.9054\n",
      "Epoch 1707/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8298 - mean_squared_error: 0.8298 - val_loss: 0.9149 - val_mean_squared_error: 0.9149\n",
      "Epoch 1708/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8286 - mean_squared_error: 0.8286 - val_loss: 0.9163 - val_mean_squared_error: 0.9163\n",
      "Epoch 1709/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8290 - mean_squared_error: 0.8290 - val_loss: 0.9182 - val_mean_squared_error: 0.9182\n",
      "Epoch 1710/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8285 - mean_squared_error: 0.8285 - val_loss: 0.9158 - val_mean_squared_error: 0.9158\n",
      "Epoch 1711/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8287 - mean_squared_error: 0.8287 - val_loss: 0.9143 - val_mean_squared_error: 0.9143\n",
      "Epoch 1712/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8289 - mean_squared_error: 0.8289 - val_loss: 0.9126 - val_mean_squared_error: 0.9126\n",
      "Epoch 1713/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8284 - mean_squared_error: 0.8284 - val_loss: 0.9124 - val_mean_squared_error: 0.9124\n",
      "Epoch 1714/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8286 - mean_squared_error: 0.8286 - val_loss: 0.9133 - val_mean_squared_error: 0.9133\n",
      "Epoch 1715/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8286 - mean_squared_error: 0.8286 - val_loss: 0.9167 - val_mean_squared_error: 0.9167\n",
      "Epoch 1716/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8287 - mean_squared_error: 0.8287 - val_loss: 0.9192 - val_mean_squared_error: 0.9192\n",
      "Epoch 1717/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8284 - mean_squared_error: 0.8284 - val_loss: 0.9138 - val_mean_squared_error: 0.9138\n",
      "Epoch 1718/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8285 - mean_squared_error: 0.8285 - val_loss: 0.9134 - val_mean_squared_error: 0.9134\n",
      "Epoch 1719/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8301 - mean_squared_error: 0.8301 - val_loss: 0.9198 - val_mean_squared_error: 0.9198\n",
      "Epoch 1720/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8281 - mean_squared_error: 0.8281 - val_loss: 0.9120 - val_mean_squared_error: 0.9120\n",
      "Epoch 1721/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8284 - mean_squared_error: 0.8284 - val_loss: 0.9084 - val_mean_squared_error: 0.9084\n",
      "Epoch 1722/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8288 - mean_squared_error: 0.8288 - val_loss: 0.9071 - val_mean_squared_error: 0.9071\n",
      "Epoch 1723/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8286 - mean_squared_error: 0.8286 - val_loss: 0.9106 - val_mean_squared_error: 0.9106\n",
      "Epoch 1724/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8293 - mean_squared_error: 0.8293 - val_loss: 0.9155 - val_mean_squared_error: 0.9155\n",
      "Epoch 1725/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8283 - mean_squared_error: 0.8283 - val_loss: 0.9138 - val_mean_squared_error: 0.9138\n",
      "Epoch 1726/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8291 - mean_squared_error: 0.8291 - val_loss: 0.9111 - val_mean_squared_error: 0.9111\n",
      "Epoch 1727/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8282 - mean_squared_error: 0.8282 - val_loss: 0.9149 - val_mean_squared_error: 0.9149\n",
      "Epoch 1728/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8282 - mean_squared_error: 0.8282 - val_loss: 0.9160 - val_mean_squared_error: 0.9160\n",
      "Epoch 1729/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8289 - mean_squared_error: 0.8289 - val_loss: 0.9113 - val_mean_squared_error: 0.9113\n",
      "Epoch 1730/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8286 - mean_squared_error: 0.8286 - val_loss: 0.9124 - val_mean_squared_error: 0.9124\n",
      "Epoch 1731/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8287 - mean_squared_error: 0.8287 - val_loss: 0.9176 - val_mean_squared_error: 0.9176\n",
      "Epoch 1732/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8288 - mean_squared_error: 0.8288 - val_loss: 0.9153 - val_mean_squared_error: 0.9153\n",
      "Epoch 1733/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8283 - mean_squared_error: 0.8283 - val_loss: 0.9169 - val_mean_squared_error: 0.9169\n",
      "Epoch 1734/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8286 - mean_squared_error: 0.8286 - val_loss: 0.9110 - val_mean_squared_error: 0.9110\n",
      "Epoch 1735/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8283 - mean_squared_error: 0.8283 - val_loss: 0.9111 - val_mean_squared_error: 0.9111\n",
      "Epoch 1736/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8285 - mean_squared_error: 0.8285 - val_loss: 0.9127 - val_mean_squared_error: 0.9127\n",
      "Epoch 1737/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8284 - mean_squared_error: 0.8284 - val_loss: 0.9091 - val_mean_squared_error: 0.9091\n",
      "Epoch 1738/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8282 - mean_squared_error: 0.8282 - val_loss: 0.9149 - val_mean_squared_error: 0.9149\n",
      "Epoch 1739/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8288 - mean_squared_error: 0.8288 - val_loss: 0.9195 - val_mean_squared_error: 0.9195\n",
      "Epoch 1740/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8285 - mean_squared_error: 0.8285 - val_loss: 0.9138 - val_mean_squared_error: 0.9138\n",
      "Epoch 1741/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8280 - mean_squared_error: 0.8280 - val_loss: 0.9128 - val_mean_squared_error: 0.9128\n",
      "Epoch 1742/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8282 - mean_squared_error: 0.8282 - val_loss: 0.9087 - val_mean_squared_error: 0.9087\n",
      "Epoch 1743/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8283 - mean_squared_error: 0.8283 - val_loss: 0.9116 - val_mean_squared_error: 0.9116\n",
      "Epoch 1744/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8283 - mean_squared_error: 0.8283 - val_loss: 0.9113 - val_mean_squared_error: 0.9113\n",
      "Epoch 1745/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8282 - mean_squared_error: 0.8282 - val_loss: 0.9099 - val_mean_squared_error: 0.9099\n",
      "Epoch 1746/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8281 - mean_squared_error: 0.8281 - val_loss: 0.9134 - val_mean_squared_error: 0.9134\n",
      "Epoch 1747/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8280 - mean_squared_error: 0.8280 - val_loss: 0.9116 - val_mean_squared_error: 0.9116\n",
      "Epoch 1748/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8285 - mean_squared_error: 0.8285 - val_loss: 0.9085 - val_mean_squared_error: 0.9085\n",
      "Epoch 1749/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8278 - mean_squared_error: 0.8278 - val_loss: 0.9123 - val_mean_squared_error: 0.9123\n",
      "Epoch 1750/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8280 - mean_squared_error: 0.8280 - val_loss: 0.9121 - val_mean_squared_error: 0.9121\n",
      "Epoch 1751/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8285 - mean_squared_error: 0.8285 - val_loss: 0.9171 - val_mean_squared_error: 0.9171\n",
      "Epoch 1752/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8284 - mean_squared_error: 0.8284 - val_loss: 0.9096 - val_mean_squared_error: 0.9096\n",
      "Epoch 1753/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8281 - mean_squared_error: 0.8281 - val_loss: 0.9129 - val_mean_squared_error: 0.9129\n",
      "Epoch 1754/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8281 - mean_squared_error: 0.8281 - val_loss: 0.9108 - val_mean_squared_error: 0.9108\n",
      "Epoch 1755/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8282 - mean_squared_error: 0.8282 - val_loss: 0.9110 - val_mean_squared_error: 0.9110\n",
      "Epoch 1756/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8319 - mean_squared_error: 0.8319 - val_loss: 0.9208 - val_mean_squared_error: 0.9208\n",
      "Epoch 1757/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8275 - mean_squared_error: 0.8275 - val_loss: 0.9100 - val_mean_squared_error: 0.9100\n",
      "Epoch 1758/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8277 - mean_squared_error: 0.8277 - val_loss: 0.9089 - val_mean_squared_error: 0.9089\n",
      "Epoch 1759/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8278 - mean_squared_error: 0.8278 - val_loss: 0.9079 - val_mean_squared_error: 0.9079\n",
      "Epoch 1760/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8289 - mean_squared_error: 0.8289 - val_loss: 0.9132 - val_mean_squared_error: 0.9132\n",
      "Epoch 1761/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8279 - mean_squared_error: 0.8279 - val_loss: 0.9098 - val_mean_squared_error: 0.9098\n",
      "Epoch 1762/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8277 - mean_squared_error: 0.8277 - val_loss: 0.9092 - val_mean_squared_error: 0.9092\n",
      "Epoch 1763/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8282 - mean_squared_error: 0.8282 - val_loss: 0.9077 - val_mean_squared_error: 0.9077\n",
      "Epoch 1764/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8276 - mean_squared_error: 0.8276 - val_loss: 0.9117 - val_mean_squared_error: 0.9117\n",
      "Epoch 1765/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8289 - mean_squared_error: 0.8289 - val_loss: 0.9204 - val_mean_squared_error: 0.9204\n",
      "Epoch 1766/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8278 - mean_squared_error: 0.8278 - val_loss: 0.9138 - val_mean_squared_error: 0.9138\n",
      "Epoch 1767/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8278 - mean_squared_error: 0.8278 - val_loss: 0.9033 - val_mean_squared_error: 0.9033\n",
      "Epoch 1768/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8277 - mean_squared_error: 0.8277 - val_loss: 0.9065 - val_mean_squared_error: 0.9065\n",
      "Epoch 1769/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8279 - mean_squared_error: 0.8279 - val_loss: 0.9108 - val_mean_squared_error: 0.9108\n",
      "Epoch 1770/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8280 - mean_squared_error: 0.8280 - val_loss: 0.9062 - val_mean_squared_error: 0.9062\n",
      "Epoch 1771/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8281 - mean_squared_error: 0.8281 - val_loss: 0.9080 - val_mean_squared_error: 0.9080\n",
      "Epoch 1772/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8294 - mean_squared_error: 0.8294 - val_loss: 0.9183 - val_mean_squared_error: 0.9183\n",
      "Epoch 1773/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8280 - mean_squared_error: 0.8280 - val_loss: 0.9078 - val_mean_squared_error: 0.9078\n",
      "Epoch 1774/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8287 - mean_squared_error: 0.8287 - val_loss: 0.9055 - val_mean_squared_error: 0.9055\n",
      "Epoch 1775/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8275 - mean_squared_error: 0.8275 - val_loss: 0.9119 - val_mean_squared_error: 0.9119\n",
      "Epoch 1776/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8276 - mean_squared_error: 0.8276 - val_loss: 0.9150 - val_mean_squared_error: 0.9150\n",
      "Epoch 1777/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8284 - mean_squared_error: 0.8284 - val_loss: 0.9087 - val_mean_squared_error: 0.9087\n",
      "Epoch 1778/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8276 - mean_squared_error: 0.8276 - val_loss: 0.9126 - val_mean_squared_error: 0.9126\n",
      "Epoch 1779/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8285 - mean_squared_error: 0.8285 - val_loss: 0.9093 - val_mean_squared_error: 0.9093\n",
      "Epoch 1780/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8284 - mean_squared_error: 0.8284 - val_loss: 0.9067 - val_mean_squared_error: 0.9067\n",
      "Epoch 1781/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8274 - mean_squared_error: 0.8274 - val_loss: 0.9140 - val_mean_squared_error: 0.9140\n",
      "Epoch 1782/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8274 - mean_squared_error: 0.8274 - val_loss: 0.9118 - val_mean_squared_error: 0.9118\n",
      "Epoch 1783/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8277 - mean_squared_error: 0.8277 - val_loss: 0.9135 - val_mean_squared_error: 0.9135\n",
      "Epoch 1784/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8274 - mean_squared_error: 0.8274 - val_loss: 0.9080 - val_mean_squared_error: 0.9080\n",
      "Epoch 1785/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8278 - mean_squared_error: 0.8278 - val_loss: 0.9088 - val_mean_squared_error: 0.9088\n",
      "Epoch 1786/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8281 - mean_squared_error: 0.8281 - val_loss: 0.9049 - val_mean_squared_error: 0.9049\n",
      "Epoch 1787/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.8278 - mean_squared_error: 0.8278 - val_loss: 0.9135 - val_mean_squared_error: 0.9135\n",
      "Epoch 1788/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8276 - mean_squared_error: 0.8276 - val_loss: 0.9086 - val_mean_squared_error: 0.9086\n",
      "Epoch 1789/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8276 - mean_squared_error: 0.8276 - val_loss: 0.9097 - val_mean_squared_error: 0.9097\n",
      "Epoch 1790/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8288 - mean_squared_error: 0.8288 - val_loss: 0.9166 - val_mean_squared_error: 0.9166\n",
      "Epoch 1791/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8295 - mean_squared_error: 0.8295 - val_loss: 0.9047 - val_mean_squared_error: 0.9047\n",
      "Epoch 1792/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8270 - mean_squared_error: 0.8270 - val_loss: 0.9086 - val_mean_squared_error: 0.9086\n",
      "Epoch 1793/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8283 - mean_squared_error: 0.8283 - val_loss: 0.9167 - val_mean_squared_error: 0.9167\n",
      "Epoch 1794/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8270 - mean_squared_error: 0.8270 - val_loss: 0.9113 - val_mean_squared_error: 0.9113\n",
      "Epoch 1795/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8269 - mean_squared_error: 0.8269 - val_loss: 0.9079 - val_mean_squared_error: 0.9079\n",
      "Epoch 1796/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8276 - mean_squared_error: 0.8276 - val_loss: 0.9066 - val_mean_squared_error: 0.9066\n",
      "Epoch 1797/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8285 - mean_squared_error: 0.8285 - val_loss: 0.9124 - val_mean_squared_error: 0.9124\n",
      "Epoch 1798/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8275 - mean_squared_error: 0.8275 - val_loss: 0.9120 - val_mean_squared_error: 0.9120\n",
      "Epoch 1799/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8286 - mean_squared_error: 0.8286 - val_loss: 0.9051 - val_mean_squared_error: 0.9051\n",
      "Epoch 1800/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8271 - mean_squared_error: 0.8271 - val_loss: 0.9099 - val_mean_squared_error: 0.9099\n",
      "Epoch 1801/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8272 - mean_squared_error: 0.8272 - val_loss: 0.9112 - val_mean_squared_error: 0.9112\n",
      "Epoch 1802/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8270 - mean_squared_error: 0.8270 - val_loss: 0.9127 - val_mean_squared_error: 0.9127\n",
      "Epoch 1803/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8273 - mean_squared_error: 0.8273 - val_loss: 0.9071 - val_mean_squared_error: 0.9071\n",
      "Epoch 1804/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8270 - mean_squared_error: 0.8270 - val_loss: 0.9070 - val_mean_squared_error: 0.9070\n",
      "Epoch 1805/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8268 - mean_squared_error: 0.8268 - val_loss: 0.9090 - val_mean_squared_error: 0.9090\n",
      "Epoch 1806/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8269 - mean_squared_error: 0.8269 - val_loss: 0.9097 - val_mean_squared_error: 0.9097\n",
      "Epoch 1807/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8271 - mean_squared_error: 0.8271 - val_loss: 0.9102 - val_mean_squared_error: 0.9102\n",
      "Epoch 1808/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8271 - mean_squared_error: 0.8271 - val_loss: 0.9085 - val_mean_squared_error: 0.9085\n",
      "Epoch 1809/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8279 - mean_squared_error: 0.8279 - val_loss: 0.9067 - val_mean_squared_error: 0.9067\n",
      "Epoch 1810/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8266 - mean_squared_error: 0.8266 - val_loss: 0.9105 - val_mean_squared_error: 0.9105\n",
      "Epoch 1811/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8279 - mean_squared_error: 0.8279 - val_loss: 0.9162 - val_mean_squared_error: 0.9162\n",
      "Epoch 1812/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8272 - mean_squared_error: 0.8272 - val_loss: 0.9076 - val_mean_squared_error: 0.9076\n",
      "Epoch 1813/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8271 - mean_squared_error: 0.8271 - val_loss: 0.9048 - val_mean_squared_error: 0.9048\n",
      "Epoch 1814/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8275 - mean_squared_error: 0.8275 - val_loss: 0.9093 - val_mean_squared_error: 0.9093\n",
      "Epoch 1815/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8274 - mean_squared_error: 0.8274 - val_loss: 0.9062 - val_mean_squared_error: 0.9062\n",
      "Epoch 1816/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8269 - mean_squared_error: 0.8269 - val_loss: 0.9123 - val_mean_squared_error: 0.9123\n",
      "Epoch 1817/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8272 - mean_squared_error: 0.8272 - val_loss: 0.9092 - val_mean_squared_error: 0.9092\n",
      "Epoch 1818/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8267 - mean_squared_error: 0.8267 - val_loss: 0.9084 - val_mean_squared_error: 0.9084\n",
      "Epoch 1819/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8271 - mean_squared_error: 0.8271 - val_loss: 0.9088 - val_mean_squared_error: 0.9088\n",
      "Epoch 1820/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8267 - mean_squared_error: 0.8267 - val_loss: 0.9122 - val_mean_squared_error: 0.9122\n",
      "Epoch 1821/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8267 - mean_squared_error: 0.8267 - val_loss: 0.9078 - val_mean_squared_error: 0.9078\n",
      "Epoch 1822/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8274 - mean_squared_error: 0.8274 - val_loss: 0.9095 - val_mean_squared_error: 0.9095\n",
      "Epoch 1823/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8269 - mean_squared_error: 0.8269 - val_loss: 0.9101 - val_mean_squared_error: 0.9101\n",
      "Epoch 1824/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8272 - mean_squared_error: 0.8272 - val_loss: 0.9046 - val_mean_squared_error: 0.9046\n",
      "Epoch 1825/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8267 - mean_squared_error: 0.8267 - val_loss: 0.9098 - val_mean_squared_error: 0.9098\n",
      "Epoch 1826/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8269 - mean_squared_error: 0.8269 - val_loss: 0.9064 - val_mean_squared_error: 0.9064\n",
      "Epoch 1827/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8265 - mean_squared_error: 0.8265 - val_loss: 0.9066 - val_mean_squared_error: 0.9066\n",
      "Epoch 1828/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8268 - mean_squared_error: 0.8268 - val_loss: 0.9068 - val_mean_squared_error: 0.9068\n",
      "Epoch 1829/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8269 - mean_squared_error: 0.8269 - val_loss: 0.9087 - val_mean_squared_error: 0.9087\n",
      "Epoch 1830/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8264 - mean_squared_error: 0.8264 - val_loss: 0.9143 - val_mean_squared_error: 0.9143\n",
      "Epoch 1831/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8265 - mean_squared_error: 0.8265 - val_loss: 0.9112 - val_mean_squared_error: 0.9112\n",
      "Epoch 1832/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8269 - mean_squared_error: 0.8269 - val_loss: 0.9111 - val_mean_squared_error: 0.9111\n",
      "Epoch 1833/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8279 - mean_squared_error: 0.8279 - val_loss: 0.9038 - val_mean_squared_error: 0.9038\n",
      "Epoch 1834/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8276 - mean_squared_error: 0.8276 - val_loss: 0.9104 - val_mean_squared_error: 0.9104\n",
      "Epoch 1835/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8272 - mean_squared_error: 0.8272 - val_loss: 0.9048 - val_mean_squared_error: 0.9048\n",
      "Epoch 1836/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8269 - mean_squared_error: 0.8269 - val_loss: 0.9101 - val_mean_squared_error: 0.9101\n",
      "Epoch 1837/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8289 - mean_squared_error: 0.8289 - val_loss: 0.9044 - val_mean_squared_error: 0.9044\n",
      "Epoch 1838/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8272 - mean_squared_error: 0.8272 - val_loss: 0.9117 - val_mean_squared_error: 0.9117\n",
      "Epoch 1839/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8266 - mean_squared_error: 0.8266 - val_loss: 0.9094 - val_mean_squared_error: 0.9094\n",
      "Epoch 1840/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8261 - mean_squared_error: 0.8261 - val_loss: 0.9073 - val_mean_squared_error: 0.9073\n",
      "Epoch 1841/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8263 - mean_squared_error: 0.8263 - val_loss: 0.9065 - val_mean_squared_error: 0.9065\n",
      "Epoch 1842/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8263 - mean_squared_error: 0.8263 - val_loss: 0.9042 - val_mean_squared_error: 0.9042\n",
      "Epoch 1843/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8264 - mean_squared_error: 0.8264 - val_loss: 0.9084 - val_mean_squared_error: 0.9084\n",
      "Epoch 1844/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8262 - mean_squared_error: 0.8262 - val_loss: 0.9089 - val_mean_squared_error: 0.9089\n",
      "Epoch 1845/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8262 - mean_squared_error: 0.8262 - val_loss: 0.9059 - val_mean_squared_error: 0.9059\n",
      "Epoch 1846/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8265 - mean_squared_error: 0.8265 - val_loss: 0.9070 - val_mean_squared_error: 0.9070\n",
      "Epoch 1847/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8270 - mean_squared_error: 0.8270 - val_loss: 0.9094 - val_mean_squared_error: 0.9094\n",
      "Epoch 1848/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8259 - mean_squared_error: 0.8259 - val_loss: 0.9079 - val_mean_squared_error: 0.9079\n",
      "Epoch 1849/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8264 - mean_squared_error: 0.8264 - val_loss: 0.9078 - val_mean_squared_error: 0.9078\n",
      "Epoch 1850/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8262 - mean_squared_error: 0.8262 - val_loss: 0.9050 - val_mean_squared_error: 0.9050\n",
      "Epoch 1851/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8262 - mean_squared_error: 0.8262 - val_loss: 0.9048 - val_mean_squared_error: 0.9048\n",
      "Epoch 1852/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8266 - mean_squared_error: 0.8266 - val_loss: 0.9089 - val_mean_squared_error: 0.9089\n",
      "Epoch 1853/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8266 - mean_squared_error: 0.8266 - val_loss: 0.9098 - val_mean_squared_error: 0.9098\n",
      "Epoch 1854/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8260 - mean_squared_error: 0.8260 - val_loss: 0.9044 - val_mean_squared_error: 0.9044\n",
      "Epoch 1855/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8276 - mean_squared_error: 0.8276 - val_loss: 0.9030 - val_mean_squared_error: 0.9030\n",
      "Epoch 1856/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8265 - mean_squared_error: 0.8265 - val_loss: 0.9086 - val_mean_squared_error: 0.9086\n",
      "Epoch 1857/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.8265 - mean_squared_error: 0.8265 - val_loss: 0.9092 - val_mean_squared_error: 0.9092\n",
      "Epoch 1858/2000\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.8271 - mean_squared_error: 0.8271 - val_loss: 0.9040 - val_mean_squared_error: 0.9040\n",
      "Epoch 1859/2000\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.8262 - mean_squared_error: 0.8262 - val_loss: 0.9064 - val_mean_squared_error: 0.9064\n",
      "Epoch 1860/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8257 - mean_squared_error: 0.8257 - val_loss: 0.9085 - val_mean_squared_error: 0.9085\n",
      "Epoch 1861/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8259 - mean_squared_error: 0.8259 - val_loss: 0.9095 - val_mean_squared_error: 0.9095\n",
      "Epoch 1862/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.8261 - mean_squared_error: 0.8261 - val_loss: 0.9125 - val_mean_squared_error: 0.9125\n",
      "Epoch 1863/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8261 - mean_squared_error: 0.8261 - val_loss: 0.9069 - val_mean_squared_error: 0.9069\n",
      "Epoch 1864/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8262 - mean_squared_error: 0.8262 - val_loss: 0.9049 - val_mean_squared_error: 0.9049\n",
      "Epoch 1865/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8259 - mean_squared_error: 0.8259 - val_loss: 0.9044 - val_mean_squared_error: 0.9044\n",
      "Epoch 1866/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8257 - mean_squared_error: 0.8257 - val_loss: 0.9078 - val_mean_squared_error: 0.9078\n",
      "Epoch 1867/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8261 - mean_squared_error: 0.8261 - val_loss: 0.9120 - val_mean_squared_error: 0.9120\n",
      "Epoch 1868/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8261 - mean_squared_error: 0.8261 - val_loss: 0.9079 - val_mean_squared_error: 0.9079\n",
      "Epoch 1869/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8258 - mean_squared_error: 0.8258 - val_loss: 0.9062 - val_mean_squared_error: 0.9062\n",
      "Epoch 1870/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8259 - mean_squared_error: 0.8259 - val_loss: 0.9056 - val_mean_squared_error: 0.9056\n",
      "Epoch 1871/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8268 - mean_squared_error: 0.8268 - val_loss: 0.9053 - val_mean_squared_error: 0.9053\n",
      "Epoch 1872/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8270 - mean_squared_error: 0.8270 - val_loss: 0.9049 - val_mean_squared_error: 0.9049\n",
      "Epoch 1873/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8258 - mean_squared_error: 0.8258 - val_loss: 0.9013 - val_mean_squared_error: 0.9013\n",
      "Epoch 1874/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8262 - mean_squared_error: 0.8262 - val_loss: 0.9041 - val_mean_squared_error: 0.9041\n",
      "Epoch 1875/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8258 - mean_squared_error: 0.8258 - val_loss: 0.9074 - val_mean_squared_error: 0.9074\n",
      "Epoch 1876/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8259 - mean_squared_error: 0.8259 - val_loss: 0.9064 - val_mean_squared_error: 0.9064\n",
      "Epoch 1877/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8261 - mean_squared_error: 0.8261 - val_loss: 0.9074 - val_mean_squared_error: 0.9074\n",
      "Epoch 1878/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8256 - mean_squared_error: 0.8256 - val_loss: 0.9106 - val_mean_squared_error: 0.9106\n",
      "Epoch 1879/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8259 - mean_squared_error: 0.8259 - val_loss: 0.9079 - val_mean_squared_error: 0.9079\n",
      "Epoch 1880/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8265 - mean_squared_error: 0.8265 - val_loss: 0.9018 - val_mean_squared_error: 0.9018\n",
      "Epoch 1881/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8259 - mean_squared_error: 0.8259 - val_loss: 0.9057 - val_mean_squared_error: 0.9057\n",
      "Epoch 1882/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8267 - mean_squared_error: 0.8267 - val_loss: 0.9117 - val_mean_squared_error: 0.9117\n",
      "Epoch 1883/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8255 - mean_squared_error: 0.8255 - val_loss: 0.9089 - val_mean_squared_error: 0.9089\n",
      "Epoch 1884/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8261 - mean_squared_error: 0.8261 - val_loss: 0.9031 - val_mean_squared_error: 0.9031\n",
      "Epoch 1885/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8257 - mean_squared_error: 0.8257 - val_loss: 0.9051 - val_mean_squared_error: 0.9051\n",
      "Epoch 1886/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8263 - mean_squared_error: 0.8263 - val_loss: 0.9030 - val_mean_squared_error: 0.9030\n",
      "Epoch 1887/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8265 - mean_squared_error: 0.8265 - val_loss: 0.9095 - val_mean_squared_error: 0.9095\n",
      "Epoch 1888/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8254 - mean_squared_error: 0.8254 - val_loss: 0.9073 - val_mean_squared_error: 0.9073\n",
      "Epoch 1889/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8255 - mean_squared_error: 0.8255 - val_loss: 0.9048 - val_mean_squared_error: 0.9048\n",
      "Epoch 1890/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8258 - mean_squared_error: 0.8258 - val_loss: 0.9082 - val_mean_squared_error: 0.9082\n",
      "Epoch 1891/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8254 - mean_squared_error: 0.8254 - val_loss: 0.9063 - val_mean_squared_error: 0.9063\n",
      "Epoch 1892/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8255 - mean_squared_error: 0.8255 - val_loss: 0.9043 - val_mean_squared_error: 0.9043\n",
      "Epoch 1893/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8258 - mean_squared_error: 0.8258 - val_loss: 0.9040 - val_mean_squared_error: 0.9040\n",
      "Epoch 1894/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8263 - mean_squared_error: 0.8263 - val_loss: 0.9123 - val_mean_squared_error: 0.9123\n",
      "Epoch 1895/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8256 - mean_squared_error: 0.8256 - val_loss: 0.9079 - val_mean_squared_error: 0.9079\n",
      "Epoch 1896/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8271 - mean_squared_error: 0.8271 - val_loss: 0.9096 - val_mean_squared_error: 0.9096\n",
      "Epoch 1897/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8263 - mean_squared_error: 0.8263 - val_loss: 0.9001 - val_mean_squared_error: 0.9001\n",
      "Epoch 1898/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8260 - mean_squared_error: 0.8260 - val_loss: 0.9046 - val_mean_squared_error: 0.9046\n",
      "Epoch 1899/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8271 - mean_squared_error: 0.8271 - val_loss: 0.9091 - val_mean_squared_error: 0.9091\n",
      "Epoch 1900/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8252 - mean_squared_error: 0.8252 - val_loss: 0.9043 - val_mean_squared_error: 0.9043\n",
      "Epoch 1901/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8254 - mean_squared_error: 0.8254 - val_loss: 0.9071 - val_mean_squared_error: 0.9071\n",
      "Epoch 1902/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8252 - mean_squared_error: 0.8252 - val_loss: 0.9044 - val_mean_squared_error: 0.9044\n",
      "Epoch 1903/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8255 - mean_squared_error: 0.8255 - val_loss: 0.9056 - val_mean_squared_error: 0.9056\n",
      "Epoch 1904/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8252 - mean_squared_error: 0.8252 - val_loss: 0.9025 - val_mean_squared_error: 0.9025\n",
      "Epoch 1905/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8250 - mean_squared_error: 0.8250 - val_loss: 0.9051 - val_mean_squared_error: 0.9051\n",
      "Epoch 1906/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8253 - mean_squared_error: 0.8253 - val_loss: 0.9091 - val_mean_squared_error: 0.9091\n",
      "Epoch 1907/2000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.8255 - mean_squared_error: 0.8255 - val_loss: 0.9062 - val_mean_squared_error: 0.9062\n",
      "Epoch 1908/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8252 - mean_squared_error: 0.8252 - val_loss: 0.9070 - val_mean_squared_error: 0.9070\n",
      "Epoch 1909/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8254 - mean_squared_error: 0.8254 - val_loss: 0.9070 - val_mean_squared_error: 0.9070\n",
      "Epoch 1910/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8253 - mean_squared_error: 0.8253 - val_loss: 0.9046 - val_mean_squared_error: 0.9046\n",
      "Epoch 1911/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8250 - mean_squared_error: 0.8250 - val_loss: 0.9026 - val_mean_squared_error: 0.9026\n",
      "Epoch 1912/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8252 - mean_squared_error: 0.8252 - val_loss: 0.9060 - val_mean_squared_error: 0.9060\n",
      "Epoch 1913/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8253 - mean_squared_error: 0.8253 - val_loss: 0.9042 - val_mean_squared_error: 0.9042\n",
      "Epoch 1914/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8260 - mean_squared_error: 0.8260 - val_loss: 0.9065 - val_mean_squared_error: 0.9065\n",
      "Epoch 1915/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8253 - mean_squared_error: 0.8253 - val_loss: 0.9042 - val_mean_squared_error: 0.9042\n",
      "Epoch 1916/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8255 - mean_squared_error: 0.8255 - val_loss: 0.9037 - val_mean_squared_error: 0.9037\n",
      "Epoch 1917/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8248 - mean_squared_error: 0.8248 - val_loss: 0.9073 - val_mean_squared_error: 0.9073\n",
      "Epoch 1918/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8251 - mean_squared_error: 0.8251 - val_loss: 0.9076 - val_mean_squared_error: 0.9076\n",
      "Epoch 1919/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8251 - mean_squared_error: 0.8251 - val_loss: 0.9060 - val_mean_squared_error: 0.9060\n",
      "Epoch 1920/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8256 - mean_squared_error: 0.8256 - val_loss: 0.9081 - val_mean_squared_error: 0.9081\n",
      "Epoch 1921/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8252 - mean_squared_error: 0.8252 - val_loss: 0.9063 - val_mean_squared_error: 0.9063\n",
      "Epoch 1922/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8249 - mean_squared_error: 0.8249 - val_loss: 0.9072 - val_mean_squared_error: 0.9072\n",
      "Epoch 1923/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8255 - mean_squared_error: 0.8255 - val_loss: 0.9014 - val_mean_squared_error: 0.9014\n",
      "Epoch 1924/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8253 - mean_squared_error: 0.8253 - val_loss: 0.9031 - val_mean_squared_error: 0.9031\n",
      "Epoch 1925/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8264 - mean_squared_error: 0.8264 - val_loss: 0.9073 - val_mean_squared_error: 0.9073\n",
      "Epoch 1926/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8254 - mean_squared_error: 0.8254 - val_loss: 0.9075 - val_mean_squared_error: 0.9075\n",
      "Epoch 1927/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8254 - mean_squared_error: 0.8254 - val_loss: 0.9048 - val_mean_squared_error: 0.9048\n",
      "Epoch 1928/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8248 - mean_squared_error: 0.8248 - val_loss: 0.9067 - val_mean_squared_error: 0.9067\n",
      "Epoch 1929/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8250 - mean_squared_error: 0.8250 - val_loss: 0.9075 - val_mean_squared_error: 0.9075\n",
      "Epoch 1930/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8251 - mean_squared_error: 0.8251 - val_loss: 0.9037 - val_mean_squared_error: 0.9037\n",
      "Epoch 1931/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8253 - mean_squared_error: 0.8253 - val_loss: 0.9065 - val_mean_squared_error: 0.9065\n",
      "Epoch 1932/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8251 - mean_squared_error: 0.8251 - val_loss: 0.9018 - val_mean_squared_error: 0.9018\n",
      "Epoch 1933/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8255 - mean_squared_error: 0.8255 - val_loss: 0.9079 - val_mean_squared_error: 0.9079\n",
      "Epoch 1934/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8250 - mean_squared_error: 0.8250 - val_loss: 0.9063 - val_mean_squared_error: 0.9063\n",
      "Epoch 1935/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8249 - mean_squared_error: 0.8249 - val_loss: 0.9028 - val_mean_squared_error: 0.9028\n",
      "Epoch 1936/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8250 - mean_squared_error: 0.8250 - val_loss: 0.9018 - val_mean_squared_error: 0.9018\n",
      "Epoch 1937/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8250 - mean_squared_error: 0.8250 - val_loss: 0.9071 - val_mean_squared_error: 0.9071\n",
      "Epoch 1938/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8252 - mean_squared_error: 0.8252 - val_loss: 0.9082 - val_mean_squared_error: 0.9082\n",
      "Epoch 1939/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8251 - mean_squared_error: 0.8251 - val_loss: 0.9072 - val_mean_squared_error: 0.9072\n",
      "Epoch 1940/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8247 - mean_squared_error: 0.8247 - val_loss: 0.9073 - val_mean_squared_error: 0.9073\n",
      "Epoch 1941/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8249 - mean_squared_error: 0.8249 - val_loss: 0.9028 - val_mean_squared_error: 0.9028\n",
      "Epoch 1942/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8246 - mean_squared_error: 0.8246 - val_loss: 0.9030 - val_mean_squared_error: 0.9030\n",
      "Epoch 1943/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8247 - mean_squared_error: 0.8247 - val_loss: 0.9043 - val_mean_squared_error: 0.9043\n",
      "Epoch 1944/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8246 - mean_squared_error: 0.8246 - val_loss: 0.9028 - val_mean_squared_error: 0.9028\n",
      "Epoch 1945/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8246 - mean_squared_error: 0.8246 - val_loss: 0.9041 - val_mean_squared_error: 0.9041\n",
      "Epoch 1946/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8250 - mean_squared_error: 0.8250 - val_loss: 0.9061 - val_mean_squared_error: 0.9061\n",
      "Epoch 1947/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8253 - mean_squared_error: 0.8253 - val_loss: 0.9015 - val_mean_squared_error: 0.9015\n",
      "Epoch 1948/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8245 - mean_squared_error: 0.8245 - val_loss: 0.9051 - val_mean_squared_error: 0.9051\n",
      "Epoch 1949/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8250 - mean_squared_error: 0.8250 - val_loss: 0.9054 - val_mean_squared_error: 0.9054\n",
      "Epoch 1950/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8250 - mean_squared_error: 0.8250 - val_loss: 0.9056 - val_mean_squared_error: 0.9056\n",
      "Epoch 1951/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8258 - mean_squared_error: 0.8258 - val_loss: 0.9021 - val_mean_squared_error: 0.9021\n",
      "Epoch 1952/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8246 - mean_squared_error: 0.8246 - val_loss: 0.9057 - val_mean_squared_error: 0.9057\n",
      "Epoch 1953/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8245 - mean_squared_error: 0.8245 - val_loss: 0.9069 - val_mean_squared_error: 0.9069\n",
      "Epoch 1954/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8245 - mean_squared_error: 0.8245 - val_loss: 0.9070 - val_mean_squared_error: 0.9070\n",
      "Epoch 1955/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8260 - mean_squared_error: 0.8260 - val_loss: 0.9007 - val_mean_squared_error: 0.9007\n",
      "Epoch 1956/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8245 - mean_squared_error: 0.8245 - val_loss: 0.9056 - val_mean_squared_error: 0.9056\n",
      "Epoch 1957/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8246 - mean_squared_error: 0.8246 - val_loss: 0.9053 - val_mean_squared_error: 0.9053\n",
      "Epoch 1958/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8243 - mean_squared_error: 0.8243 - val_loss: 0.9047 - val_mean_squared_error: 0.9047\n",
      "Epoch 1959/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8249 - mean_squared_error: 0.8249 - val_loss: 0.9036 - val_mean_squared_error: 0.9036\n",
      "Epoch 1960/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8256 - mean_squared_error: 0.8256 - val_loss: 0.9086 - val_mean_squared_error: 0.9086\n",
      "Epoch 1961/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8242 - mean_squared_error: 0.8242 - val_loss: 0.9059 - val_mean_squared_error: 0.9059\n",
      "Epoch 1962/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8242 - mean_squared_error: 0.8242 - val_loss: 0.9010 - val_mean_squared_error: 0.9010\n",
      "Epoch 1963/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8248 - mean_squared_error: 0.8248 - val_loss: 0.9036 - val_mean_squared_error: 0.9036\n",
      "Epoch 1964/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8242 - mean_squared_error: 0.8242 - val_loss: 0.9026 - val_mean_squared_error: 0.9026\n",
      "Epoch 1965/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8243 - mean_squared_error: 0.8243 - val_loss: 0.9068 - val_mean_squared_error: 0.9068\n",
      "Epoch 1966/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8243 - mean_squared_error: 0.8243 - val_loss: 0.9062 - val_mean_squared_error: 0.9062\n",
      "Epoch 1967/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8250 - mean_squared_error: 0.8250 - val_loss: 0.9005 - val_mean_squared_error: 0.9005\n",
      "Epoch 1968/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8246 - mean_squared_error: 0.8246 - val_loss: 0.9053 - val_mean_squared_error: 0.9053\n",
      "Epoch 1969/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8245 - mean_squared_error: 0.8245 - val_loss: 0.9042 - val_mean_squared_error: 0.9042\n",
      "Epoch 1970/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8240 - mean_squared_error: 0.8240 - val_loss: 0.9060 - val_mean_squared_error: 0.9060\n",
      "Epoch 1971/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8240 - mean_squared_error: 0.8240 - val_loss: 0.9053 - val_mean_squared_error: 0.9053\n",
      "Epoch 1972/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8243 - mean_squared_error: 0.8243 - val_loss: 0.9045 - val_mean_squared_error: 0.9045\n",
      "Epoch 1973/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8249 - mean_squared_error: 0.8249 - val_loss: 0.9026 - val_mean_squared_error: 0.9026\n",
      "Epoch 1974/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8247 - mean_squared_error: 0.8247 - val_loss: 0.9064 - val_mean_squared_error: 0.9064\n",
      "Epoch 1975/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8245 - mean_squared_error: 0.8245 - val_loss: 0.9074 - val_mean_squared_error: 0.9074\n",
      "Epoch 1976/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8257 - mean_squared_error: 0.8257 - val_loss: 0.9001 - val_mean_squared_error: 0.9001\n",
      "Epoch 1977/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8253 - mean_squared_error: 0.8253 - val_loss: 0.9077 - val_mean_squared_error: 0.9077\n",
      "Epoch 1978/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8255 - mean_squared_error: 0.8255 - val_loss: 0.9023 - val_mean_squared_error: 0.9023\n",
      "Epoch 1979/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8238 - mean_squared_error: 0.8238 - val_loss: 0.9042 - val_mean_squared_error: 0.9042\n",
      "Epoch 1980/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8243 - mean_squared_error: 0.8243 - val_loss: 0.9015 - val_mean_squared_error: 0.9015\n",
      "Epoch 1981/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8244 - mean_squared_error: 0.8244 - val_loss: 0.9040 - val_mean_squared_error: 0.9040\n",
      "Epoch 1982/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8240 - mean_squared_error: 0.8240 - val_loss: 0.9034 - val_mean_squared_error: 0.9034\n",
      "Epoch 1983/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8236 - mean_squared_error: 0.8236 - val_loss: 0.9064 - val_mean_squared_error: 0.9064\n",
      "Epoch 1984/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8245 - mean_squared_error: 0.8245 - val_loss: 0.9074 - val_mean_squared_error: 0.9074\n",
      "Epoch 1985/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8232 - mean_squared_error: 0.8232 - val_loss: 0.8997 - val_mean_squared_error: 0.8997\n",
      "Epoch 1986/2000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8241 - mean_squared_error: 0.8241 - val_loss: 0.9009 - val_mean_squared_error: 0.9009\n",
      "Epoch 1987/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8239 - mean_squared_error: 0.8239 - val_loss: 0.9032 - val_mean_squared_error: 0.9032\n",
      "Epoch 1988/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8239 - mean_squared_error: 0.8239 - val_loss: 0.9051 - val_mean_squared_error: 0.9051\n",
      "Epoch 1989/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8251 - mean_squared_error: 0.8251 - val_loss: 0.9054 - val_mean_squared_error: 0.9054\n",
      "Epoch 1990/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8238 - mean_squared_error: 0.8238 - val_loss: 0.9032 - val_mean_squared_error: 0.9032\n",
      "Epoch 1991/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8240 - mean_squared_error: 0.8240 - val_loss: 0.9038 - val_mean_squared_error: 0.9038\n",
      "Epoch 1992/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8239 - mean_squared_error: 0.8239 - val_loss: 0.9050 - val_mean_squared_error: 0.9050\n",
      "Epoch 1993/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8238 - mean_squared_error: 0.8238 - val_loss: 0.9026 - val_mean_squared_error: 0.9026\n",
      "Epoch 1994/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8237 - mean_squared_error: 0.8237 - val_loss: 0.9028 - val_mean_squared_error: 0.9028\n",
      "Epoch 1995/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8232 - mean_squared_error: 0.8232 - val_loss: 0.9013 - val_mean_squared_error: 0.9013\n",
      "Epoch 1996/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8233 - mean_squared_error: 0.8233 - val_loss: 0.9007 - val_mean_squared_error: 0.9007\n",
      "Epoch 1997/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8234 - mean_squared_error: 0.8234 - val_loss: 0.9036 - val_mean_squared_error: 0.9036\n",
      "Epoch 1998/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8237 - mean_squared_error: 0.8237 - val_loss: 0.9011 - val_mean_squared_error: 0.9011\n",
      "Epoch 1999/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8254 - mean_squared_error: 0.8254 - val_loss: 0.8973 - val_mean_squared_error: 0.8973\n",
      "Epoch 2000/2000\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8244 - mean_squared_error: 0.8244 - val_loss: 0.9077 - val_mean_squared_error: 0.9077\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluate the new model against the test set:\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7975 - mean_squared_error: 0.7975\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'loss': 0.7975153923034668, 'mean_squared_error': 0.7975153923034668}"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.00003\n",
    "epochs = 2000\n",
    "batch_size = 15\n",
    "l2_regularization = 1\n",
    "patience = 70\n",
    "\n",
    "# Specify the label\n",
    "label_name = \"BodyFat\"\n",
    "\n",
    "# Split the original training set into a reduced training set and a\n",
    "# validation set.\n",
    "validation_split = 0.2\n",
    "\n",
    "dnn_outputs = get_outputs_dnn()\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(\n",
    "    inputs,\n",
    "    dnn_outputs,\n",
    "    learning_rate, l2_regularization)\n",
    "\n",
    "# Train the model on the normalized training set. We're passing the entire\n",
    "# normalized training set, but the model will only use the features\n",
    "# defined in our inputs.\n",
    "epochs, mse, history = train_model(my_model, trainDF, epochs,\n",
    "                                   batch_size, label_name, patience, validation_split)\n",
    "plot_the_loss_curve(epochs, mse, history[\"val_mean_squared_error\"])\n",
    "\n",
    "# After building a model against the training set, test that model\n",
    "# against the test set.\n",
    "test_features = {name:np.array(value) for name, value in testDF.items()}\n",
    "test_label = test_bodyfat_normalized(np.array(test_features.pop(label_name))) # isolate the label\n",
    "\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size, return_dict=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "              Feature  Total Weight\n4       HipThighRatio          3.86\n1                 BMI          3.71\n7   ForearmBicepRatio          3.46\n11            Forearm          3.42\n13   StaticWaistRatio          3.31\n6   ForearmWristRatio          3.28\n0                 Age          3.04\n3     ChestWaistRatio          2.83\n5                Neck          2.71\n12              Wrist          2.70\n8                Knee          2.70\n9               Ankle          2.62\n10             Biceps          2.58\n2      ChestNeckRatio          2.23",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Total Weight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>HipThighRatio</td>\n      <td>3.86</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BMI</td>\n      <td>3.71</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ForearmBicepRatio</td>\n      <td>3.46</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Forearm</td>\n      <td>3.42</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>StaticWaistRatio</td>\n      <td>3.31</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ForearmWristRatio</td>\n      <td>3.28</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Age</td>\n      <td>3.04</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ChestWaistRatio</td>\n      <td>2.83</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Neck</td>\n      <td>2.71</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Wrist</td>\n      <td>2.70</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Knee</td>\n      <td>2.70</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Ankle</td>\n      <td>2.62</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Biceps</td>\n      <td>2.58</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ChestNeckRatio</td>\n      <td>2.23</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Output weights\n",
    "input_layer = my_model.layers[15]\n",
    "weights, biases = input_layer.get_weights()\n",
    "\n",
    "weights_table = pd.DataFrame(columns=['Feature', 'Total Weight'])\n",
    "\n",
    "for feature, i in zip(inputs, weights):\n",
    "    for j in i:\n",
    "        j = abs(j)\n",
    "        if feature not in weights_table['Feature'].values:\n",
    "            row = pd.DataFrame({'Feature': feature, 'Total Weight': j}, index=[0])\n",
    "            weights_table = pd.concat([weights_table, row], ignore_index=True)\n",
    "        else:\n",
    "            condition = weights_table['Feature'] == feature\n",
    "            index = weights_table.loc[condition, 'Total Weight'].index\n",
    "            weights_table.loc[condition, 'Total Weight'] = weights_table.loc[condition, 'Total Weight'][index] + j\n",
    "\n",
    "weights_table = weights_table.sort_values(by='Total Weight', ascending=False)\n",
    "weights_table\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "    Row Predicted Body Fat Correct Body Fat Difference +-\n0   164             [19.5]           [27.3]        [-7.8]\n1    66             [19.5]           [21.5]        [-2.0]\n2   180             [19.5]           [26.6]        [-7.1]\n3   148             [19.5]            [5.3]        [14.2]\n4     0             [19.5]           [12.3]         [7.2]\n5   120             [19.5]           [27.9]        [-8.4]\n6    49             [19.5]            [4.0]        [15.5]\n7   127             [19.5]           [17.4]         [2.1]\n8   219             [19.5]           [15.0]         [4.5]\n9   189             [19.5]           [24.4]        [-4.9]\n10  139             [19.5]           [20.4]        [-0.9]\n11   25             [19.5]            [3.7]        [15.8]\n12  108             [19.5]           [17.3]         [2.2]\n13  231             [19.5]           [16.1]         [3.4]\n14  119             [19.5]           [18.1]         [1.4]\n15  200             [19.5]           [12.2]         [7.3]\n16   46             [19.5]           [10.8]         [8.7]\n17   51             [19.5]            [6.6]        [12.9]\n18  216             [19.5]           [13.6]         [5.9]\n19   87             [19.5]           [23.1]        [-3.6]\n20  202             [19.5]           [28.7]        [-9.2]\n21   67             [19.5]           [13.8]         [5.7]\n22   97             [19.5]           [11.3]         [8.2]\n23  157             [19.5]           [10.0]         [9.5]\n24  190             [19.5]           [11.4]         [8.1]\n25  237             [19.5]           [27.3]        [-7.8]\n26   30             [19.5]           [11.9]         [7.6]\n27  109             [19.5]           [21.4]        [-1.9]\n28   26             [19.5]            [7.9]        [11.6]\n29  184             [19.5]           [17.5]         [2.0]\n30  161             [19.5]           [14.6]         [4.9]\n31  239             [19.5]           [29.9]       [-10.4]\n32   45             [19.5]           [13.9]         [5.6]\n33    4             [19.5]           [28.7]        [-9.2]\n34  106             [19.5]           [19.3]         [0.2]\n35  158             [19.5]           [12.5]         [7.0]\n36  178             [19.5]           [22.5]        [-3.0]\n37   61             [19.5]           [29.8]       [-10.3]\n38  168             [19.5]           [34.3]       [-14.8]\n39  238             [19.5]           [12.4]         [7.1]\n40  196             [19.5]           [22.0]        [-2.5]\n41   73             [19.5]           [13.5]         [6.0]\n42  191             [19.5]           [38.1]       [-18.6]\n43   59             [19.5]           [24.6]        [-5.1]\n44  205             [19.5]           [16.6]         [2.9]\n45  152             [19.5]           [10.1]         [9.4]\n46  230             [19.5]           [10.6]         [8.9]\n47  250             [19.5]           [26.0]        [-6.5]\n48   31             [19.5]            [5.7]        [13.8]\n49  134             [19.5]           [24.4]        [-4.9]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Row</th>\n      <th>Predicted Body Fat</th>\n      <th>Correct Body Fat</th>\n      <th>Difference +-</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>164</td>\n      <td>[19.5]</td>\n      <td>[27.3]</td>\n      <td>[-7.8]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>66</td>\n      <td>[19.5]</td>\n      <td>[21.5]</td>\n      <td>[-2.0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>180</td>\n      <td>[19.5]</td>\n      <td>[26.6]</td>\n      <td>[-7.1]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>148</td>\n      <td>[19.5]</td>\n      <td>[5.3]</td>\n      <td>[14.2]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>[19.5]</td>\n      <td>[12.3]</td>\n      <td>[7.2]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>120</td>\n      <td>[19.5]</td>\n      <td>[27.9]</td>\n      <td>[-8.4]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>49</td>\n      <td>[19.5]</td>\n      <td>[4.0]</td>\n      <td>[15.5]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>127</td>\n      <td>[19.5]</td>\n      <td>[17.4]</td>\n      <td>[2.1]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>219</td>\n      <td>[19.5]</td>\n      <td>[15.0]</td>\n      <td>[4.5]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>189</td>\n      <td>[19.5]</td>\n      <td>[24.4]</td>\n      <td>[-4.9]</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>139</td>\n      <td>[19.5]</td>\n      <td>[20.4]</td>\n      <td>[-0.9]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>25</td>\n      <td>[19.5]</td>\n      <td>[3.7]</td>\n      <td>[15.8]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>108</td>\n      <td>[19.5]</td>\n      <td>[17.3]</td>\n      <td>[2.2]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>231</td>\n      <td>[19.5]</td>\n      <td>[16.1]</td>\n      <td>[3.4]</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>119</td>\n      <td>[19.5]</td>\n      <td>[18.1]</td>\n      <td>[1.4]</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>200</td>\n      <td>[19.5]</td>\n      <td>[12.2]</td>\n      <td>[7.3]</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>46</td>\n      <td>[19.5]</td>\n      <td>[10.8]</td>\n      <td>[8.7]</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>51</td>\n      <td>[19.5]</td>\n      <td>[6.6]</td>\n      <td>[12.9]</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>216</td>\n      <td>[19.5]</td>\n      <td>[13.6]</td>\n      <td>[5.9]</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>87</td>\n      <td>[19.5]</td>\n      <td>[23.1]</td>\n      <td>[-3.6]</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>202</td>\n      <td>[19.5]</td>\n      <td>[28.7]</td>\n      <td>[-9.2]</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>67</td>\n      <td>[19.5]</td>\n      <td>[13.8]</td>\n      <td>[5.7]</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>97</td>\n      <td>[19.5]</td>\n      <td>[11.3]</td>\n      <td>[8.2]</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>157</td>\n      <td>[19.5]</td>\n      <td>[10.0]</td>\n      <td>[9.5]</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>190</td>\n      <td>[19.5]</td>\n      <td>[11.4]</td>\n      <td>[8.1]</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>237</td>\n      <td>[19.5]</td>\n      <td>[27.3]</td>\n      <td>[-7.8]</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>30</td>\n      <td>[19.5]</td>\n      <td>[11.9]</td>\n      <td>[7.6]</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>109</td>\n      <td>[19.5]</td>\n      <td>[21.4]</td>\n      <td>[-1.9]</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>26</td>\n      <td>[19.5]</td>\n      <td>[7.9]</td>\n      <td>[11.6]</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>184</td>\n      <td>[19.5]</td>\n      <td>[17.5]</td>\n      <td>[2.0]</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>161</td>\n      <td>[19.5]</td>\n      <td>[14.6]</td>\n      <td>[4.9]</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>239</td>\n      <td>[19.5]</td>\n      <td>[29.9]</td>\n      <td>[-10.4]</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>45</td>\n      <td>[19.5]</td>\n      <td>[13.9]</td>\n      <td>[5.6]</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>4</td>\n      <td>[19.5]</td>\n      <td>[28.7]</td>\n      <td>[-9.2]</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>106</td>\n      <td>[19.5]</td>\n      <td>[19.3]</td>\n      <td>[0.2]</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>158</td>\n      <td>[19.5]</td>\n      <td>[12.5]</td>\n      <td>[7.0]</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>178</td>\n      <td>[19.5]</td>\n      <td>[22.5]</td>\n      <td>[-3.0]</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>61</td>\n      <td>[19.5]</td>\n      <td>[29.8]</td>\n      <td>[-10.3]</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>168</td>\n      <td>[19.5]</td>\n      <td>[34.3]</td>\n      <td>[-14.8]</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>238</td>\n      <td>[19.5]</td>\n      <td>[12.4]</td>\n      <td>[7.1]</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>196</td>\n      <td>[19.5]</td>\n      <td>[22.0]</td>\n      <td>[-2.5]</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>73</td>\n      <td>[19.5]</td>\n      <td>[13.5]</td>\n      <td>[6.0]</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>191</td>\n      <td>[19.5]</td>\n      <td>[38.1]</td>\n      <td>[-18.6]</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>59</td>\n      <td>[19.5]</td>\n      <td>[24.6]</td>\n      <td>[-5.1]</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>205</td>\n      <td>[19.5]</td>\n      <td>[16.6]</td>\n      <td>[2.9]</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>152</td>\n      <td>[19.5]</td>\n      <td>[10.1]</td>\n      <td>[9.4]</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>230</td>\n      <td>[19.5]</td>\n      <td>[10.6]</td>\n      <td>[8.9]</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>250</td>\n      <td>[19.5]</td>\n      <td>[26.0]</td>\n      <td>[-6.5]</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>31</td>\n      <td>[19.5]</td>\n      <td>[5.7]</td>\n      <td>[13.8]</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>134</td>\n      <td>[19.5]</td>\n      <td>[24.4]</td>\n      <td>[-4.9]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "result_table = pd.DataFrame(columns=['Row', 'Predicted Body Fat', 'Correct Body Fat', 'Difference +-'])\n",
    "\n",
    "# Get the mean and variance from the normalization layer\n",
    "mean = train_bodyfat_normalized.mean.numpy()\n",
    "variance = train_bodyfat_normalized.variance.numpy()\n",
    "\n",
    "# Get the minimum and maximum values of the original body fat data\n",
    "min_value = trainDF['BodyFat'].min()\n",
    "max_value = trainDF['BodyFat'].max()\n",
    "\n",
    "# Iterate over the rows of the test dataframe\n",
    "for index, row in testDF.iterrows():\n",
    "    test_features = {name: np.array([value]) for name, value in row.items() if name != 'BodyFat'}\n",
    "\n",
    "    # Get the correct label for the current row\n",
    "    correct_label = np.array([row['BodyFat']])\n",
    "    correct_label_normalized = (correct_label - mean) / np.sqrt(variance)\n",
    "\n",
    "    # Make predictions using the model\n",
    "    predictions = my_model.predict(test_features, verbose=0)\n",
    "\n",
    "    predicted_bodyfat_normalized = predictions['dense_output'][0][0]\n",
    "    predicted_bodyfat = (predicted_bodyfat_normalized * np.sqrt(variance)) + mean\n",
    "    predicted_bodyfat = np.round(predicted_bodyfat, decimals=1)\n",
    "\n",
    "    actual_bodyfat = np.round((correct_label_normalized * np.sqrt(variance)) + mean, decimals=1)\n",
    "    difference = np.round(predicted_bodyfat - actual_bodyfat, 1)\n",
    "    row_data = pd.DataFrame({'Row': [index],\n",
    "                             'Predicted Body Fat': [predicted_bodyfat],\n",
    "                             'Correct Body Fat': [actual_bodyfat],\n",
    "                             'Difference +-': [difference]})\n",
    "\n",
    "    # Concatenate the row DataFrame to the result_table\n",
    "    result_table = pd.concat([result_table, row_data], ignore_index=True)\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "result_table"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
